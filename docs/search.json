[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of Real World Data Science (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders of Real World Data Science (RWDS) pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Our standards",
    "text": "Our standards\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement responsibilities",
    "text": "Enforcement responsibilities\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Scope",
    "text": "Scope\nThis Code of Conduct applies within all community spaces (encompassing this site, our GitHub repository, our social media channels, and any RWDS-organised online and offline events). It also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\nNote that unless prior permission is agreed in writing with the editor of RWDS, only the editor and editorial board of RWDS may officially represent the community. Comment to the media must only be given by appointed representatives and must be approved by the RSS press office.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement",
    "text": "Enforcement\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at rwds@rss.org.uk. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement guidelines",
    "text": "Enforcement guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Attribution",
    "text": "Attribution\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributor Covenant Code of Conduct"
    ]
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html",
    "title": "A demonstration of the law of the flowering plants",
    "section": "",
    "text": "This tutorial will demonstrate a popular method for predicting the day a flower will bloom. There are many reasons why you might want to predict a bloom date. You might be a scientist studying ecosystems stressed by climate change. Or you might be planning a trip to Amsterdam and would like to time your stay to when the tulips are in bloom. Or maybe you are participating in the annual Cherry Blossom Prediction Competition and want some ideas to help you get started.\nIn any case, you might be surprised to learn that the day a flower blooms is one of the earliest phenomena studied with systematic data collection and analysis. The mathematical rule developed in the eighteenth century to make these predictions – now called the “law of the flowering plants” – shaped the direction of statistics as a field and is still used by scientists with relatively few changes.\nWe present the law of the flowering plants as it was stated by Adolphe Quetelet, an influential nineteenth century statistician. Upon completing this tutorial, you will be able to:\nAt the end of the tutorial, we challenge you to design an algorithm that beats our predictions. The tutorial uses the R programming language. In particular, the code relies on the following packages:"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "The law of the flowering plants",
    "text": "The law of the flowering plants\nWe begin by reviewing the law of the flowering plants as it was stated by Adolphe Quetelet. You may already know Quetelet as the inventor of the body mass index. Less known is that Quetelet recorded the bloom dates of hundreds of different plants between 1833 and 1852 at the Brussels Observatory, which he founded and directed. Quetelet reported that a plant flowers when exposed to a specific quantity of heat, measured in degrees of Celsius squared (°C²). For example, he calculated that a lilac blooms when the sum of the daily temperatures squared exceeds 4264°C² following the last frost.\nHe communicated this law in his Letters addressed to HRH the grand duke of Saxe-Coburg and Gotha (Number 33, 1846; translated 1849) and in his reporting On the climate of Belgium (Chapter 4, Part 4, 1848; data updated in Part 7, 1857). A picture of Quetelet and the title page of On the climate of Belgium are displayed in Figure 1.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Quetelet reported on the law of the flowering plants in On the climate of Belgium (1857). Sources: Wikimedia Commons, Gallica.\n\nQuetelet was not the first to study bloom dates. Anthophiles have recorded the dates that flowers bloom for centuries. Written records of cherry trees go back as far as 812 AD in Japan and peach and plum trees as far as 1308 AD in China. Systematic record keeping began a century before Quetelet with Robert Marsham’s Indications of Spring (1789).\nQuetelet was also not the first to study the relationship between temperature and bloom dates. René Réaumur (1735), an early adopter of the thermometer, noted the relationship before Marsham published his Indications. But Quetelet was the first to systematically study the relationship across a wide variety of plants and derive the amount of heat needed to bloom. An example of Quetelet’s careful record keeping can be seen in Figure 2, one of many tables he reported in his publications.\n\n\n\n\n\n\nFigure 2: Bloom dates at Brussels Observatory observed by Quetelet between 1839 and 1852. Source: Gallica."
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#reproducing-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Reproducing Quetelet’s law of the flowering plants",
    "text": "Reproducing Quetelet’s law of the flowering plants\nTo reproduce Quetelet’s law, we combine the data in Figure 2 with additional observations from his Letters. We focus on Quetelet’s primary example, the bloom date of the common lilac, Syringa vulgaris, row 18 of Figure 2. We do this because Quetelet carefully describes his methodology for measuring the bloom date of lilacs. For example, Quetelet considers a lilac to have bloomed when “the first corolla opens and shows the stamina.” That event is closest to what the USA Phenology Network describes as “open flowers”, depicted in the center image of Figure 3 below. This detail will become relevant when we attempt to replicate Quetelet’s law in a later section. Note that although we focus on lilacs in this tutorial, the R code is easily edited to predict the day that other plants will bloom.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: The bloom date occurs when the first corolla opens and shows the stamina (center image). Source: USA National Phenology Network.\n\nIn the R code below, the five-column tibble lilac contains the date each year that Quetelet observed the lilacs bloom at Brussels Observatory. The first three columns are the month, day, and year the lilacs bloomed between 1839 and 1852. These columns are combined to form the fourth column, the full date the lilacs bloomed. The last column converts the date to the day of the year the lilacs bloomed, abbreviated “doy.” That is, “doy” is the number of days it took for the lilacs bloom following January 1. Both “date” and “doy” representations of Quetelet’s observations will be useful throughout this tutorial.\n```{r}\nlilac &lt;-                   \n  tibble(month = c(\"May\", \"April\", \"April\", \"April\", \"April\", \"April\", \"May\", \n                   \"April\", \"May\", \"April\", \"May\", \"April\", \"May\", \"May\"),\n         day   =  c(10, 28, 24, 28, 20, 25, 13, 12, 9, 21, 2, 30, 1, 12),\n         year  = 1839:1852,\n         date  = as.Date(paste(month, day, year), format = \"%B %d %Y\"),\n         doy   = parse_number(format(date, \"%j\"))) \n\nlilac %&gt;% \n  kable(align = \"c\",\n        caption = \"Table 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 1: Bloom dates of lilacs observed by Quetelet between 1839 and 1852.\n\n\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\nMay\n\n\n10\n\n\n1839\n\n\n1839-05-10\n\n\n130\n\n\n\n\nApril\n\n\n28\n\n\n1840\n\n\n1840-04-28\n\n\n119\n\n\n\n\nApril\n\n\n24\n\n\n1841\n\n\n1841-04-24\n\n\n114\n\n\n\n\nApril\n\n\n28\n\n\n1842\n\n\n1842-04-28\n\n\n118\n\n\n\n\nApril\n\n\n20\n\n\n1843\n\n\n1843-04-20\n\n\n110\n\n\n\n\nApril\n\n\n25\n\n\n1844\n\n\n1844-04-25\n\n\n116\n\n\n\n\nMay\n\n\n13\n\n\n1845\n\n\n1845-05-13\n\n\n133\n\n\n\n\nApril\n\n\n12\n\n\n1846\n\n\n1846-04-12\n\n\n102\n\n\n\n\nMay\n\n\n9\n\n\n1847\n\n\n1847-05-09\n\n\n129\n\n\n\n\nApril\n\n\n21\n\n\n1848\n\n\n1848-04-21\n\n\n112\n\n\n\n\nMay\n\n\n2\n\n\n1849\n\n\n1849-05-02\n\n\n122\n\n\n\n\nApril\n\n\n30\n\n\n1850\n\n\n1850-04-30\n\n\n120\n\n\n\n\nMay\n\n\n1\n\n\n1851\n\n\n1851-05-01\n\n\n121\n\n\n\n\nMay\n\n\n12\n\n\n1852\n\n\n1852-05-12\n\n\n133\n\n\n\n\n\n\nTo reproduce Quetelet’s law of the flowering plants, we will combine these bloom dates with daily temperature. The daily maximum and minimum temperatures at Brussels Observatory between 1839 and 1852 are available from the Global Historical Climatology Network. The data can be downloaded using the ghcnd_search function contained within the R package rnoaa (2021). The station id for Brussels Observatory is “BE000006447”.\nThe ghcnd_search function returns the maximum and minimum temperature as separate tibbles in a list. In the R code below, we join the tibbles using the reduce function. Note that the temperature is reported in tenths of a degree (i.e. 0.1°C) so we divide by 10 before calculating the temperature midrange, our estimate of the daily temperature.\nThe result is a five-column tibble temp, which contains the year of the temperature record (“year”), the date of the temperature record (“date”), the maximum temperature (“tmax”), the minimum temperature (“tmin”), and the midrange temperature (“temp”). The first 10 rows of the table are below. When you produce the full table yourself, you may notice that a small portion of temperature records are missing. We found that imputing these missing values does not significantly change the results. Therefore, we ignore these days when conducting our analysis.\n```{r}\ntemp &lt;- \n  ghcnd_search(stationid = \"BE000006447\",\n               var = c(\"tmax\", \"tmin\"),\n               date_min = \"1839-01-01\",\n               date_max = \"1852-12-31\") %&gt;%\n  reduce(left_join) %&gt;%\n  transmute(year = parse_number(format(date, \"%Y\")), \n            date, \n            tmax = tmax / 10, \n            tmin = tmin / 10, \n            temp = (tmax + tmin) / 2)\n  \ntemp %&gt;% \n  kable(align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 2: Temperature observed at Brussels Observatory between 1839 and 1852.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 2: Temperature observed at Brussels Observatory between 1839 and 1852.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n1839\n\n\n1839-01-01\n\n\n5.7\n\n\n-0.2\n\n\n2.75\n\n\n\n\n1839\n\n\n1839-01-02\n\n\n6.3\n\n\n0.8\n\n\n3.55\n\n\n\n\n1839\n\n\n1839-01-03\n\n\n7.2\n\n\n1.8\n\n\n4.50\n\n\n\n\n1839\n\n\n1839-01-04\n\n\n8.0\n\n\n1.8\n\n\n4.90\n\n\n\n\n1839\n\n\n1839-01-05\n\n\n5.3\n\n\n0.8\n\n\n3.05\n\n\n\n\n1839\n\n\n1839-01-06\n\n\n10.0\n\n\n1.3\n\n\n5.65\n\n\n\n\n1839\n\n\n1839-01-07\n\n\n8.9\n\n\n1.4\n\n\n5.15\n\n\n\n\n1839\n\n\n1839-01-08\n\n\n3.0\n\n\n0.1\n\n\n1.55\n\n\n\n\n1839\n\n\n1839-01-09\n\n\n0.8\n\n\n-0.1\n\n\n0.35\n\n\n\n\n1839\n\n\n1839-01-10\n\n\n2.8\n\n\n-2.8\n\n\n0.00\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nReproducing Quetelet’s law is now a simple matter of calculating the sum of the squared daily temperature from the day of last frost until the bloom day. We could use the day of last frost reported in Quetelet’s Letters. However, since we will replicate Quetelet’s analysis with recent data in a later section, we use our own definition of the day of last frost. We define the day of last frost to be the day following the last day the maximum temperature is below 0. The R code below creates the function doy_last_frost to extract the day of last frost from the maximum temperature. To demonstrate this function, we then compare the bloom date with the last frost date in 1839, the first year Quetelet observed.\n```{r}\ndoy_last_frost &lt;- function(tmax, doy_max = 100) {\n  dof &lt;- which(tmax[1:doy_max] &lt;= 0)\n  if(length(dof) == 0) 1 else max(dof) + 1\n  }\n\nbloom_day &lt;- \n  lilac %&gt;% \n  filter(year == 1839) %&gt;%\n  pull(doy) + \n  as.Date(\"1839-01-01\")\n  \nfrost_day &lt;- \n  temp %&gt;% \n  filter(year == 1839) %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"1839-01-01\") \n\ntibble(`last frost date` = frost_day, \n       `bloom date` = bloom_day) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 3: Last frost date and lilac bloom date at Brussels Observatory in 1839.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n1839-03-08\n\n\n1839-05-11\n\n\n\n\n\n\nIf Quetelet’s law of the flowering plants is correct, Table 3 has the following interpretation. On March 8, 1839 the lilacs at Brussels Observatory began “collecting” temperature. The lilacs continued to “collect” temperature until May 11, at which point they exceeded their 4264°C² quota and bloomed. We visualize this theory in Figure 4 with the R packages ggplot2, a member of the set of packages that constitute the “tidyverse” (2019), and plotly.\n```{r}\n(temp %&gt;% \n  filter(date &lt; as.Date(\"1839-06-01\")) %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title = \n      \"Figure 4: According to Quetelet's law, the lilacs bloom when exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(bloom_day, frost_day)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day, bloom_day)),\n                  y = c(-4, -4),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-10, -12)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 4: According to Quetelet’s law, the lilacs bloom when exposed to 4264°C² following the last frost. Author provided, CC BY 4.0.\n\nWe now have all the ingredients necessary to reproduce Quetelet’s findings. Our reproduction is greatly simplified by using the nest function from the tidyr package, another member of the “tidyverse”. For an overview of nest, see the “Nested data” section of Grolemund and Wickham (2017). We will group the data by year, nest, calculate the cumulative squared temperature from the frost date to the bloom date within each year, and then unnest. We ignore temperatures below 0°C. That is, temperatures below 0°C are set to 0°C. We do this because it is clear from Quetelet’s derivation of the law that only positive temperatures should be squared. See the next section for details.\n```{r}\nquetelet &lt;- \n  temp %&gt;% \n  group_by(year) %&gt;% \n  nest() %&gt;% \n  left_join(lilac) %&gt;% \n  mutate(law = map(data, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax) + 1):doy]^2))) %&gt;% \n  unnest(law) %&gt;% \n  ungroup()\n\nquetelet %&gt;% \n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law)/sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\", \n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\", \n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 4: Reproduction of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 4: Reproduction of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4261\n\n\n197\n\n\n[3867, 4656]\n\n\n\n\n\n\nThe results show that Quetelet’s findings are indeed reproducible. Quetelet estimated that lilacs bloom once exposed to 4264°C² following the last frost. Our reanalysis suggests a similar amount. However, 4264°C² is the overall average across all years – the estimated amount needed to bloom varies year to year. As a result, the average has a 95% confidence interval of approximately 3870°C² to 4660°C². Quetelet was well aware of this variation. He argued it was due to unobserved factors that influence growing conditions and change each year, and he dedicated significant space in his Letters to discuss them.\nThese unobserved factors limit the accuracy of predictions made using the law. To assess the predictive accuracy of the law, we temporarily ignore the bloom dates Quetelet observed. Instead, we apply the 4264°C² quota to the temperature records at Brussels Observatory to predict the bloom date. We then compare our predictions with the bloom date Quetelet observed. The R code below creates the function doy_prediction to estimate the day the lilac will bloom from temperature records. Table 5 summarizes the accuracy of Quetelet’s law by the mean absolute error and root mean squared error.\n```{r}\ndoy_prediction &lt;- function(temp, tmax)\n  doy_last_frost(tmax) + which.max(cumsum(pmax(temp[(doy_last_frost(tmax) + 1):365], 0, na.rm = TRUE)^2) &gt; 4264)\n\nquetelet %&gt;% \n  mutate(pred = map(data, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup() %&gt;%\n  summarize(mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\", \"root mean squared error (days)\"),\n        caption = \"Table 5: Predictions using Quetelet's law are accurate within a week on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 5: Predictions using Quetelet’s law are accurate within a week on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n5\n\n\n6\n\n\n\n\n\n\nTable 5 indicates that predictions made using the law are accurate to within a week on average. For comparison purposes, we also predict the day the lilacs will bloom using the average bloom date between 1839 and 1852. That is, on average the lilac bloomed on April 30 (April 29 on leap years), and we check the accuracy of simply predicting this average date each year. Table 6 indicates the average bloom date yields predictions that are less accurate by an average of two days.\n```{r}\nquetelet %&gt;%\n  summarize(pred = mean(doy),\n            mae  = mean(abs(doy - pred)),\n            rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(dig = 0,\n        align = \"c\",\n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 6: Predictions using the average bloom date are off by a week or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 6: Predictions using the average bloom date are off by a week or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n7\n\n\n9"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#quetelets-derivation-of-the-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s derivation of the law of the flowering plants",
    "text": "Quetelet’s derivation of the law of the flowering plants\nQuetelet believed that, as in physics, universal laws govern social and biological phenomenon. Quetelet was not only inspired by physics to describe social and biological patterns using mathematical formulas. He often took his formulas directly from physics. In fact, you may have already recognized similarities between his law and Newton’s second law of motion.\nQuetelet reasoned that temperature exerts a “force” on plants in the same way that gravity exerts a force on a falling object. Newton’s second law states that acceleration is proportional to force. It follows that an object initially at rest and subject to a constant force will travel a distance proportional to time squared. Quetelet simply substituted temperature for time.\nWe briefly elaborate. Let \\(d(t)\\) denote the distance an object travels after time \\(t\\). Let \\(v(t) = d'(t)\\) denote its speed and \\(a(t) = v'(t)\\) its acceleration. If acceleration is constant, i.e. \\(a(t) = c\\),\n\n\\(v(t) = \\int_0^t a(s) \\, ds = \\int_0^t c \\, ds = c t\\)\n\nand\n\n\\(d(t) = \\int_0^t v(s) \\, ds = \\int_0^t c s \\, ds = \\tfrac{c}{2} t^2\\)\n\nQuetelet imagined plants experience time in temperature and bloom after “traveling” distance \\(d_*\\). If a plant is exposed to temperature \\(t_i\\) on day \\(i = 1, 2, \\ldots\\), then the bloom date, \\(n_*\\), is the first day \\(\\sum_{i=1}^{n_*} \\tfrac{c}{2} t_i^2 \\geq d_*\\). Multiplying both sides of the inequality by \\(\\tfrac{2}{c}\\), yields Quetelet’s law: the bloom is the first day, \\(n_*\\), that \\(\\sum_{i=1}^{n_*} t_i^2 \\geq \\tfrac{2}{c} d_*\\).\nThe derivation of laws like the law of the flowering plants was popular in the nineteenth century. But any similarities between the “force” of temperature and the force of gravity are likely coincidental. We are not aware of any biological mechanisms that justify Quetelet’s application of Newton’s law.\nToday, the law of the flowering plants is considered a heuristic, or rule of thumb, that approximates complicated biological mechanisms. Like Quetelet, scientists model plants as experiencing time in temperature instead of calendar time. These temperature units are typically called “growing degree days”. Scientists often find that plants may only be sensitive to temperatures in specific ranges or “modified growing degree days”. Although modern statistical methods can greatly improve the accuracy of predictions, laws like Quetelet’s remain popular because they are simple to communicate and easy to replicate, as we demonstrate in the next section."
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#replicating-quetelets-law-of-the-flowering-plants",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Replicating Quetelet’s law of the flowering plants",
    "text": "Replicating Quetelet’s law of the flowering plants\nIn the previous section, we explained how Quetelet derived the law of the flowering plants. Quetelet believed the law of the flowering plants was universal, describing the bloom date of all flowers around the world and in any year. Whether the law can in fact be considered universal requires replicating Quetelet’s results with new data collected at a different location in a different year.\nIn this section, we replicate the law of the flowering plants using lilac bloom dates observed by scientists between 1956 and 2009 at 53 locations throughout the Pacific Northwest (2015). The data can be downloaded from the USA National Phenology Network using the rnpn package (2022). For space considerations, the R code that downloads and cleans the data is provided in the Appendix. Running this code yields the tibble usa_npn. Each row of the tibble corresponds with a bloom date observed at a given site in a given year. There are 31 columns, only seven of which we use in our replication. The remaining columns are documented in the rnpn package, and we will not review them here.\nTable 7 displays six of the seven columns (and only the first 10 rows of the full table). These columns are defined in the same way as the columns of Table 1, except for “site_id”, which denotes the site at which the observation was made. Table 1 does not have a “site_id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nload(url(\"https://github.com/jauerbach/miscellaneous/blob/main/usa_npn.RData?raw=true\"))\n\nusa_npn %&gt;%\n  transmute(site_id, \n            month = first_yes_month, \n            day   = first_yes_day, \n            year  = first_yes_year, \n            date  = as.Date(paste(month, day, year), format = \"%m %d %Y\"),\n            doy) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 7: Bloom dates of lilacs observed in pacific northwest between 1956 and 2009.\n\n\n\n\nsite_id\n\n\nmonth\n\n\nday\n\n\nyear\n\n\ndate\n\n\ndoy\n\n\n\n\n\n\n150\n\n\n5\n\n\n25\n\n\n1956\n\n\n1956-05-25\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n22\n\n\n1957\n\n\n1957-05-22\n\n\n142\n\n\n\n\n150\n\n\n5\n\n\n12\n\n\n1958\n\n\n1958-05-12\n\n\n132\n\n\n\n\n150\n\n\n6\n\n\n3\n\n\n1959\n\n\n1959-06-03\n\n\n154\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1960\n\n\n1960-05-27\n\n\n148\n\n\n\n\n150\n\n\n5\n\n\n27\n\n\n1961\n\n\n1961-05-27\n\n\n147\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1962\n\n\n1962-05-26\n\n\n146\n\n\n\n\n150\n\n\n5\n\n\n24\n\n\n1963\n\n\n1963-05-24\n\n\n144\n\n\n\n\n150\n\n\n5\n\n\n28\n\n\n1964\n\n\n1964-05-28\n\n\n149\n\n\n\n\n150\n\n\n5\n\n\n26\n\n\n1966\n\n\n1966-05-26\n\n\n146\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nThe seventh column we review is “temp”. Each row of “temp” is a tibble of temperature records taken at the nearest station in the Global Historical Climatology Network. The first tibble (again, only the first 10 rows) is displayed in Table 8 below. The columns are defined in the same way as the columns of Table 2, except for “id”, which denotes the location at which the temperature record was made. Table 2 does not have an “id” column because all observations were made at the same site, Brussels Observatory.\n```{r}\nusa_npn %&gt;%\n  pull(temp) %&gt;%\n  .[[1]] %&gt;%\n  mutate(year = parse_number(format(date, \"%Y\"))) %&gt;%\n  select(id, year, date, tmax, tmin, temp) %&gt;%\n  kable(align = \"c\",\n        col.names = c(\"id\", \"year\", \"date\", \"maximum temperature (°C)\", \n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 8: Temperature observed at an example pacific northwest site in 1956.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", \n             height = \"400px\")\n```\n\n\n\n\nTable 8: Temperature observed at an example pacific northwest site in 1956.\n\n\n\n\nid\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-01\n\n\n5.6\n\n\n-5.6\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-02\n\n\n1.7\n\n\n-7.2\n\n\n-2.75\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-03\n\n\n3.3\n\n\n-11.7\n\n\n-4.20\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-04\n\n\n4.4\n\n\n-10.0\n\n\n-2.80\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-05\n\n\n7.8\n\n\n0.0\n\n\n3.90\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-06\n\n\n4.4\n\n\n-11.1\n\n\n-3.35\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-07\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-08\n\n\n4.4\n\n\n-4.4\n\n\n0.00\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-09\n\n\n1.7\n\n\n-9.4\n\n\n-3.85\n\n\n\n\nUSC00245761\n\n\n1956\n\n\n1956-01-10\n\n\n2.8\n\n\n-6.1\n\n\n-1.65\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe are now prepared to replicate Quetelet’s findings. We will use R code nearly identical to the code we used to reproduce Quetelet’s findings earlier. The main difference is due to the fact that temperature records are dependent across sites within a year. To account for this dependence, we compute the cumulative temperature squared from the last frost to the bloom date for each site and year. We then take the average across all sites within a year. Finally, we calculate the standard error and confidence interval using only the variation of the averages across years. Table 9 displays the results.\n```{r}\nusa_npn %&gt;%             \n  group_by(rownames(usa_npn)) %&gt;%\n  mutate(law = \n           map(temp, ~ sum(pmax(.$temp, 0, na.rm = TRUE)[(doy_last_frost(.$tmax, doy) + 1):(doy - 1)]^2))) %&gt;%\n  unnest(law) %&gt;% \n  group_by(year) %&gt;%    \n  summarize(law = mean(law)) %&gt;%\n  summarize(Quetelet = 4264, \n            est = mean(law), \n            se = sd(law) / sqrt(n()),\n            ci  = str_c(\"[\", round(est - 2 * se), \", \", round(est + 2 * se), \"]\")) %&gt;%\n  kable(dig = 0, \n        align = \"c\",\n        col.names = c(\"Quetelet's law (°C²)\", \"estimate (°C²)\",\n                      \"standard error (°C²)\", \"95% confidence interval (°C²)\"),\n        caption = \"Table 9: Replication of Quetelet's analysis.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 9: Replication of Quetelet’s analysis.\n\n\n\n\nQuetelet’s law (°C²)\n\n\nestimate (°C²)\n\n\nstandard error (°C²)\n\n\n95% confidence interval (°C²)\n\n\n\n\n\n\n4264\n\n\n4329\n\n\n116\n\n\n[4098, 4560]\n\n\n\n\n\n\nTable 9 indicates that Quetelet’s findings are replicable in the sense that the confidence interval calculated using Quetelet’s data (Table 4) overlaps with the confidence interval calculated using the USA lilac data (Table 9). The standard error in Table 9 is smaller than Table 4 because the replication uses 54 years of data compared to Quetelet’s 14. Note that in the R code above, we subtract 1 from “doy” to correct for differences in how the bloom date is reported. This correction is not particularly important; the confidence intervals still overlap when this correction is removed.\nWe now investigate the accuracy of Quetelet’s law when applied to the USA lilac data. As before, we make use of the doy_prediction function.\n```{r}\nusa_npn &lt;- \n  usa_npn %&gt;% \n  mutate(pred = map(temp, ~ doy_prediction(.$temp, .$tmax))) %&gt;% \n  unnest(pred) %&gt;% \n  ungroup()\n\nusa_npn %&gt;% \n  summarize(mae  = mean(abs(doy - 1 - pred)),\n            rmse = sqrt(mean((doy - 1 - pred)^2))) %&gt;%\n  kable(dig = 0,\n        align = \"c\", \n        col.names = c(\"mean absolute error (days)\",\n                      \"root mean squared error (days)\"),\n        caption = \"Table 10: Predictions using Quetelet's law are accurate within about two weeks on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 10: Predictions using Quetelet’s law are accurate within about two weeks on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n10\n\n\n15\n\n\n\n\n\n\nTable 10 indicates that the predictions are accurate to within two weeks on average. Recall that the predictions using Quetelet’s own data were accurate to within one week on average (Table 5). We speculate that the decrease in accuracy is due in part to the fact that both Quetelet’s lilacs and the temperature were observed at the same site, Brussels Observatory. In some cases, the USA lilacs were a few miles from where the temperature was recorded.\nAlthough the accuracy of the predictions made using Quetelet’s law is lower when applied to the USA lilac data, Figure 5 indicates that the law produces the correct bloom date on average. The figure plots the predictions made by the law against the actual bloom dates scientists observed. Note that instead of representing prediction-observation pairs as points in a scatter plot, the data are represented using blue contours. We use contours because there are more than 1,500 observations – too many to study using a scatter plot.\n```{r}\n(usa_npn %&gt;% \n   mutate(doy = first_yes_doy) %&gt;%\n   unnest(pred) %&gt;% \n   ungroup() %&gt;%\n   mutate(predicted = as.Date(\"2020-01-01\") + pred,\n          observed = as.Date(\"2020-01-01\") + doy) %&gt;%\n   ggplot() + \n    aes(x = observed, y = predicted) +\n    geom_density2d(contour_var = \"ndensity\") +\n    geom_abline(intercept = 0, slope = 1, linetype = 2) +\n    labs(x = \"date observed\", \n         y = \"date predicted\",\n         title = \"Figure 5: Predictions using Quetelet's law are accurate within about two weeks on average.\") +\n    theme(legend.position = \"none\")) %&gt;%\n  ggplotly(tooltip = \"\") %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 5: Predictions using Quetelet’s law are accurate within about two weeks on average. Author provided, CC BY 4.0.\n\nThe contours are easy to interpret. The blue lines are much like a mountain range observed from above. The inner circles are peaks of high elevation in which many prediction-observation pairs co-occur. The outer circles are areas of low elevation in which few prediction-observation pairs co-occur.\nThe dotted line is the “y = x” line, having zero intercept and unit slope. Prediction-observation pairs that lie on the line indicate perfect predictions. The fact that the dotted line intersects the blue contours at their peak suggests the law derived from Quetelet’s data accurately predicts the typical bloom date of the USA data. This accuracy is impressive given the fact that the USA lilacs were observed more than a century later and on a different continent. The blue curves deviate from the line by about two weeks in the vertical direction, which is consistent with Table 10.\nAn average accuracy of two weeks might not sound impressive. But it is far more accurate than using the average bloom date Quetelet observed, April 30 (April 29 on leap years). The average bloom date yields predictions that are off by an additional eleven days on average.\n```{r}\nusa_npn %&gt;%\n  mutate(doy = first_yes_doy) %&gt;%\n  ungroup() %&gt;%\n  summarize(\n    pred = mean(quetelet$doy), \n    mae  = mean(abs(doy - pred)),\n    rmse = sqrt(mean((doy - pred)^2))) %&gt;%\n  select(mae, rmse) %&gt;%\n  kable(\n    dig = 0,\n    align = \"c\",\n    col.names = c(\"mean absolute error (days)\",\n                  \"root mean squared error (days)\"),\n    caption = \"Table 11: Predictions using the average bloom date are off by three weeks or more on average.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 11: Predictions using the average bloom date are off by three weeks or more on average.\n\n\n\n\nmean absolute error (days)\n\n\nroot mean squared error (days)\n\n\n\n\n\n\n21\n\n\n24"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#predicting-the-day-the-lilac-will-bloom-in-brussels-in-2023",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Predicting the day the lilac will bloom in Brussels in 2023",
    "text": "Predicting the day the lilac will bloom in Brussels in 2023\nAny weather forecast can become a flower forecast by applying the law of the flowering plants. In this section, we use the AccuWeather forecast to predict the day a hypothetical lilac will bloom in Brussels in 2023. AccuWeather forecasts daily maximum and minimum temperatures three months into the future. We do not evaluate the quality of these forecasts. The purpose of this section is to simply convert them into flower forecasts.\nWe use the AccuWeather forecast as it appeared on the webpage AccuWeather.com on February 19, 2023. AccuWeather reports the forecast for each month on a separate webpage. For reproducibility, we saved each page on the Internet Archive. The following R code creates the function get_weather_table to retrieve each page we saved, extract the forecast contained within that page, and arrange the data as a tibble. The get_weather_table function combines several functions from the rvest package, which is yet another member of the “tidyverse”. In particular, the forecast on each page is contained within the div “monthly-calendar” and can be extracted with the html_nodes and html_text2 functions.\nApplying the get_weather_table function to the url for each page yields a five column tibble temp_br, with columns defined in the same way as the tibble temp, discussed in previous sections. The first 10 rows are below; the data are also available on the author’s GitHub.\n```{r}\n get_weather_table &lt;- function(url)\n  read_html(url) %&gt;% \n  html_nodes(\"div.monthly-calendar\") %&gt;% \n  html_text2() %&gt;%\n  str_remove_all(\"°|Hist. Avg. \") %&gt;%\n  str_split(\" \", simplify = TRUE) %&gt;%\n  parse_number() %&gt;%\n  matrix(ncol = 3, \n         byrow = TRUE,\n         dimnames = list(NULL, c(\"day\", \"tmax\", \"tmin\"))) %&gt;%\n  as_tibble() %&gt;%\n  filter(\n    row_number() %in%\n      (which(diff(day) &lt; 0) %&gt;% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))\n\ntemp_br &lt;-\n  tibble(\n    base_url = \"https://web.archive.org/web/20230219151906/https://www.accuweather.com/en/be/brussels/27581/\",\n    month = month.name[1:5],\n    year = 2023,\n    url = str_c(base_url, tolower(month), \"-weather/27581?year=\", year, \"&unit=c\")) %&gt;%\n  mutate(temp = map(url, get_weather_table)) %&gt;%\n  pull(temp) %&gt;%\n  reduce(bind_rows) %&gt;%\n  transmute(date = seq(as.Date(\"2023-01-01\"), as.Date(\"2023-05-31\"), 1),\n            year = parse_number(format(date, \"%Y\")),\n            tmax,\n            tmin,\n            temp = (tmax + tmin) / 2)\n\ntemp_br %&gt;%\n  relocate(year) %&gt;%\n  kable(dig = 2,\n        align = \"c\", \n        col.names = c(\"year\", \"date\", \"maximum temperature (°C)\",\n                      \"minimum temperature (°C)\", \"midrange temperature (°C)\"),\n        caption = \"Table 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\") %&gt;%\n  kable_styling() %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\")\n```\n\n\n\n\nTable 12: Temperature forecast for Brussels, retrieved on February 19, 2023.\n\n\n\n\nyear\n\n\ndate\n\n\nmaximum temperature (°C)\n\n\nminimum temperature (°C)\n\n\nmidrange temperature (°C)\n\n\n\n\n\n\n2023\n\n\n2023-01-01\n\n\n15\n\n\n11\n\n\n13.0\n\n\n\n\n2023\n\n\n2023-01-02\n\n\n14\n\n\n5\n\n\n9.5\n\n\n\n\n2023\n\n\n2023-01-03\n\n\n9\n\n\n3\n\n\n6.0\n\n\n\n\n2023\n\n\n2023-01-04\n\n\n13\n\n\n8\n\n\n10.5\n\n\n\n\n2023\n\n\n2023-01-05\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-06\n\n\n12\n\n\n10\n\n\n11.0\n\n\n\n\n2023\n\n\n2023-01-07\n\n\n11\n\n\n9\n\n\n10.0\n\n\n\n\n2023\n\n\n2023-01-08\n\n\n10\n\n\n6\n\n\n8.0\n\n\n\n\n2023\n\n\n2023-01-09\n\n\n8\n\n\n5\n\n\n6.5\n\n\n\n\n2023\n\n\n2023-01-10\n\n\n12\n\n\n4\n\n\n8.0\n\n\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n…\n\n\n\n\n\n\nWe now predict the day the lilacs will bloom. The R code below uses the doy_prediction and doy_last_frost functions created in earlier sections and displays the prediction in Table 13. At the time of our writing, the predicted date is April 19. The forecast is easily updated by providing the url to the updated AccuWeather webpage. (You might use the url https://web.archive.org/save to save a webpage to the Internet Archive to ensure your work is reproducible.)\n```{r}\nbloom_day_br &lt;-\n  temp_br %&gt;%\n  summarize(date = doy_prediction(temp, tmax) + as.Date(\"2023-01-01\")) %&gt;%\n  pull(date)\n\nfrost_day_br &lt;- \n  temp_br %&gt;% \n  pull(tmax) %&gt;% \n  doy_last_frost() + as.Date(\"2023-01-01\") \n\ntibble(`last frost date` = frost_day_br, \n       `bloom date` = bloom_day_br) %&gt;%\n  kable(align = \"c\",\n        caption = \"Table 13: Last frost date and lilac bloom date in Brussels in 2023.\") %&gt;%\n  kable_styling()\n```\n\n\n\n\nTable 13: Last frost date and lilac bloom date in Brussels in 2023.\n\n\n\n\nlast frost date\n\n\nbloom date\n\n\n\n\n\n\n2023-01-27\n\n\n2023-04-19\n\n\n\n\n\n\nWe visualize the predictions in Figure 6, which has the same interpretation as Figure 4. If the temperature forecast and Quetelet’s law are correct, on January 27, 2023 the lilacs in Brussels began “collecting” temperature. The lilacs will continue to “collect” temperature until April 19, at which point they will exceed their 4264°C² quota and bloom.\n```{r}\n(temp_br %&gt;% \n  ggplot() + \n  aes(date, temp) + \n  geom_line() + \n  labs(\n    x = \"\",\n    y = \"midrange temperature (°C)\",\n    title =\n      \"Figure 6: According to Quetelet's law, the lilacs will bloom once exposed to 4264°C² following the last frost.\") +\n  geom_vline(xintercept = as.numeric(c(frost_day_br, bloom_day_br)), \n             linetype = \"dotted\")) %&gt;%\n  ggplotly() %&gt;% \n  add_annotations(x = as.numeric(c(frost_day_br, bloom_day_br)),\n                  y = c(14, 14),\n                  text = c(\"last\\nfrost\", \"first\\nbloom\"),\n                  font = list(size = 14),\n                  ay = 0,\n                  xshift = c(-14, -16)) %&gt;%\n  config(displaylogo = FALSE)\n```\n\n\nFigure 6: According to Quetelet’s law, the lilacs will bloom once exposed to 4264°C² following the last frost. Author provided, CC BY 4.0."
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#quetelets-legacy-advocate-mentor-and-perhaps-data-scientist",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist",
    "text": "Quetelet’s legacy: advocate, mentor, and perhaps data scientist\nIn this tutorial, we stated the law of the flowering plants and explained how Quetelet derived it. We also reproduced and replicated Quetelet’s findings before using his law to predict the day the lilac will bloom in Brussels. We now conclude with a reflection on Quetelet’s legacy.\nThe law of the flowering plants surely stands the test of time. It continues to be used by scientists – with relatively few changes – to plan harvests, manage pests, and study ecosystems stressed by climate change. We speculate the law’s longevity is due to the fact that it balances simplicity with relatively accurate predictions.\nAlthough Quetelet did not discover the law, he did much to advance it. Quetelet founded an international network for “observations of the periodical phenomena” (in addition to numerous statistical societies and publications, including the precursor to the Royal Statistical Society). Quetelet’s network of 80 stations collected observations throughout Europe from 1841 until 1872. In particular, Quetelet collaborated with Charles Morren – who later coined the term phenology, the name of the field that now studies biological life-cycle events like the timing of flower blooms (Demarée and Rutishauser 2011).\nIn recent years, the observations collected through phenology networks have become an important resource for understanding the impacts of climate change. For example, the USA National Phenology Network calculates the Spring Bloom Index, which measures the “first day of spring” using the days lilacs are observed to bloom at locations across the United States. The index is then compared to previous years. Figure 7 shows one comparison, called the Return Interval. The Return Interval is much like a p-value, calculating how frequently more extreme spring indices were observed in previous decades. Bloom dates that are uncommonly early (green) or late (purple) may indicate environments stressed by changing climate. Scientists exploit the relationship between temperature and bloom date to extrapolate the index to areas with few observations.\n\n\nFigure 7: The Spring Bloom Index Return Interval measures whether spring is typical when compared to recent decades. Source: USA National Phenology Network.\n\nQuetelet’s emphasis on discovering the universal laws he believed govern social and biological phenomenon has not endured. But data scientists continue to appropriate laws from one area of science to study another. For example, data scientists use neural networks and genetic algorithms to study a wide variety of phenomenon unrelated to neuroscience or genetics. Perhaps Quetelet’s appropriation of Newton’s law, in addition to his careful use of data, make him among the first data scientists?"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#your-turn-do-you-have-what-it-takes-to-beat-quetelets-law",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Your turn: Do you have what it takes to beat Quetelet’s law?",
    "text": "Your turn: Do you have what it takes to beat Quetelet’s law?\nQuetelet reported that a plant flowers when the sum of the daily temperatures squared exceeds a specific quantity. His prediction rule was state of the art in 1833. But surely you, a twenty-first century data scientist, can do better. Here are some ideas to get you started.\n\nQuetelet squared the temperature before calculating the sum. Would another function of temperature produce a more accurate prediction?\n\nRemove the square so that a plant flowers once the sum of the daily temperatures exceeds a (different) specific quantity. Does this version of the law produce more accurate predictions? What if you use the daily temperatures cubed? (Beginner)\nSuppose a lilac only registers temperatures between 0°C and 10°C. That is, a lilac experiences temperature below the lower limit, 0°C, as 0°C, and above the upper limit, 10°C, as 10°C. Does the accuracy of the predictions improve if you use the temperature the lilac experienced instead of the ambient temperature measured by a weather station? Write a program that finds the lower and upper limits that produce the most accurate predictions. (Intermediate)\nQuetelet used mean absolute error to evaluate the accuracy of his predictions. But his estimate of the specific quantity of heat needed to bloom, 4264°C², does not actually minimize mean absolute error. Write a program that finds the specific quantity that minimizes mean absolute error. Redo part i. and ii. using this function. (Advanced)\n\nQuetelet calculated the sum of the daily temperature squared between the day of last frost and the bloom date. Would another time interval produce more accurate predictions?\n\nWe estimated the day of last frost using the last day the maximum temperature was below 0°C. Try estimating the day of last frost by the last day the midrange temperature was below 0°C? Which estimate yields the most accurate predictions? What if you ignore the day of last frost and simply calculate the sum of the daily temperatures squared between February 1 and the bloom date? When you change the time interval, be sure to calculate the new specific quantity of heat needed to bloom. (Beginner)\nWrite a program that finds the time interval which yields the best predictions. (Intermediate)\nWrite a program that calculates the prediction rule for many different time intervals. Use cross-validation to combine these prediction rules into a single prediction rule. (Advanced)\n\nQuetelet’s law only considers the temperature. Would additional information provide more accurate predictions?\n\nIs the specific quantity of heat needed to bloom different in years with abnormally cold winters? Would the predictions be more accurate if you use one quantity of heat for years with cold winters and a different quantity of heat for years with warm winters? (Beginner)\nIs the estimated quantity of heat needed to bloom similar for locations close in space and time? Write a program that leverages spatial and temporal correlation to improve the accuracy of the predictions. (Intermediate)\nSome biologists report that a plant must be exposed to a fixed amount of cold temperature in the winter – in addition to a fixed amount of warm temperature in the spring – before it can bloom. Augment the law of the flowering plants to require the accumulation of a specific quantity of cold temperature before the accumulation of a specific quantity of warm temperature. Write a program that uses this new law to predict the day the lilac blooms. (Advanced)\n\n\nFeeling good about your prediction algorithm? Show it off at the annual Cherry Blossom Prediction Competition!"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "href": "instruments_old/tutorials/posts/2023/04/13/flowers.html#appendix-preparing-usa-npn-data",
    "title": "A demonstration of the law of the flowering plants",
    "section": "Appendix: Preparing USA NPN Data",
    "text": "Appendix: Preparing USA NPN Data\n```{r}\n# 1. download lilac data using `rnpn`\nusa_npn &lt;- \n  npn_download_individual_phenometrics(request_source = \"Jonathan Auerbach\",\n                                       year = 1900:2050,\n                                       species_ids = 36,                       \n                                       phenophase_ids = c(77, 412))            \n\n# 2. limit analysis to sites that report more than 25 times\nsite_ids &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(n = n()) %&gt;% filter(n &gt; 25) %&gt;% pull(site_id)\n\nusa_npn &lt;- \n  usa_npn %&gt;% \n  filter(site_id %in% site_ids)\n\n# 3. find nearest weather stations for each site\nlocations &lt;- \n  usa_npn %&gt;% \n  group_by(site_id) %&gt;% \n  summarize(latitude = first(latitude), \n            longitude = first(longitude))\n\nstations &lt;- \n  ghcnd_stations() %&gt;%\n  filter(first_year &lt;= min(usa_npn$first_yes_year),\n         last_year  &gt;= max(usa_npn$first_yes_year),\n         state != \"\") %&gt;%\n  group_by(id, latitude, longitude, state) %&gt;%\n  summarize(temp_flag = sum(element %in% c(\"TMIN\", \"TMAX\"))) %&gt;%            \n  filter(temp_flag == 2) %&gt;% \n  ungroup()\n\ndist &lt;- function(x, y = stations %&gt;% select(latitude, longitude)) \n  stations$id[which.min(sqrt((x[1] - y[,1])^2 + (x[2] - y[,2])^2)[,1])]\n\nlocations$station_id &lt;- apply(locations, 1, function(x) dist(c(x[\"latitude\"], x[\"longitude\"])))\n\n# 4. get weather data from nearest station using `rnoaa`\nget_station_data &lt;- function(station_id) \n  ghcnd_search(stationid = station_id,\n               var = c(\"tmin\", \"tmax\"),\n               date_min = \"1956-01-01\",\n               date_max = \"2011-12-31\") %&gt;%\n  reduce(left_join, by = c(\"id\", \"date\")) %&gt;%\n  transmute(id, \n            date, \n            tmax = tmax / 10,\n            tmin = tmin / 10)\n\nusa_npn &lt;- \n  locations %&gt;%\n  mutate(temp = map(station_id, get_station_data)) %&gt;%\n  right_join(usa_npn, by = c(\"site_id\", \"latitude\", \"longitude\")) %&gt;% \n  group_by(rownames(usa_npn)) %&gt;% \n  mutate(temp = map(temp, ~ .x %&gt;% \n                      filter(format(date, \"%Y\") == first_yes_year) %&gt;%\n                      mutate(temp = (tmin + tmax) / 2)),\n         num_obs = map(temp,~ sum(format(.x$date,\"%j\") &lt;= 150)),\n         doy = first_yes_doy, year = first_yes_year) %&gt;% \n  unnest(num_obs) %&gt;%  \n  filter(num_obs == 150) %&gt;%\n  ungroup()\n```\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nJonathan Auerbach is an assistant professor in the Department of Statistics at George Mason University. His research covers a wide range of topics at the intersection of statistics and public policy. His interests include the analysis of longitudinal data, particularly for data science and causal inference, as well as urban analytics, open data, and the collection, evaluation, and communication of official statistics. He co-organizes the annual Cherry Blossom Prediction Competition with David Kepplinger and Elizabeth Wolkovich.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Jonathan Auerbach\n\n\n  Text and code are licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Images are not covered by this licence, except where otherwise noted.\n\n\n\nHow to cite\n\nAuerbach, Jonathan. 2023. “A demonstration of the law of the flowering plants.” Real World Data Science, April 13, 2023. \n\n\n\n\n\n\n\n\n\nSource: Wikimedia Commons\n\n\nSource: Gallica\n\n\nSource: Gallica\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nSource: USA National Phenology Network\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nAuthor provided, CC BY 4.0\n\n\nSource: USA National Phenology Network"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "",
    "text": "In 2019, the data linking team at the Ministry of Justice was challenged to develop a new data linking methodology to produce new, higher quality linked datasets from the justice system.\nThe ultimate goal was to share new linked datasets with academic researchers, as part of the ADR UK-funded Data First programme. These datasets – which include data from prisons, probation, and the criminal and family courts – are now available, and researchers can apply for secure access.\nThe linking methodology is widely applicable and has been published as a free and open source software package called Splink. The software applies statistical best practice to accurately and quickly link and deduplicate large datasets. The software has now been downloaded over 7 million times, and has been used widely in government, academia and the private sector."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html#the-problem",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html#the-problem",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "The problem",
    "text": "The problem\nData duplication is a ubiquitous problem affecting data quality. Organisations often have multiple records that refer to the same entity but no unique identifier that ties these entities together. Data entry errors and other issues mean that variations usually exist, so the records belonging to a single entity aren’t necessarily identical.\nFor example, in a company, customer data may have been entered multiple times in multiple different databases, with different spellings of names, different addresses, and other typos. The inability to identify which records belong to each customer presents a data quality problem at all stages of data analysis – from basic questions such as counting the number of unique customers, through to advanced statistical analysis.\nWith the growing size of datasets held by many organisations, any solution must be able to work on very large datasets of tens of millions of records or more."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html#approach",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html#approach",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Approach",
    "text": "Approach\nIn collaboration with academic experts, the team started with desk research into data linking theory and practice, and a review of existing open source software implementations.\nOne of the most common theoretical approaches described in the literature is the Fellegi-Sunter model. This statistical model has a long history of application for high profile, important record linking tasks such as in the US Census Bureau and the UK Office for National Statistics (ONS).\nThe model takes pairwise comparisons of records as an input, and outputs a match score between 0 and 1, which (loosely) can be interpreted as the probability of the two records being a match. Since the record comparison can be either two records from the same dataset, or records from different datasets, this is applicable to both deduplication and linkage problems.\nAn important benefit of the model is explainability. The model uses a number of parameters, each of which has an intuitive explanation that can be understood by a non-technical audience. The relative simplicity of the model also means it is easier to understand and explain how biases in linkage may occur, such as varying levels of accuracy for different ethnic groups.\n\nExample\nConsider the following simple record comparison. Are these records a match?\n\n\n\nFigure 1: Colour coded comparison of two records.\n\n\nThe parameters of the model are known as partial match weights, which capture the strength of the evidence in favour or against these records being a match.\nThey can be represented in a chart as follows, in which the highlighted bars correspond to the above example record comparison:\n\n\n\nFigure 2: Chart showing partial match weights of model.\n\n\nWe can see, for example, that the first name (Robin vs Robyn) is not an exact match, but they have a Jaro-Winkler similarity of above 0.9. As a result, the model ‘activates’ the corresponding partial match weight (in orange). This lends some evidence in favour of a match, but the partial match weight is not as strong as it would have been for an exact match.\nSimilarly we can see that the non-match on gender leads to the activation (in purple) of a strong negative partial match weight.\nThe activated partial match weight can then be represented in a waterfall chart as follows, which shows how the final match score is calculated:\n\n\n\nFigure 3: Waterfall chart showing how partial match weights combine to calculate the final prediction.\n\n\nThe parameter estimates in these charts all have intuitive explanations:\n\nThe partial match weight on first name is positive, but relatively weak. This makes sense, because the first names are a fuzzy match, not an exact match, so this provides only moderate evidence in favour of the record being a match.\nThe match weight for the exact match on postcode is stronger than the equivalent weight for surname. This is because the cardinality of the postcode field in the underlying data is higher than the cardinality for surname, so matches on postcode are less likely to occur by chance than matches on surname.\nThe negative match weight for the mismatch on gender is relatively strong. This reflects the fact that, in this dataset, it’s uncommon for the ‘gender’ field to match amongst truly matching records.\n\nThe final result is that the model predicts these records are a match, but with only 94% probability: it’s not sure. Most examples would be less ambiguous than this one, and would have a match probability very close to either 0 or 1.\nFor further details of the theory behind the Fellegi-Sunter model, and a deep dive into the intuitive explanations of the model, I have have developed a series of interactive tutorials."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html#implementation",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html#implementation",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Implementation",
    "text": "Implementation\nThrough our desk research and open source software review, an existing software package called fastLink was identified which implements the Fellegi-Sunter model, but unfortunately the software is not able to handle very large datasets of more than a few hundred thousand records.\nInspired by the popularity of fastLink, the team quickly realised that the methodology it was developing was generally applicable and could be valuable to a wide range of users if published as a software package.\nAs we spoke to colleagues across government and beyond, we found record linkage and deduplication problems are pervasive, and crop up in many different guises, meaning that any software needed to be very general and flexible.\nThe result is Splink – which is a Python package that implements the Fellegi-Sunter model, and enables parameters to be estimated using the Expectation Maximisation algorithm.\nThe package is free to use, and open source. It is accompanied by detailed documentation, including a tutorial and a set of examples.\nSplink makes no assumptions about the type of entity being linked, so it is very flexible. We are aware of its use to match data on a variety of entity types including persons, companies, financial transactions and court cases.\nThe package closely follows the statistical approach described in fastLink. In particular it implements the same mathematical model and likelihood functions described in the fastLink paper (see pages 354 to 357), with a comprehensive suite of tests to ensure correctness of the implementation.\nIn addition, Splink introduces a number of innovations:\n\nAble to work at massive scale – with proven examples of its use on over 100 million records.\nExtremely fast – capable of linking 1 million records on a laptop in around a minute.\nComprehensive graphical output showing parameter estimates and iteration history make it easier to understand the model and diagnose statistical issues.\nA waterfall chart which can be generated for any record pair, which explains how the estimated match probability is derived.\nSupport for deduplication, linking, and a combination of both, including support for deduplicating and linking multiple datasets.\nGreater customisability of record comparisons, including the ability to specify custom, user defined comparison functions.\nTerm frequency adjustments on any number of columns.\nIt’s possible to save a model once it’s been estimated – enabling a model to be estimated, quality assured, and then reused as new data becomes available.\nA companion website provides a complete description of the various configuration options, and examples of how to achieve different linking objectives."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html#using-splink",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html#using-splink",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Using Splink",
    "text": "Using Splink\nFull documentation and a tutorial are available for Splink, but the following snippet gives a simple example of Splink in action:\nfrom splink.datasets import splink_datasets\nfrom splink.duckdb.blocking_rule_library import block_on\nfrom splink.duckdb.comparison_library import (\n    exact_match,\n    jaro_winkler_at_thresholds,\n    levenshtein_at_thresholds,\n)\nfrom splink.duckdb.linker import DuckDBLinker\n\ndf = splink_datasets.fake_1000\n\n# Specify a data linkage model\nsettings = {\n    \"link_type\": \"dedupe_only\",\n    \"blocking_rules_to_generate_predictions\": [\n      block_on(\"first_name\"),\n      block_on(\"surname\"),\n    ],\n    \"comparisons\": [\n        jaro_winkler_at_thresholds(\"first_name\", 2),\n        jaro_winkler_at_thresholds(\"surname\"),\n        levenshtein_at_thresholds(\"dob\"),\n        exact_match(\"city\", term_frequency_adjustments=True),\n        exact_match(\"email\"),\n    ],\n}\n\nlinker = DuckDBLinker(df, settings)\n\n# Estimate model parameters\n\n# Direct estimation using random sampling can be used for the u probabilities\nlinker.estimate_u_using_random_sampling(target_rows=1e6)\n\n# Expectation maximisation is used to train the m values\nbr_training = block_on([\"first_name\", \"surname\"])\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\nbr_training = block_on(\"dob\")\nlinker.estimate_parameters_using_expectation_maximisation(br_training)\n\n# Use the model to compute pairwise match scores\npairwise_predictions = linker.predict()\n\n# Cluster the match scores into groups to produce a synthetic unique person id\nclusters = linker.cluster_pairwise_predictions_at_threshold(\n  pairwise_predictions, 0.95\n)\nclusters.as_pandas_dataframe(limit=5)\nThe example shows the flexibility of Splink, and how various types of configuration can be used:\n\nHow should different data fields be compared? In this example, the Jaro-Winkler distance is used for names, whereas Levenshtein is used for date of birth since Jaro-Winkler is not appropriate for numeric data.\nWhat blocking rules should be used? Blocking rules are the primary determinants of how fast Splink will run, but there is a trade off between speed and accuracy. In this case, the input data is small, so the blocking rules are loose.\nHow should the model parameters be estimated? In this case, the user has no labels for supervised training, and so uses the unsupervised Expectation Maximisation approach.\nIs clustering needed? In this case, each person may potentially have many duplicates, so clustering is used. This creates an estimated (synthetic) unique identifier for each entity (person) in the input dataset."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/22/splink.html#outcomes",
    "href": "instruments_old/case-studies/posts/2023/11/22/splink.html#outcomes",
    "title": "Deduplicating and linking large datasets using Splink",
    "section": "Outcomes",
    "text": "Outcomes\nSplink has been used to link some of the largest datasets held by the Ministry of Justice as part of the Data First programme, and researchers are now able to apply for secure access to these datasets. Research using this data won the ONS Linked Administrative Data Award at the 2022 Research Excellence Awards.\nMore widely, the demand for Splink has been higher than we expected – with over 7 million downloads. It has been used in other government departments including the Office for National Statistics and internationally, the private sector, and published academic research from top international universities.\nSplink has also had external contributions from over 30 people, including staff at the Australian Bureau of Statistics, DataBricks, other government departments, academics, and various private sector consultancies.\n\n\n\n\n\n\nEditor’s note: For more on data linkage, check out our interview with Helen Miller-Bakewell of the UK Office for Statistics Regulation, discussing the OSR report, Data Sharing and Linkage for the Public Good.\n\n\n\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nRobin Linacre is an economist, data scientist and data engineer based at the UK Ministry of Justice. He is the lead author of Splink.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Robin Linacre\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Possessed Photography on Unsplash.\n\n\n\nHow to cite\n\nLinacre, Robin. 2023. “Deduplicating and linking large datasets using Splink.” Real World Data Science, November 22, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html",
    "title": "Food for Thought: Competition and challenge design",
    "section": "",
    "text": "Since 2014, the professional services firm Westat, Inc. has been developing the Purchase to Plate Crosswalk (PPC) for the United States Department of Agriculture (USDA) Economic Research Service (ERS). The PPC links the retail food transactions database from IRI’s InfoScan service and the USDA Food and Nutrient Database for Dietary Studies (FNDDS). However, the current linkage process uses only partly automated data matching, meaning it is resource intensive, time consuming, and requires manual review.\nWith sponsorship from ERS, Westat partnered with the Coleridge Initiative to host the Food for Thought competition to challenge researchers and data scientists to use machine learning and natural language processing to find accurate and efficient methods for creating the PPC. Figure 1 provides a visual overview of the challenge set by the competition.\nThe one-to-many matching task that is central to the competition throws up many challenges for researchers to wrestle with. Because IRI data contains food transactions collected from partnered retail establishments for over 350,000 items, the matchings need to be made based on limited data features, including categories, providers, and semantically inconsistent descriptions that consist of short phrases. Consider this hypothetical example: IRI product-related information about a (fictional) “Cheesy Hashbrowns Hamburger Helper, 5.5 Oz Box” needs to be linked to FNDDS nutrition-related information found under “Mixed dishes – meat, poultry, seafood: Mixed meat dishes”. Figure 2 demonstrates how the two databases are linked with each other to create the PPC. As can be seen, there is no common word that easily indicates that “Cheesy Hashbrowns Hamburger Helper…” should be matched with “Mixed dishes…”, and such cases exist in all IRI tables used for the challenge, from 2012 through 2018.\nAlso, because nutritionists or food scientists will always need to review the matching, regardless of the matching method used, it was important that our evaluation of proposed matching methods focused both on the accuracy of prediction models and also on metrics that would lead participants to develop models that facilitate qualified reviewers to reduce their workloads.\nOrganising the competition was also a challenge in its own right, for data privacy reasons. IRI scanner data contains sensitive information, such as store name, location, unit price, and weekly quantity sold for each item. This ruled out using existing online platforms like Kaggle, DrivenData or AIcrowd to host the competition, and instead required a private secure data enclave to ensure the safe use of sensitive and confidential data assets. The need for such an environment imposed capacity constraints on the competition, meaning only dozens of teams could be invited to take part, whereas on open platforms it is common to have thousands of teams competing and sharing ideas and code."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#competition-structure",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Competition structure",
    "text": "Competition structure\nThe competition ran over 10 months and consisted of three separate challenges: two interim, one final. Applications opened in September 2021, and the competition started in January 2022. Submission deadlines for the first and second interim challenges were in July and September 2022, respectively. For these rounds, participants submitted preliminary solutions for evaluation based solely on quantitative metrics, and two awards of $10,000 were given to the highest-scoring teams. The deadline for the final challenge was in October 2022. Here, solutions were evaluated by the scientific review board based on three judging criteria: quantitative metrics, transferability, and innovation. First, second, and third place winners received awards of $30,000, $1,500, and $1,000 respectively. Final presentations were given at the Food for Thought symposium in December 2022.\nThe competition was run entirely within the Coleridge Initiative’s Administrative Data Research Facility (ADRF), which was established by the United States Census Bureau to inform the decision-making of the Commission on Evidence-Based Policy under the Evidence Act. ADRF follows the Five Safes Framework: safe projects, safe people, safe data, safe settings, and safe outputs.\nIn keeping with this framework, participants were provided with ADRF login credentials after signing the relevant data use agreements during the onboarding process. All participants were required to agree to the ADRF terms of use, to complete security training, and to pass a security training assessment prior to accessing the challenge data. Participants’ access within ADRF was limited to the challenge environment and data only. There was no internet access, so Coleridge Initiative ensured that any packages requested by teams were available for use within the environment after passing security review. All codes and documentation were only allowed to be exported outside ADRF after export reviews from both Coleridge Initiative and USDA staff. At the end of each challenge, the teams submitted write-ups and supporting files by placing all the necessary submission files in their ADRF team folder. Detailed submission instructions are available via the Real World Data Science GitHub repository."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#metrics",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Metrics",
    "text": "Metrics\nSubmissions were evaluated by Coleridge Initiative and technical review and subject review boards based on the following criteria:\n\nQuantitative metrics were used to measure the predictive accuracy and runtime of the model.\n\nTransferability measured the quality of documentation and code, and the ability of individuals who are not involved in model development to replicate and implement the team’s approach.\n\nInnovation measured novelty and creativity of the model in addressing the linkage problem.\n\nTechnical review was overseen by faculty members from computer science and engineering departments of top US universities. Subject review was handled by subject matter experts from USDA and Westat.\nFrom a quantitative perspective, the most common way to evaluate machine learning competition submissions is to use model predictive accuracy. However, single metrics are typically incomplete descriptions of real-world tasks, and they can easily hide significant differences between models which simple predictive accuracy cannot capture. To select the most appropriate official challenge metrics, Coleridge Initiative reviewed the literature on the use of evaluation measures in both classification and ranking task machine learning competitions. Success at 5 (S@5) and Normalized Discounted Cumulative Gain at 5 (NDCG@5) scores were ultimately used as the quantitative metrics.\nThe metrics were applied as follows: models proposed by each team were tasked with outputting five potential FNDDS matches for each IRI code, with potential FNDDS matches ordered from most likely to least likely. S@5 and NDCG@5 scores are broadly similar – both measure whether a correct match is present in the five proposed matches that participants were asked to identify. However, S@5 does not take rank position into account and only considers whether the five proposed FNDDS matches contain the correct FNDDS response. NDCG@5 does take rank into account and also measures how highly the correct FNDDS response is ranked among the five proposed matches. Both measures range from 0 to 1 (or 0% to 100%). Models get a “full credit” for S@5 as long as they contain the correct FNDDS option. NDCG@5 penalizes models when the correct match is ranked lower on the list of 5 proposed matches."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#technical-description",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Technical description",
    "text": "Technical description\n\nEnvironment setup\nColeridge Initiative solicited technical requirements from participants at the challenge application stage to prepare the ADRF environment as much as possible before the competition began. Each team was asked to share anticipated workspace specifications and software library requests in their application package. From this we identified, reviewed, and installed the requested Python and R packages, libraries, and library components (e.g., pre-trained models, training data) that were not yet available within ADRF.\nThe setup of graphics processing units (GPUs) was also a critical part of competition preparation. We created an environment with 16 gibibyte (GiB) of GPU memory for each team. Our technology team met with multiple teams several times to discuss computing environment configurations to ensure the GPU could work properly. None of these efforts was wasted: without GPU access, it would be impossible for teams to use state-of-the-art pre-trained models such as the Bidirectional Encoder Representations from Transformers (BERT, Devlin et al. 2018).\nWe completed the setup of new team workspaces, each customized to the individual team’s resource and library requirements, including GPU configuration. The isolation and customization of workspaces was vital because teams may request different versions of libraries that potentially have version conflict with other libraries. We ensured the configurations were all set before the challenge began because such data challenges are bursty in nature (Macavaney et al. 2021), and handling support requests in the private data enclave risked causing delays. We hoped to avoid receiving too many requests in the beginning phase of the competition in order to give participants a better experience, though we did of course provide participants with instructions on how to request additional libraries during the challenge period.\n\n\nSupporting materials\nIn addition to environment preparation, we made available a list of supporting documentation, including IRI, PPC, and FNDDS codebooks, technical reports, and related publications that could help teams understand the challenge datasets. The FNDDS codebook pooled information on variable availability, coding, and descriptions across dataset files and years. It also included internal Westat food category coding difficulty ratings and notes on created PPC codes and provided UPC code, EC code, and general dataset remarks and observations that may take time for analysts to discover on their own.\nWe developed a baseline model to demonstrate the challenge task and the expected outputs – both outside of ADRF using FNDDS and fictitious data in place of IRI data, and an analogous model using FNDDS and IRI data within the ADRF secure environment. Moreover, we provided the teams with an evaluation script to read in their submissions and evaluate them for predictive accuracy against the public test set using S@5 and NDCG@5 challenge metrics. Finally, we held multiple webinars during the course of the challenge to explain next steps, address participant questions, solicit feedback, and provide general support. Multiple teams also met with our technology team to clarify ADRF-related questions or troubleshoot technical issues.\n(Baseline model, toolkits, and evaluation script are available from the Real World Data Science GitHub repository.)\n\n\nData splitting\nTo mimic the real-world scenario, the competition used 2012–2016 IRI data as the training set, and the 2017–2018 IRI data as the test set, since the data change over time and USDA could provide the most recent data available. To make sure that models were generalizable and not just overfit to the test set, we split the test set into private and public test sets. In this way, we guaranteed that the models were evaluated on completely hidden data. In order to keep the similar distribution of the two sets, we first divided the data into five quintiles based on EC code frequencies and then randomly sampled 80% of records in each group without repetition for placement into the private test set. Later in the competition, because of the computation limit, we further shrank the private test set to 40% of its original size using the same data-splitting method.\n\n\nJudging\nIn the first two rounds, submissions were evaluated based on the quantitative metrics, as previously mentioned above. Coleridge Initiative was responsible for running the evaluation script, making sure not to re-train the model or modify the configs in any way, and only applying the model to predict the private test set. Prediction results were then compared against ground truth to get the private scores.\nThe final challenge was reviewed by the scientific review board on all three judging criteria. Submitted models were first evaluated by Coleridge Initiative in the same way as in the first two rounds. The runtime of models was also recorded as an assessment of model cost. The scientific review boards then assessed the models by the quality of documentation, the quality of code, and the ability to replicate and implement the team’s approach, and scored the models for innovation and creativity in addressing the linkage problem. Lastly, scores were summarized and the scientific review board discussed and decided the winners of the competition."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#results",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#results",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Results",
    "text": "Results\nThe next few articles in this collection walk readers through the solutions proposed by competition finalists. Figure 3 provides a brief summary.\n\n\nFigure 3: Top competitors and their solutions to the Food for Thought challenge."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "href": "instruments_old/case-studies/posts/2023/08/21/02-competition-design.html#lessons-learned",
    "title": "Food for Thought: Competition and challenge design",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was undoubtedly challenging for teams to work with highly secured data in a private data enclave for this data challenge. We solicited feedback from teams and summarized the issues that we experienced throughout the competitions, together with the solutions to resolve those issues. Below are our main lessons learned and we hope this summary can serve to inform future competitions.\n\nEnvironmental factors: The installation and setup of packages, libraries, and resources, as well as the configuration of GPUs, system dependencies, and workspace design were expected to take a long time as each team had their own needs. To accelerate the process, we requested a list of specific package and environment requirements from the teams in advance. However, due to the complexity of the system configuration required by the teams, environment setup took longer than expected. Thus, the challenge deadlines had to be postponed a few times to accommodate this.\nTime commitment: Twelve teams were selected to participate in the challenge, but only three teams remained in the final challenge. Other than one team that was disqualified for violating the ADRF terms of use agreement, eight dropped out because of other commitments and insufficient time to meaningfully participate. To ensure security, ADRF does not allow jobs to run in the backend, which also adds to the time commitment of teams. To encourage teams to participate in the final challenge, we gave out additional awards for second and third places.\nComputing resource limit: One issue encountered in evaluating submitted models was computing environment resource limits due to the secured nature of the data enclave. The original private test dataset is four times larger than the public test dataset, making it unfeasible to evaluate. To overcome this issue, given the fixed resource constraints, we decided to reduce the private test set to 40% of its original size. It would have been helpful, though, if the competition had set a model running time limit at the outset, so that participants could build simpler yet effective models.\nSupporting code: Although the initial baseline model we provided was extremely simple, we found this helped participants a lot in the initial phase – yet there is space to improve. To be specific, supporting codes should be constructed so that all relevant data tables are used and specify the main function to run the code, especially how the model should be tested. The teams only used the main table, which was the only table that was used in the baseline model, for training and did not touch the other supporting table. If we included the other table in the baseline model, it could help participants to have a better use of this data as well. In addition, a baseline model should be intuitive for the participants to follow, allowing evaluators to easily replace the public test set with the private test set without any programming modifications.\n\n\n\n\n\n← Part 1: Purchase to Plate\n\n\n\n\nPart 3: First place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nZheyuan Zhang and Uyen Le are research scientists at the Coleridge Initiative.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Zheyuan Zhang and Uyen Le\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nZhang, Zheyuan, and Uyen Le. 2023. “Food for Thought: Competition and challenge design.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "",
    "text": "The Auburn Big Data team from Auburn University consists of five members, including three assistant professors: Dr Wenying Li of the Department of Agricultural Economics and Rural Sociology, Dr Jingyi Zheng of the Department of Mathematics and Statistics, and Dr Shubhra Kanti Karmaker of the Department of Computer Science and Software Engineering. Additionally, the team comprises two PhD students, Naman Bansal and Alex Knipper, who are affiliated with Dr Karmaker’s big data lab at Auburn University.\nIt is estimated that our team has spent approximately 1,400 hours on this project."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nAt the start of this competition, we decided to test three general approaches, in the order listed:\n\nA heuristic approach, where we use only the data and a defined similarity metric to predict which FNDDS label a given IRI item should have.\nA simpler modeling approach, where we train a simple statistical classifier, like a random forest (Parmar, Katariya, and Patel 2019), logistic regression, etc., to predict the FNDDS label for a given IRI item. For this method, we opted to use a random forest as our statistical model, as it was a simpler model to use as a baseline, having shown decent performance in a wide range of classification tasks. As it turned out, this approach was quite robust and accurate, so we kept it as our main model for this approach.\nA large language modeling approach, where we train a model like BERT (Devlin et al. 2018) to map the descriptions for given IRI and FNDDS items to the FNDDS category the supplied IRI item belongs to."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-approach",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our approach",
    "text": "Our approach\nAs we explored the data provided, we opted to use the given 2017–2018 PPC dataset as our primary dataset for both training and testing. To ensure a fair evaluation of the model, we randomly split the dataset into 60% training samples and 40% testing samples, making sure our training process never sees the testing dataset. For evaluating our models, we adopted the competition’s metrics: Success@5 and NDCG@5. After months of testing, our statistical classifier (approach #2) proved itself to be the model that both processes the data fastest and achieves the highest performance on our testing metrics.\nThis approach, at a high level, takes in the provided data (among other configuration parameters), formats the data in a computer-readable format – converting the IRI and FNDDS descriptions to a numerical representation with word embeddings (2018; Mikolov et al. 2013; Pennington, Socher, and Manning 2014) and then using that numerical representation to calculate the distances between each description – and then trains a classification model (random forest (2019)/neural network (Schmidhuber 2015)) that can predict an FNDDS label for a given IRI item.\nIn terms of data, our approach uses the FNDDS/IRI descriptions, combining them into a single “description” field, and the IRI item’s categorical items – department, aisle, category, product, brand, manufacturer, and parent company – to further discern between items.\nWhile most industrial methods require use of a graphics processing unit (graphics card, or GPU) to perform this kind of processing, our primary method only requires the computer’s internal processor (CPU) to function properly. With that in mind, to achieve the best possible performance on our test metrics, the most time-consuming operations are run in parallel. The time taken to train our primary model can likely be further improved if we parallelize these operations across a GPU, with the only downside being the imposition of a GPU requirement for systems aiming to run this method.\nIn addition to our primary method, our team has worked with alternate approaches on the GPU (using BERT (2018), neural networks (2015), etc.) to either: 1) speed up the time it takes to process and make inferences for the data, achieving similar performance on our test metrics, or 2) achieve higher performance, likely at a cost to the time it takes to process everything. Our reasoning behind doing so is that if a simple statistical model performs well, then a larger language model should be able to demonstrate a higher performance on our test metrics without much of an increase in training time. At the current time, these methods are still unable to match the performance/efficiency tradeoff of our primary method.\nAfter exploring alternate methods to no avail, our team then decided to focus again on our primary method, the random forest (2019), and a secondary method, feed-forward neural network mapping our input features (X) to the FNDDS labels (Y) (2015), to optimize their training hyperparameters for the dataset. Our aim in this is to see which of our already-implemented, easier-to-run downstream methods would better optimize the performance/efficiency tradeoff after having its training parameters optimized to the fullest. This has resulted in a marginal increase in training time (+20-30 minutes) and a roughly 5% increase in performance for our still-highest performing model, the random forest.\nOverall, our primary method – the random forest – gave us an approximate training time (including data pre-processing) of 4 hours 30 minutes for our ~38,000 IRI item training set, and an approximate inference time of 15 minutes on our testing set of ~15,000 IRI items. Furthermore, our method gave us a Success@5 score of .789 and an NDCG@5 score of .705 on our testing set.\n\nKey features\nHere is a list of the key features we utilize, along with what type of data we treat it as.\n\nFNDDS\n\nfood_code – identifier\nmain_food_description – text\nadditional_food_description – text\ningredient_description – text\n\nIRI\n\nupc – identifier\nupcdesc – text\ndept – categorical\naisle – categorical\ncategory – categorical\nproduct – categorical\nbrand – categorical\nmanufacturer – categorical\nparent – categorical\n\n\nThe intuition behind using these particular features is that the text-based descriptions provide the majority of the “meaning” of the item. By converting each description to a numerical representation (2013; 2014), we can then calculate the similarity between each “meaning” to determine which FNDDS label is most similar to the IRI item provided. However, that alone is not enough. The categorical features on the IRI item help to further enhance the model’s classifications using the logic and categories people use in places like grocery stores. For example, if given an item whose aisle was “fruit” and brand was “Dole”, the item could be reasonably expected to be something like “peaches” over something like “broccoli”.\n\n\nFeature selection\nAforementioned intuition aside, our feature selection was rather naive, in that we manually examined the data and removed any redundant text features before doing anything else. After that, we decided to use description fields as “text” data to comprise the main “meaning” of the item, represented numerically after converting the text using a word embedding (2013; 2014). We also decided to use the non-description fields (aisle, category, etc.) as “categorical” data that would be turned into its own numerical representation, allowing our model to more easily discern between items using similar systems to people.\n\n\nFeature transformations\nOur feature transformations are also relatively simple. First, we combine all description fields for each item to make one large description, and then use a word embedding method (like GloVe (2014) or BERT (2018)) to convert the description into a numerical representation, resulting in a 300-dimensional GloVe or 768-dimensional BERT vector of numbers for each description. Then, for each IRI item, we calculate the cosine and Euclidean distances from each FNDDS item, resulting in two vectors, both equal in length to the original FNDDS data (in this case, two vectors of length ~7,300). The intuition behind this is that while cosine and Euclidean distances can tell us similar things, providing both of these sets of distances to the model should allow it to pick up on a more nuanced set of relationships between the IRI and FNDDS items.\nFor categorical data, we take all unique values in each field and assign them an ID number. While that is often not the best practice for making a numerical representation out of categorical data (Potdar, Pardawala, and Pai 2017), it seemed to work for the downstream model.\nRegardless, the aforementioned feature transformations give us (ad hoc) ~14,900 features if we use GloVe and ~15,300 features if we use BERT. Both feature sets can then be sent to the downstream random forest/neural network to start classifying items.\nIt should be noted that processing the data is by far the most time-consuming part of our method. The data processing times for each embedding are as follows:\n\nGloVe: ~3 hours\nBERT: ~6 hours\n\nDue to BERT both taking so long to process data and performing lower than our GloVe embeddings on the classification task, we opt to use GloVe embeddings for our primary method. Our only theoretical explanation here is that since BERT is better at context-dependent tasks (Wang, Nulty, and Lillis 2021), it likely will expect something similar to well-structured sentences as input, which is not what the IRI/FNDDS descriptions are. Rather, GloVe – being a method that depends less on context (2013; 2014) – should excel better when the input text is not a well-formed sentence.\n\n\nTraining methods\nOnce the data has been processed, we collect the following data for each IRI item:\n\nUPC code\nDescription (converted to numerical representation)\nCategorical variables (converted to numerical representation)\nDistances to each FNDDS item\n\nOnce that has been collected for each IRI item, we can finally use our classification model. We initialize our model and begin the training process with the IRI data mentioned above and the target FNDDS labels for each one, so the model knows what the “correct” answer is for the given data. Once the model has trained on our training dataset, we save the model and it is ready for use.\nThis part of training takes much less time than preparing the data, since calculating the embeddings takes a lot more computation than a random forest model. The training times for each method are as follows:\n\nRandom Forest: ~1 hour 15 minutes\nNeural Network: ~25 minutes\n\nDespite the neural network taking far less time to train than the random forest, it still scores lower on the scoring metrics than the random forest, so we opt to continue using the random forest model as our primary method.\n\n\nGeneral approach to developing the model\nSince the linkage problem involves mapping tens of thousands of items to a smaller category set of a few thousand items, we decided to frame this problem as a multi-class classification problem (Aly 2005), where we then rank the top “k” most probable class mappings, as requested by the competition ruleset.\nMost of the usable data available to us is text data, so we need a method that can use that text-based information to accurately map classes based on the aforementioned text information. To best accomplish this, we opt to use word embedding techniques to calculate an average numerical representation for each text description (both IRI and FNDDS), so we can calculate distances between each description, giving our model a sense of how similar each description is.\n\n\nThe key “trick” to the model\nSince text descriptions hold the most information that can be used to link between an IRI item and an FNDDS item, finding a way to calculate the similarity between each description is paramount to making this method work.\nBoth distance calculation methods used in this work, cosine and Euclidean distance, are very similar in the type of information encoded, the only major difference being that cosine distance is implicitly normalized and Euclidean distance is not (Qian et al. 2004).\n\n\nNotable observations\nJust by building the ranking using the cosine similarities between each IRI item and all FNDDS items, we can achieve a Success@5 performance of 0.234 and an NDCG@5 performance of 0.312. The other features are provided and the random forest classifier is used to add some extra discriminative power to the model.\n\n\nData disclaimer\nOur current method only uses the data readily available from the 2017–2018 dataset, which we acknowledge is intended for testing. To remedy this, we further split this dataset into train/test sets and report results on our unseen test subset for our primary performance metrics. This gives a decent look into how the model will perform on unseen data.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#our-results",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Our results",
    "text": "Our results\n\nApproximate training time\nOverall, our approximate training time for our primary method is 4 hours 30 minutes broken down (approximately) as follows:\n\nReading data from database: 30 seconds\nCalculating ~7,300 FNDDS description embeddings: 15 minutes 45 seconds\nCalculating ~38,000 IRI description embeddings and similarity scores: 2 hours 20 minutes 45 seconds\nFormatting calculated data for the random forest classifier: 35 minutes\nTraining the random forest classifier: 1 hour 15 minutes\n\n\n\nApproximate inference time\nOur approximate inference time for our primary method is 15 minutes to make inferences for ~15,000 IRI items.\n\n\nS@5 & NDCG@5 performance\nThis is how our best-performing model (GloVe + random forest) performs at the current time on the testing set:\n\nNDCG@5: 0.705\nSuccess@5: 0.789\n\nWhen we evaluate that same model on the full PPC dataset we were provided (~38,000 items), we get the following scores:\n\nNDCG@5: 0.879\nSuccess@5: 0.916\n\n(Note: The full PPC dataset contains approximately 15,000 items that we used to train the model, so these scores are not as representative of our method’s performance as the previous scores.)"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#future-workrefinement",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nAs mentioned previously, we only used the given 2017–2018 PPC dataset as our primary dataset for both training and testing. Going forward, we would like to include datasets from previous years as well, which we believe would further increase our model performance. Additionally, the datasets generated from this research have the potential to inform and support additional studies from a variety of perspectives, including nutrition, consumer research, and public health. Further research utilizing these datasets has the potential to make significant contributions to our understanding of consumer behavior and the role of food and nutrient consumption in overall health and well-being."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "href": "instruments_old/case-studies/posts/2023/08/21/03-first-place-winners.html#lessons-learned",
    "title": "Food for Thought: First place winners – Auburn Big Data",
    "section": "Lessons learned",
    "text": "Lessons learned\nIt was interesting that the random forest model performed better than the vanilla neural network model. This shows that a simple solution can work better, depending on the application. This observation is in line with the well-established principle in machine learning that the choice of model should be guided by the nature of the problem and the characteristics of the data. In this case, the random forest model, being a simpler and more interpretable model, was better suited to the problem at hand and was able to outperform the more complex neural network model. These results underscore the importance of careful model selection and the need to consider both the complexity of the model and the specific requirements of the problem when choosing an algorithm for a particular application.\n\n\n\n\n← Part 2: Competition design\n\n\n\n\nPart 4: Second place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAlex Knipper and Naman Bansal are PhD students, and Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker are assistant professors at Auburn University.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alex Knipper, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by nrd on Unsplash.\n\n\n\nHow to cite\n\nKnipper, Alex, Naman Bansal, Jingyi Zheng, Wenying Li, and Shubhra Kanti Karmaker. 2023. “Food for Thought: First place winners – Auburn Big Data.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/06-value-of-competitions.html",
    "title": "Food for Thought: The value of competitions for confidential data",
    "section": "",
    "text": "We are witnessing a sea change in data collection practices by both governments and businesses – from purposeful collection (through surveys and censuses, for example) to opportunistic (drawing on web and social media data, and administrative datasets). This shift has made clear the importance of record linkage – a government might, for example, look to link records held by its various departments to understand how citizens make use of the gamut of public services.\nHowever, creating manual linkages between datasets can be prohibitively expensive, time consuming, and subject to human constraints and bias. Machine learning (ML) techniques offer the potential to combine data better, faster, and more cheaply. But, as the recently released National AI Research Resources Task Force report highlights, it is important to have an open and transparent approach to ensure that unintended biases do not occur.\nIn other words, ML tools are not a substitute for thoughtful analysis. Both private and public producers of a linked dataset have to determine the level of linkage quality – such as what precision/recall tradeoff is best for the intended purpose (that is, the balance between false-positive links and failure to cover links that should be there), how much processing time and cost is acceptable, and how to address coverage issues. The challenge is made more difficult by the idiosyncrasies of heterogeneous datasets, and more difficult yet when datasets to be linked include confidential data (Christensen and Miguel 2018; Christen, Ranbaduge, and Schnell 2020).\nAnd, of course, an ML solution is never the end of the road: many data linkage scenarios are highly dynamic, involving use cases, datasets, and technical ecosystems that change and evolve over time; effective use of ML in practice necessitates an ongoing and continuous investment (Koch et al. 2021). Because techniques are constantly improving, producers need to keep abreast of new approaches. A model that is working well today may no longer work in a year because of changes in the data, or because the organizational needs have changed so that a certain type of error is no longer acceptable. As Sculley et al. point out, “it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning” (Sculley et al. 2014).\nAlso important is that record linkage is not seen as a technical problem relegated to the realm of computer scientists to solve. The full engagement of domain experts in designing the optimization problem, identifying measures of success, and evaluating the quality of the results is absolutely critical, as is building an understanding of the pros and cons of different measures (Schafer et al. 2021; Hand and Christen 2018). There will need to be much learning by doing in “sandbox” environments, and back and forth communication across communities to achieve successful outcomes, as noted in the recommendations of the Advisory Committee on Data for Evidence Building (a screenshot of which is shown in Figure 1).\n\n\nFigure 1: A recommendation for building an “innovation sandbox” as part of the creation of a new National Secure Data Service in the United States.\n\nDespite the importance of trial and error and transparency about linkage quality, there is no handbook that guides domain experts in how to design such sandboxes. There is a very real need for agreed-upon, domain-independent guidelines, or better yet, official standards to evaluate sandboxes. Those standards would define “who” could and would conduct the evaluation, and help guarantee independence and repeatability. And while innovation challenges have been embraced by the federal government, the devil can be very much in the details (Williams 2012).\nIt is for this reason that the approach taken in the Food for Thought linkage competition, and described in this compendium, provides an important first step towards a well specified, replicable framework for achieving high quality outcomes. In that respect it joins other recent efforts to bring together community-level research on shared sensitive data (MacAvaney et al. 2021; Tsakalidis et al. 2022). This competition, like those, helped bring to the foreground both the opportunities and challenges of doing research in secure sandboxes with sensitive data. Notably, these exercises highlight a kind of cultural tension between secure, managed environments, on the one hand, and unfettered machine learning research, on the other. The need for flexibility and agility in computational research bumps up against the need for advance planning and careful step-by-step processes in environments with well-defined data governance rules, and one of the key lessons learned is that the tradeoffs here need to be recognized and planned for.\nThis particular competition was important for a number of other reasons. Thanks to its organization as a competition, complete with prizes and bragging rights for strongly performing teams, it attracted new eyes from computer science and data science to think about how to address a critical real-world linkage problem. It offered the potential to produce approaches that were scalable, transparent, and reproducible. The engagement of domain experts and statisticians meant that it will be possible to conduct an informed error analysis, to explicitly relate the performance metrics in the task to the problem being solved in the real world, and to bring in the expertise of survey methodologists to think about the possible adjustments. And because it identified different approaches of addressing the same problem, it created an environment for new innovative ideas.\nMore generally, in addition to the excitement of the new approaches, this exercise laid bare the fragility of linkages in general and highlighted the importance of secure sandboxes for confidential data. While the promise of privacy preserving technologies is alluring as an alternative to bringing confidential data together in one place, such approaches are likely too immature to deploy ad hoc until a better understanding is established of how to translate real-world problems and their associated data into well-defined tasks, how to measure quality, and particularly how to assess the impact of match quality on different subgroups (Domingo-Ferrer, Sánchez, and Blanco-Justicia 2021). The scientific profession has gone through too painful a lesson with the premature application of differential privacy techniques to ignore the lessons that can be learned from a careful and systematic analysis of different approaches (2021; Van Riper et al. 2020; Ruggles et al. 2019; Giles et al. 2022).\nWe hope that the articles in this collection provide not only the first steps towards a handbook of best practices, but also an inspiration to share lessons learned, so that success can be emulated, and failures understood and avoided.\n\n\n\n\n← Part 5: Third place winners\n\n\n\n\nFind more case studies\n\n\n\n\n\n\n\n\nAbout the authors\n\nSteven Bedrick is an associate professor in Oregon Health and Science University’s Department of Medical Informatics and Clinical Epidemiology.\n\n\nOphir Frieder is a professor in Georgetown University’s Department of Computer Science, and in the Department of Biostatistics, Bioinformatics & Biomathematics at Georgetown University Medical Center.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative.\n\n\nPhilip Resnik holds a joint appointment as professor in the University of Maryland Institute for Advanced Computer Studies and the Department of Linguistics, and an affiliate professor appointment in computer science.\n\n\n\nCopyright and licence\n\n© 2023 Steven Bedrick, Ophir Frieder, Julia Lane, and Philip Resnik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Alexandru Tugui on Unsplash.\n\n\n\nHow to cite\n\nBedrick, Steven, Ophir Frieder, Julia Lane, and Philip Resnik. 2023. “Food for Thought: The value of competitions for confidential data.” Real World Data Science, August 21, 2023. URL\n\n\n\n\n\n\n\n\n\nReferences\n\nChristen, P., T. Ranbaduge, and R. Schnell. 2020. Linking Sensitive Data - Methods and Techniques for Practical Privacy-Preserving Information Sharing. Springer. https://doi.org/10.1007/978-3-030-59706-1.\n\n\nChristensen, G., and E. Miguel. 2018. “Transparency, Reproducibility, and the Credibility of Economics Research.” Journal of Economic Literature 56 (3): 920–80. https://doi.org/10.1257/jel.20171350.\n\n\nDomingo-Ferrer, J., D. Sánchez, and A. Blanco-Justicia. 2021. “The Limits of Differential Privacy (and Its Misuse in Data Release and Machine Learning).” Communications of the ACM 64 (7): 33–35. https://doi.org/10.1145/3433638.\n\n\nGiles, O., K. Hosseini, G. Mingas, O. Strickson, L. Bowler, C. Rangel Smith, H. Wilde, et al. 2022. “Faking Feature Importance: A Cautionary Tale on the Use of Differentially-Private Synthetic Data.” https://arxiv.org/abs/2203.01363.\n\n\nHand, D., and P. Christen. 2018. “A Note on Using the f-Measure for Evaluating Record Linkage Algorithms.” Statistics and Computing 28 (3): 539–47. https://doi.org/10.1007/s11222-017-9746-6.\n\n\nKoch, B., E. Denton, A. Hanna, and J. G. Foster. 2021. “Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research.” CoRR abs/2112.01716. https://arxiv.org/abs/2112.01716.\n\n\nMacAvaney, S., A. Mittu, G. Coppersmith, J. Leintz, and P. Resnik. 2021. “Community-Level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task.” In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, 70–80. Online: Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.clpsych-1.7.\n\n\nRuggles, S., C. Fitch, D. Magnuson, and J. Schroeder. 2019. “Differential Privacy and Census Data: Implications for Social and Economic Research.” AEA Papers and Proceedings 109 (May): 403–8. https://doi.org/10.1257/pandp.20191107.\n\n\nSchafer, K. M., G. Kennedy, A. Gallyer, and P. Resnik. 2021. “A Direct Comparison of Theory-Driven and Machine Learning Prediction of Suicide: A Meta-Analysis.” PLOS ONE 16 (4): 1–23. https://doi.org/10.1371/journal.pone.0249833.\n\n\nSculley, D., G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. 2014. “Machine Learning: The High Interest Credit Card of Technical Debt.” In SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop).\n\n\nTsakalidis, A., J. Chim, I. M. Bilal, A. Zirikly, D. Atzil-Slonim, F. Nanni, P. Resnik, et al. 2022. “Overview of the CLPsych 2022 Shared Task: Capturing Moments of Change in Longitudinal User Posts.” In Proceedings of the Eighth Workshop on Computational Linguistics and Clinical Psychology, 184–98. Seattle, USA: Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.clpsych-1.16.\n\n\nVan Riper, D., T. Kugler, J. Schroeder, and S. Ruggles. 2020. “Differential Privacy and Racial Residential Segregation.” In 2020 APPAM Fall Research Conference.\n\n\nWilliams, H. 2012. “Innovation Inducement Prizes: Connecting Research to Policy.” Journal of Policy Analysis and Management 31 (3): 752–76. http://www.jstor.org/stable/41653827."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "",
    "text": "DeepFFTLink team members: Yang Wu and Kai Zhang are PhD students at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student at Indiana University Bloomington. Xuhong Zhang is an assistant professor at Indiana University Bloomington. Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#perspective-on-the-challenge",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Perspective on the challenge",
    "text": "Perspective on the challenge\nText matching is an essential task in natural language processing (NLP, Pang et al. 2016), while record linkage across different sources is an essential task in data science. Machine learning techniques allow people to combine data faster and cheaper than using manual linkage. However, in the context of the Food for Thought challenge, existing methods for matching universal product codes (UPCs) to ensemble codes (ECs) require every UPC to be compared with every EC code (Figure 1a). Such approaches can be computationally expensive in the training process when data is noisy. Here, we propose an ensemble model with a category-based adapter to tackle this problem, drawing on the category information included in UPC and EC data. The category-based adapter allows UPCs to be first matched with only a small and reliable set of ECs (Figure 1b). Then, an ensemble model will be deployed to make predictions for UPC-EC matching. Our proposed approach can achieve competitive performance compared with state-of-the-art models.\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 1: A toy example of our method. Panel (a) shows the traditional matching method, while (b) is our proposed ensemble model with category-based adapter. With the help of the adapter, UPC 1 only needs to be matched with EC 1 and EC 3."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#our-approach",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our approach",
    "text": "Our approach\nWe propose a two-step framework to address this problem. To begin with, we use a category-based adapter to get reliable candidate ECs for each UPC. Then, an ensemble model (Dietterich 2000) is deployed to make a prediction for each UPC-EC pair.\n\nCategory-based adapter\nBy using 2015–2016 UPC-EC data, we created a knowledge base, which is a UPC category–EC pair-wised table for generating candidate ECs. Within this setting, each UPC category is, on average, related to only 32 ECs. This knowledge base is then used as context to further filter the candidate ECs. Note that there are some new ECs generated year by year, which can also be part of the potential ECs in the UPC-EC matching task, since the contextual information of new ECs does not exist in our knowledge base.\n\n\nEnsembled model\nWe ensemble the base-string match and BERT models. BERT is a deep learning model for natural language processing (Devlin et al. 2018). In the base-string match model, we used the Term Frequency-Inverse Document Frequency (TFIDF) of each UPC and EC description as features to calculate a pairwise cosine similarity, which is a distance between instances. Meanwhile, we used features extracted from UPC and EC descriptions to fine-tune the BERT base model and calculated the cosine similarity of embeddings between each UPC and EC. Then we rank ECs based on their similarity scores with the UPC.\n\n\nFigure 2: The framework of our proposed model. A two-step strategy is used to make the final prediction.\n\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#our-results",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Our results",
    "text": "Our results\nWe randomly selected 500 samples from the 2017–2018 UPC-EC data to train the ensembled weight for each model. Two functions were adapted to make a fusion of base-string and BERT models:\n\\[\nC = a * X + b * Y  \n\\tag{1}\\]\n\\[\nC =  a * log(X) + b * log(Y) \\text{. }\n\\tag{2}\\]\n\\(C\\) denotes the final confidence score. \\(X\\) and \\(Y\\) represent base_string_similarity_score and BERT_similarity_score, respectively. \\(a\\) and \\(b\\) are corresponding model weights for base_string and BERT models.\nA better Success@5 is achieved with function (1). The ensembled weights for the base-string model and BERT model are 0.738 and 0.262, respectively. The experiment result indicates that the base_string model contributes more than the BERT model when the ensemble model makes predictions. The prediction result for the 2017–2018 data is:\n\nSuccess@5: 0.727\nNDCG@5: 0.528\n\nComputation time is 6 hours."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#future-work",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Future work",
    "text": "Future work\nOur next step will focus on adding the newly generated EC data into our knowledge base, which allows the model to be more stable to make predictions for UPC-EC matching. Our model is an unsupervised method, which does not need labels for each instance. We use cosine similarity to rank the matches, so no labels are needed in the training process. However, our future work will try to label some instances to handle the UPC-EC matching task in a supervised manner."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "href": "instruments_old/case-studies/posts/2023/08/21/04-second-place-winners.html#lessons-learned",
    "title": "Food for Thought: Second place winners – DeepFFTLink",
    "section": "Lessons learned",
    "text": "Lessons learned\n\nIf the data is not complex, simple models may outperform complex models. For example, in our experiment, we found that the base-string model outperforms single RoBERTa (Liu et al. 2019) or BERT models. However, our ensemble model can outperform each individual model since model fusion allows information aggregation from multiple models.\nMulti-label models may not work well on UPC-EC data. In our early work, we tried to consider the UPC-EC matching task as a multi-label problem, e.g., we labeled each EC as a binary label which indicated whether the EC was an appropriate match or not. We mapped UPC and EC pairs into a multi-label table. However, we find that the UPC and EC keeps a one-to-one relation for most UPCs. The model performance of a multi-label model, i.e., Label-Specific Attention Network (LSAN, Xiao et al. 2019), is lower than base-string model on both Success@5 and NDCG@5 metrics.\n\n\n\n\n\n← Part 3: First place winners\n\n\n\n\nPart 5: Third place winners →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYang Wu and Kai Zhang are PhD students, and Xiaozhong Liu is an associate professor at Worcester Polytechnic Institute. Aishwarya Budhkar is a PhD student and Xuhong Zhang is an assistant professor at Indiana University Bloomington.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yang Wu, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Hanson Lu on Unsplash.\n\n\n\nHow to cite\n\nWu, Yang, Aishwarya Budhkar, Kai Zhang, Xuhong Zhang, and Xiaozhong Liu. 2023. “Food for Thought: Second place winners – DeepFFTLink.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#background",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#background",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Background",
    "text": "Background\nDecisions around medium and long-term allocation of healthcare resources are fraught with challenges and uncertainties, which explains the use of blunt resource allocations based on across-the-board annual percentage uplifts.\nThe Bristol, North Somerset, South Gloucestershire Integrated Care Board (BNSSG ICB - we love elaborate acronyms in the National Health Service!), in the south west of England, is part of the local NHS apparatus responsible for planning the current and future health needs of the one million resident population.\n\n\n\n\n  \n\n\n\nFigure 1: A map of the area covered by BNSSG, a space covered by three local authorities, with about 1 million people living inside it."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#population-segmentation",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Population Segmentation",
    "text": "Population Segmentation\nBefore tackling the complex problem of forecasting healthcare resources into the future, we first need to understand the current situation regarding the distribution of health needs.\nWhile every individual has a unique set of circumstances, population segmentation is an approach used to help understand overall need by combining individuals into different groups, based on certain criteria.\nWe use the Cambridge Multimorbidity Score which is a metric designed to summarise the presence of multiple health conditions, known as multimorbidity. Using that score, which applies different weights to different health conditions, we previously found a way of splitting the adult (17+) population into five Core Segments, with Core Segment 1 patients having the lowest score and being the least ill and Core Segment 5 being those with the most multimorbidity.\nApplied to the BNSSG adult population (of around 750K individuals), the following interesting properties were found:\n\nHalving: Going up one segment results in roughly half the number of people in that segment\nDoubling: Going up one segment results in roughly twice the NHS monetary spend per person per year\n\nWe can see this in Figure 2.\n\n\n\n\n\n\nFigure 2: Halving-Doubling Effect of the Core Segments"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#sec-creating-the-model",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Creating The Model",
    "text": "Creating The Model\nTo forecast health needs of the population, in terms of how many people will be in which Core Segment in what future year, the Dynamic Population Model (DPM) takes information from two different sources:\n\nThe Office for National Statistics projections for our area. From this, we get yearly projections for not just the total 17+ population, but also the predicted number of people turning 17 (and so entering our model), deaths, and in- and out-ward migration.\nNHS patient attribute and activity data, stored in the System Wide Dataset (SWD). This gives us: past and current information on the adult population’s NHS healthcare usage; the Core Segment breakdown of our current and past populations; the proportion of those turning 17, migrating, and dying that are in each Core Segment. From this, we estimate the historical rates of transition within Core Segments, which is essentially the yearly number of people getting sicker or healthier.\n\nBy synthesising these pieces of data, we create our DPM forecast. Starting from the most up to date Core Segment population breakdown, the model takes yearly time steps into the future, at each time step using the inputs to estimate how many people are to be in each Core Segment. This modelling approach of having discrete time steps and different movements between states can be set up as a Markov chain, although here we have formulated it as a set of difference equations - through which the outflow of each Core Segment population at each time step is deterministic. The design was led by Zehra and Christos, through a collaboration between the NHS and the Centre for Healthcare Innovation and Improvement (CHI2) at the University of Bath.\nThe model can be thought of as having the following inputs:\n\n\n\n\n\n\n\n\nModel Input\nDescription\nData Source\n\n\n\n\ninitial population\nThe starting number of people in each Core Segment\nSWD\n\n\ninner transition matrix\nThe yearly proportions of people moving from one Core Segment to another\nSWD\n\n\nbirths, net migration, deaths - numbers\nThe yearly number of people moving in and out of the area\nONS\n\n\nbirths, net migration, deaths - proportions\nThe proportion of births/migrations/deaths that come from each Core Segment group\nSWD\n\n\n\nFrom these inputs, it deterministically outputs the yearly forecasts for the number of people in each Core Segment. From these yearly Core Segment population figures, we can also forecast use by point of delivery by taking historic SWD information on the activity used by current Core Segment breakdown, under the assumption that stays the same into the future.\nWe combine these population health segment projections – i.e., how many people will be in which Core Segment in what future year – with recent NHS healthcare usage data to yield forecasted changes for various delivery points, like Emergency Department (ED) visits or maternity service appointments."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#findings",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#findings",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Findings",
    "text": "Findings\nThe first output of the model is the population forecast for each Core Segment, as plotted in Figure 3. The visualisation is a type of sankey diagram called an alluvial plot, which shows the proportion of people moving between the Core Segments each year. As it is to be expected, the majority of individuals stay in the same Core Segment year-on-year as the process of acquiring conditions and developing multimorbidity takes places over many years and decades.\nThe concerning insight shown in Figure 3 is that all Core Segments apart from (the most healthy) Core Segment 1 are due to increase in size, with Core Segment 5 having the largest percentage increase over the next 20 years. While, at first glance, this could be attributed to the effect an ageing population, in which people are staying alive for longer we will see in the next set of results that this itself does not wholly explain the forecasted Core Segment changes.\n\n\n\n\n\n\nFigure 3: All Core Segments, except the most healthy (CS1), are forecast to increase in size. BNSSG Population rescaled to have an initial population of 1,000.\n\n\n\nIn applying the typical NHS healthcare usage per Core Segment to the projections of Figure 3, we derive the expected future healthcare usage for various healthcare settings (Figure 4). In overlaying to these the equivalent projections due solely to demographic factors (both for total population size and capturing the effect of Age and Sex), we see that the DPM projections for increased resource use are not solely attributable to an ageing and growing population, but also to a population becoming gradually less healthy over time.\nSpecifically, from Figure 4 we can glean the following insights:\n\nIn all areas except Maternity, the DPM forecasts an increased use beyond just the growing, aging population. The reason that Maternity can be explained as the exception is due to it closely following the demographic changes forecast, specifically for numbers of women of child bearing age.\nFor Community contacts, with the highest proportion of use from Core Segment 5 patients, the DPM forecasts the highest increase into the future. This is because, relative to current size, the number of Core Segment 5 patients is set to increase the largest and so that has the largest impact on Community contacts, which include home visits to patients to support rehabilitation and services to manage long-term mobility issues such as physiotherapy.\nWhilst Secondary Elective and Non-Elective activity is forecast to grow at similar rates, the Carbon and Cost values are forecast to grow more for Secondary Non-Elective due to the average Carbon and Cost usage per person in Core Segment 5 being higher. In this context ‘Secondary’ is a hospital stay, with ‘Elective’ being planned and ‘Non-Elective’ being unplanned. For example, a hip replacement is elective whereas an admission following a road traffic accident is non-elective.\n\n\n\n\n\n\n\nFigure 4: Forecasts by activity, carbon, and cost for four different points of delivery."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#limitations",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#limitations",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Limitations",
    "text": "Limitations\n\nIt’s difficult to make predictions, especially about the future.\n– Danish Proverb\n\nAs with any modelling / forecasting method, there are limitations to be mindful of.\n\nThe cost and activity usage estimates are made under the assumption that we will continue to deliver services as they are currently being delivered. We know this isn’t going to be true, as healthcare-seeking behaviour evolves over time, with younger people accessing healthcare in different ways to previous generations. On top of that, healthcare advances can result in significant changes in healthcare provision, in ways unaccounted for within this model.\nThe model is tied to ONS forecasts for population change, and robust forecasting is hard. It is difficult to estimate what the population will look like in 20 years’ time, and the influence of uncertain and unknown future local development and housing plans. Having said this, population forecasts tend to be robust, one way to consider this is that everyone who will be an adult by the end of the forecast in 20 years’ time has already been born.\nThe DPM does not explicitly account for the interaction of demand and capacity: it simply predicts future healthcare resource requirement assuming that health needs of a given Core Segment patient are met in the same way they are met now. This is an essential assumption to help ensure legitimate use of the empirically derived Core Segment transition rates. However, it inevitably limits practical use, as flexing demand and capacity assumptions is of importance to planners and service managers.\nIt is not possible to validate the model on historic data, firstly because of point 3. above but also because we only have good quality SWD information for the past two years, so cannot reliably look further back into the past and create a forecast that we can check against what actually happened.\nWhilst it is possible to use the model in other healthcare systems and geographic areas, the underlying data required to generate the Core Segments is non-trivial, so significant data pipelining may be required to get to create local model inputs, as explained above in Section 3."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#what-next",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#what-next",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "What Next",
    "text": "What Next\nWe have already generated local use cases for the DPM in forecasting different geographical areas or specific hospital trusts. We envisage the DPM becoming a standard tool in most forward planning initiatives and will continue to refine the model as more information becomes available both for calibration and validation.\nOutside of BNSSG, we are keen to disseminate our modelling approach to others who may be interested, as well as expanding our collaboration. There are also other innovative approaches in this space, such as the Health in 2040 report by the Health Foundation which looks at England-level and uses the same ONS forecasts, but using a different ‘micro simulation’ modelling approach.\n\nIf long-term forecasting in the NHS is of interest to you and your work, we’d love to chat! Please get in touch at bnssg.analytics@nhs.net"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/05/08/dpm.html#summary",
    "href": "instruments_old/case-studies/posts/2024/05/08/dpm.html#summary",
    "title": "Forecasting the Health Needs of a Changing Population",
    "section": "Summary",
    "text": "Summary\nReliably forecasting longer-term population health needs and healthcare resource requirements is essential if the NHS is to effectively plan for tomorrow’s problems today.\nWhile this is undoubtedly a difficult problem – both conceptually and statistically – our modelling, undertaken through an academic-NHS collaboration, demonstrates that there are alternatives beyond the commonly-used but simplistic approaches based only on demographic factors.\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nLuke Shaw is a Data Scientist working in the NHS.\n\n\nRich Wood is Head of Modelling Analytics at BNSSG ICB and Senior Visiting Research Follow at University of Bath School of Management.\n\n\nChristos Vasilakis is Director of the Centre for Healthcare Innovation and Improvement (CHI2), and Professor at the University of Bath School of Management.\n\n\nZehra Onen Dumlu is a Research Associate at CHI2 and Lecturer at the University of Bath.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Luke Shaw\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nShaw, Luke et al 2024. “Forecasting the Health Needs of a Changing Population” Real World Data Science, May 08, 2024. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html",
    "href": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this article are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html#summing-it-up",
    "href": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html#summing-it-up",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "1 Summing it up",
    "text": "1 Summing it up\nWe end where we began in the first article of our series. Through this four-part series, we introduced a Curated Data Enterprise (CDE) Framework (see Figure 1) that can guide the development and dissemination of statistics broadly applicable to addressing social and economic issues while ensuring replicability and reusability. The CDE provides the scaffold for scaling the statistical product development of interest to the US Census Bureau and broadly applies to official statistics agencies (Keller et al. 2022). We illustrated this through a use case on climate resiliency of skilled nursing facilities, highlighting the replicability and reusability of the capabilities that would benefit inclusion in a CDE.\n\n\n\n\n\n\nFigure 1: The CDE Framework starts with the purposes & uses of the statistical products. The outer rectangle identifies the guiding principles for ethical, transparent, reproducible statistical product development and dissemination. The inner rectangle identifies the statistical product development steps.\n\n\n\nAs noted in the first three articles, the process begins with articulating purposes and uses through stakeholder engagement and continues by leveraging that engagement, including subject matter expertise, to inform statistical product development. Eliciting purposes and uses from stakeholders and data users is facilitated by asking questions such as:  \n\nWhat questions keep you awake at night because you don’t have data insights to address them? What are those purposes and uses that you need statistical products to support?\nHow do we collaborate and engage with you to better understand your needs and help you identify gaps in understanding regarding purpose and use?\nHow do we prioritize what statistical products to develop first?\n\nExamples of purposes and uses that drive new statistical products include accurately measuring gig employment (Salvo, Shipp, and Zhang 2022a), migration due to extreme climate events (Salvo, Shipp, and Zhang 2022b), the various dimensions of housing affordability (Wu et al. 2023), and addressing the undercount of young children (Salvo, Lancaster, and Shipp 2023). Other topics that require multiple sources and types of data include creating a household living budget based on the minimum necessary to ensure an adequate standard of living (Lancaster et al. 2023) and using this budget as a starting point for measuring insecurity across components such as food or housing (Montalvo et al. 2023)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html#developing-an-end-to-end-e2e-curation-system",
    "href": "instruments_old/case-studies/posts/2024/11/22/development-plan-2.html#developing-an-end-to-end-e2e-curation-system",
    "title": "Defining Purposes and Uses to Support the Development of Statistical Products in a 21st Century Census Curated Data Enterprise Environment",
    "section": "2 Developing an end-to-end (E2E) curation system",
    "text": "2 Developing an end-to-end (E2E) curation system\nPurposes and uses defined in use cases are important to support the rapid development of statistical products. These use cases will capture the imagination of those working to address today’s critical issues and advance public understanding and trust in federal statistics. The above paragraph provides examples of purposes and uses for which we have developed use cases.\nUse cases are a powerful mechanism to promote methodological research to develop and implement capabilities needed in a CDE. The objectives are to undertake research projects that have the potential to create statistical products with explicit purposes and uses that will exercise the end-to-end (E2E) curation components.\nWhen implemented, these proposed use cases will demonstrate a sequence of capabilities needed to build the CDE, such as agile data discovery, reusing modules and data (including synthetic data), tracking the provenance of collected and generated data, reusing synthetic data and methods to integrate many types of data, conducting statistical analysis involving heterogeneous data integration, and reviewing data and statistical results with an equity and ethics lens. These steps will be captured in an end-to-end curation system.\n\nCriteria for developing and evaluating use cases that will uncover the capabilities and research necessary to develop the CDE\n\nCriteria are needed to evaluate, and partner with researchers and stakeholders in developing and implementing the capabilities to capture in the CDE. The choice of use cases, when curated, needs to provide unique insight into CDE capabilities and statistical product development. The capabilities to be developed include addressing some purpose and use that no single source of information can resolve, generating practical diagnostics to improve existing methods, creating pilot software, and validating new and improved statistical products. These criteria, developed through listening sessions and discussions with experts, guide the prioritization and selection of use cases and their evaluation after curation (see Table 2) (Keller et al. 2022).\n\nTable 2. Criteria for Selecting and Prioritizing Use Cases to Identify CDE Capabilities\n\n\n\n\n\nValue and feasibility of the CDE approach described in the existing research (potential use case) to address emerging or long-standing issues, ie, its purpose and use over and above existing approaches to address high-priority problems. | | Stakeholders’ challenges and issues as the source of purposes and uses. | | Subject matter experts to advise on the approach and implementation. | | Partners to access data from local and state governments, non-profit organizations, and the private sector, and strategies to overcome legal and administrative barriers to such access that benefits to both the providers and recipients of the data. | Survey, administrative, opportunity, and procedural data from multiple sources (eg, local, state, federal, third-party) to address the purpose and use (issue) in an integrated way. There are well-defined data ingestion and governance requirements. | | Computation and measurement requirements for statistical products include the unit(s) of analysis and their characteristics, temporal sequence, geocoded location data, and methods for imputations, projections, and statistical analysis. | | Equity and ethical dimensions are considered at each step to ensure that the use case provides fair and accurate representation across groups and an assessment that the potential benefits outweigh the potential harm. | | Evidence of CDE capabilities to be built, including the code, data, and documentation to create the statistical products, which can be described in the curation step. | | Statistical products include integrated data sources, indicators, maps, visualizations, storytelling and analysis. | | Potential viability of proposed dissemination platforms for interactive access to data products at all levels of data acumen (Keller and Shipp 2021) while adhering to confidentiality and privacy rules. |\n\n\n\n\nAn end-to-end curation process\n\nCuration is an end-to-end process defined by the context of the purposes and uses that document the decisions and trade-offs at each step in the CDE Framework. The following curation definition will be used as it serves the CDE’s vision.\nCuration involves documenting, for each statistical product, the inputs from which the product is derived, the wrangling used to transform the information into product, and the statistical product itself. Purposes and uses provide the context for each statistic and statistical product.\nThis definition has evolved from numerous stakeholder discussions via listening sessions and discussions with Census Bureau staff. (Nusser et al. forthcoming; Faniel, Frank, and Yakel 2019; NASEM 2022).\nAs use cases are curated, the CDE capabilities will evolve to quickly develop statistical products. These curated use cases are integral to developing an E2E curation process for the CDE.  \n\nInvitation to contribute purpose and use ideas for developing new statistical products\n\nThe CDE development aims to curate a significant number of use cases that address social and economic issues that have the potential to define capabilities to be built in the CDE. Initially, they are seeking ideas for purposes and uses to define these use cases and statistical products.\nThe skilled nursing facility use case included code, data, and documentation to calculate the probability of workers getting to work during a weather event, resilience indicators at the county or sub-county level, alternative skilled nursing home deficiency measures, and other capabilities.\nIncorporating capabilities in the CDE\nTo accelerate the development of statistical products, the Census Bureau will develop use cases to articulate and create CDE capabilities. This requires identifying those valuable nuggets for learning and quickly translating and incorporating this information into the CDE. Examples of critical capabilities of interest are learning about the utility of synthetic data, the ability to aggregate data into custom geographies, and combining different units of analysis. The expected outcome is the creation of an innovative 21st Century Census Curated Data Enterprise focused on purposes and uses that overcome the limitations and challenges of today’s survey-alone model.  \nThe 21st Century Census Curated Data Enterprise development presents an opportunity for researchers to help drive the development of the CDE as the foundation for creating new statistical products. The US Census Bureau is seeking ideas for purposes and uses that will define new statistical products. They are interested in research projects (use cases) that are guided by the CDE framework as potential new statistical products. They want to learn from and understand your experiences in using the CDE framework, for example, what worked well, what challenges you faced, how each step in the framework was curated, and what capabilities are replicable and reusable for developing and enhancing statistical products.\n\n\n\n\n← Part 3: Climate resiliency of skilled nursing facilities\n\n\n\n\n\n\n\n\nAbout the authors\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Lukas Blazek on Unsplash.\n\n\n\nHow to cite\n\nShipp S, Salvo J, Lancaster V (2024). “Statistical Products in a 21st Century Census Curated Data Enterprise Environment” Real World Data Science, November 22, 2024. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html",
    "href": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this perspective are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#introduction",
    "href": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#introduction",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Introduction",
    "text": "Introduction\nToday, official statistics – tables, reports and microdata – are produced using data from a single survey. These surveys are foundational for researchers and policymakers. However, many issues cannot be answered by surveys alone. For example, creating a picture of how prepared skilled nursing facilities (SNFs) are for climate emergencies requires wrangling all types of data about the facilities and their communities.(Note: A skilled nursing facility is a facility that meets specific federal regulatory certification requirements that enable it to provide short-term inpatient care and services to patients who require medical, nursing, or rehabilitative services.) This includes SNF data on the number and dates of inspections, deficiencies, residents’ mental and physical health, the number of nursing staff and where they live, community assets data on the number of shelter facilities, health professionals and emergency service providers, and community risks data on the probability of an extreme climate event. How can we create new statistical products useful to policymakers, emergency responders, skilled nursing facility staff, and others to inform their decisions?\n\n\n\n\n\n\nOfficial statistics\n\n\n\nOfficial statistics are essential for a democratic society as they provide economic, demographic, social, and environmental data about the government, the economy, and the environment. Official statistical agencies should compile and make these statistics available impartially to honor the right to public information.\nObjective, reliable, and accessible official statistics instill confidence in the integrity of government and public decision-making regarding a country’s economic, social, and environmental situation at national and international levels. They should be widely available and meet the needs of various users (United Nations 2024).\n\n\nWith the explosion of available data, there is an opportunity to combine all types of information to create statistical products that address cross-cutting topics for a wide range of purposes and uses. The US Census Bureau is modernizing and transforming its enterprise system to accommodate a new way to produce statistical products that take advantage of all data types: designed surveys and censuses, public and private administrative data, opportunity data scraped from the internet, and procedural data (Keller et al. 2022).\n\n‘We are moving towards a single enterprise, data-centric operation that enables us to funnel data from many sources in a single data lake using common collection and ingestion platforms… This is the essence of a curated data approach — assemble, assess, and fill in the gaps to create quality statistical data.’\n\n\nRobert Santos, Director, US Census Bureau\n\nThis curated approach is embodied in the Curated Data Enterprise (CDE). The Curated Data Enterprise Framework in Figure 1 provides a guide for creating statistical products that enable the full integration of data from many sources (Keller et al. 2020). At the heart of the framework are the purposes and uses that provide the context and driving force for developing the statistical product. The outer rectangle in Figure 1 identifies the guiding principles for ethical, transparent and reproducible product development and dissemination. The inner rectangle identifies the steps in the statistical product development, including integrating primary and secondary data sources. The arrows convey that this process may only sometimes be linear. Instead, the process is iterative, where new information may be discovered at any point, requiring reevaluating and updating prior steps. Our Social and Decision Analytics research group in the Biocomplexity Institute developed, tested, and refined the CDE (data science) Framework in our research since 2013 (Keller, Lancaster, and Shipp 2017; Keller et al. 2020). The proposed use of the CDE to develop statistical products at the US Census Bureau is in its early stages.\n\n\n\n\n\n\nFigure 1: The CDE Framework starts with the purposes & uses of the statistical products. The outer rectangle identifies the guiding principles for ethical, transparent, reproducible statistical product development and dissemination. The inner rectangle identifies the statistical product development steps.\n\n\n\nThe next article in this series will put the CDE Framework into practice by demonstrating the use case on skilled nursing facilities’ preparedness for emergencies during extreme climate events. As a prelude to that article, we have created a visual for the statistical product development component of how that process works in action in Figure 2.\n\n\n\n\n\n\nFigure 2: Example: Steps in the statistical product development for the skilled nursing facility use case. The diagram describes the steps applied to a use case on the resilience of skilled nursing facilities. Section 3 of this series describes the steps in detail.\n\n\n\nThe CDE Framework’s guiding principles and research steps are described below. To find out more click on a cross reference.\nGuiding principles:\n\nPurposes and uses\nStakeholders\nCuration\nEquity and ethics\nPrivacy and confidentiality\nCommunications and dissemination\n\nResearch steps:\n\nSubject matter input\nData discovery\nData ingestion & Governance\nData wrangling\nFitness-for-purpose\nStatistics development"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#guiding-principles",
    "href": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#guiding-principles",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Guiding principles",
    "text": "Guiding principles\n\nPurposes and uses\nThe CDE is centered on developing statistical products to meet specific purposes and uses. Researchers and stakeholders propose the purposes and uses, defining the ‘why’ for developing statistics and statistical products. They include questions or issues that the statistics should be designed to support and are clarified by documented best practices, literature reviews and conversations with subject matter experts.\n\n\nStakeholders\nStakeholders include individuals, groups, and organizations that have the potential to affect or be affected by the outcome of the research. Engaging stakeholders is crucial for fostering the connection and trust that can lead to better decision making. Kujala et al. (2022) best described the principle of stakeholder engagement: ‘Stakeholder engagement refers to the aims, activities, and impacts of stakeholder relations in a moral, strategic, and pragmatic manner.’ When placed within the CDE context and represented in the Framework, collaborative engagement with stakeholders occurs at all stages of product development to better understand what the final product needs to look like. Further, product development is not a linear process but occurs through successive waves of iteration with users.\nForming partnerships with stakeholders is instrumental in identifying requirements and implementing statistical products. This requires listening to community voices in an active engagement strategy.1 Of necessity, these partnerships entail collaboration, such as creative and collaborative problem-solving workshops and the development of innovative digital tools vetted by networks of users.2\n\n\nCuration\nThe broad meaning of curation is the act of organizing, documenting and maintaining a collection of artifacts. The artifacts of the development and dissemination of statistics or statistical products include all the components in Figure 1, from meeting with stakeholders to formulating the purposes and uses to creating and disseminating the statistical products. Maintaining the artifacts is the essence of the CDE. Every step in the process should be documented and easily accessible in a repository, for example, GitHub, for the work to be transparent and reproducible. Curation in the context of the CDE is an end-to-end activity. It involves documenting the purpose and use, providing the context for acquiring, wrangling, and archiving data from many sources to support the development of statistical products. It will include metadata (Cannon 2013), the code used to read and write the data, and the code that ingested the data from the source and prepared it for analysis.\nCuration steps\n\nDocument the development of the research questions, why this research is important, and how it supports the purposes and uses and resulting statistical product.\nDocument the context for the purposes and uses, ie, a policy directive, stakeholder request, policy evaluation, etc.\nWhat stakeholder engagement and transparency are built into the process?\n\n\n\nEquity and ethics\nAn ethics review ensures dialogue on this topic throughout the statistical product development and dissemination life cycle. This involves teams of researchers and stakeholders across many areas of expertise, each with its own research integrity norms and practices. This requires that ethics be woven into every aspect of the CDE. An equity review ensures that underserved groups are represented and biases inherent in various data sources are acknowledged.\nCuration questions\n\nWhat are the project’s expected benefits to the ‘public good’? Do they outweigh potential risks to specific sub-populations, eg, individuals, firms and their locations by different levels of geography?\nAre there implicit assumptions and biases regarding the studied communities in framing the project and associated data sources? If yes, how will they be addressed?\nWhat type of institutional approval process and contracts are needed? What statistical quality standards and confidentiality standards will be needed? For an explanation of the Institution Review Board see Note 1.\n\nAn ethics checklist can help with this process. Links to ethics checklists are provided below.\n\nUniversity of Virginia, Biocomplexity Institute, Social and Decision Analytics Division Data Science Project Ethics Tool\nUnited Kingdom Government, Data Ethics Framework\n\n\n\nPrivacy and confidentiality\nPrivacy is about the individual, whereas confidentiality is about the individual’s information. Privacy refers to an individual’s desire to control their information. Confidentiality refers to the researcher’s agreement with the individual, which could be an agency like the Census Bureau, regarding how their information will be handled, managed, and disseminated (Keller, Shipp, and Schroeder 2016). This is a guiding principle because it needs to be considered and embraced at the earliest possible stages of statistical product development and will impact dissemination choices.\nCuration questions\n\nWhat steps are taken to ensure the privacy and confidentiality of the data?\nWhat statistical methods (if any) are used to ensure the privacy and confidentiality of the data?\nHow do the methods chosen to protect confidentiality affect the purposes and uses of the data?\nWhat stakeholder engagement and transparency are built into the process?\nDoes the context surrounding the purposes, uses, and anticipated data sources require an Institutional Review Board (IRB) review and approval? If yes, is it archived?\n\n\n\n\n\n\n\nNote 1: Institutional Review Board\n\n\n\nIn the United States, institutional review boards (IRBs) assess the ethics and safety of research studies involving human subjects, such as behavioral studies or clinical trials for new drugs or medical devices. Today, the definition of human subjects has evolved to include secondary data, such as administrative data collected for other purposes, eg, local property data collected for tax purposes.\nThe Belmont Commission was convened in the late 1970s after the ethical failures of many research projects that involved vulnerable populations surfaced. The Belmont Commission issued three principles for the conduct of ethical research:\n\nRespect for people — treating people as autonomous and honoring their wishes\nBeneficence — understanding the risks and benefits of the study and weighing the balance between (1) doing no harm and (2) maximizing possible benefits and minimizing possible harms\nJustice — deciding if the risks and benefits of research are distributed fairly.\n\nThese principles were translated to a set of regulations called the Common Rule that govern federally-funded research. The Belmont Commission provided the foundation for IRB principles and focused on research involving human subjects in experiments and studies. IRB approval is required to be eligible for federal grants and contracts. Many universities also require IRB review for research conducted by faculty, students, and researchers (Shipp, LaLonde, and Martinez 2023).\n\n\n\n\nCommunication and dissemination\nCommunication involves sharing data, statistical method choices, well-documented code, working papers, and dissemination through research team meetings, stakeholder engagements, conference presentations, publications, webinars, websites, and social media. As a principle, communication and dissemination are critical to ensure that statistical product development processes and findings are transparent and reproducible (Berman et al. 2016). An essential facet of this step is to tell the story of the analysis by conveying the context, purpose, and implications of the research and findings (Berinato 2019; Wing 2019; NASEM 2022).\nCuration questions\n\nAre the meeting notes, statistical products, code, reports, and presentations archived in a repository?\nBriefly describe what did not work in this process, eg, data wrangling challenges where data sources could not be integrated, data source changes after a fitness-for-purpose assessment, analyses that were changed because assumptions were not met, etc.\nHave project methods and outputs been made as transparent as possible?\nAre the potential limitations of the research clearly presented?\nWhy or why not should the research be used as the basis for an institutional or policy action?\nHave the predicted benefits and social costs to all potentially affected communities been considered?"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#research-steps",
    "href": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#research-steps",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Research steps",
    "text": "Research steps\n\nSubject matter input\nSubject matter (domain) expertise plays a role in translating the information acquired into understanding the underlying phenomena in the data (Box et al. 1978). Domain knowledge provides the context to define, evaluate and interpret the findings at each research stage (Leonelli 2019; Snee, DeVeaux, and Hoerl 2014). Subject matter input can be obtained through a review of the literature, talking to experts, or learning about their work at conferences or other convenings. Subject matter experts are different than stakeholders. Both provide important input to identifying and clarifying purposes and uses.\nCuration steps\n\nDocument the meetings with subject matter experts and stakeholders.\nDocument the literature search methods and the results of the literature review.\nDocument choices are made during the development of the products.\nWere subject matter experts and stakeholders recruited from underrepresented groups?\n\n\n\nData discovery\nData discovery identifies potential sources that address the research goals defined by purposes and uses. Data sources include the following types (Keller et al. 2020).\n\nDesigned data are collected using statistically designed methods, such as surveys, censuses, and data generated from an experimental or quasi-experimental design, such as a clinical trial or agricultural field study.\nAdministrative data are collected for the administration of an organization or program by entities such as government agencies.\nOpportunity data are derived from internet-based information, such as websites, wearable and other sensor devices, and social media, and captured through application programming interfaces (APIs) and web scraping, eg, geocoded place-based data, transportation routes, and other data sources.\nProcedural data are processes and policies, such as a change in health care coverage, a data repository policy outlining procedures and the metadata required to store data, or a responsible AI policy.\n\nThe goal of the data discovery process is to think broadly and imaginatively about all data types and to capture the variety of data sources that could be useful for the problem. There are three steps in the data discovery process (Keller, Shipp, and Schroeder 2016).\n\nIdentify potential data sources and make an inventory.\nCreate a set of questions to screen the data sources to ensure the data meet the criteria for use.\nSelect and acquire the data sources that meet the screening criteria.\n\nCuration steps\n\nDescribe your data discovery process and reasoning behind the selected data sources.\n\nDo underrepresented groups have adequate geographic coverage? If not, are there methods, such as synthetic data, you can use to provide adequate coverage?\nHave checks and balances been established to identify and address implicit biases in the data and interpretation of the data? Has the team engaged in discussion and provided insights across their diverse perspectives?\n\nDescribe the assumptions that need to be made to use these data sources.\nIdentify and document the paradata and metadata that describe each data source. Paradata describe how the data were collected, while metadata are ‘data about data’. It includes information about the data’s content, data dictionaries and technical documents that will help the user assess its fitness for purpose (Cannon 2013; NASEM 2022).\nDiscuss data sources you would have used if they were available.\n\n\n\nData ingest and governance\nData ingestion is the process of bringing data into the data management platform(s) for use. Data governance establishes and adheres to rules and procedures regarding data access, dissemination and destruction.\nCuration steps\n\nDocument policies and institutional agreements for data use.\n\nHave team members reviewed data use agreements, standard operating procedures (SOPs), and data management plans? Are they fair?\nDo additional procedures need to be defined for this project?\n\nDocument the code and processes used to ingest the data sources and manage governance.\n\n\n\nData wrangling\nData wrangling includes the activities of data profiling, preparing, linking and exploring used to assess the data’s quality and representativeness and what analyses the data can support.\n\nTable 1. Activities of data wrangling\n\n\n\n\n\n\n\n\nProfiling\nPreparing\nLinking\nExploring\n\n\n\n\n\ndata quality\ndata structure\nmeta data, paradata, and provenance\n\n\ncleaning\ntransforming\nstructuring\n\n\nontology selection & alignment\nentity resolution / harmonization\n\n\nvisualizations\ndescriptive statistics\ncharacterizations\n\n\n\n\nCuration steps\n\nDescribe any data quality issues within the stated purpose and use context and how they were resolved. This can include statistical solutions like imputing missing data, identifying outliers or constructing synthetic populations.\n\nHow representative are the data?\nWhat populations are and are not covered?\n\nDescribe any issues with the wrangling process and how they were resolved.\nDocument the code used to wrangle the data and describe how it was validated.\nDocument assumptions made regarding the transformation and use of the data.\n\n\n\nFitness-for-purpose\nFitness-for-purpose starts with assessing the constraints imposed on the data by the particular statistical methods used and the population to which the inferences extend. It is a function of the modeling, data quality needs of the models, and data coverage (representativeness) needs of the models. The statistical product’s ‘fitness-for-purpose’ involves those on the receiving end of the data helping identify issues germane to the data application, such as identifying biases affecting equity. For example, given known differences in their availability, does using administrative records lead to better modeling outcomes for some groups more than others? What can be done to compensate for such bias?\nCuration steps\n\nDocument the constraints and limitations of the data. \n\nWhat are the limitations of the results? Are the results useful, given the purpose of the study?\n\nDiscuss the populations to which any inferences will generalize.\n\nDo the statistical results support the potential benefits of the study previously stated?\nDo any data require revisiting the question of potential biases being introduced through the choice of data sets and variables?\n\n\n\n\nStatistics development\nThe development of statistics and statistical products for dissemination is a function of the research questions, the data’s limitations and the assumptions of the statistical method(s) used.\nCuration steps\n\nDescribe the statistical methods planned and used and how the method assumptions were evaluated.\nDiscuss the conclusions of the statistical analyses and any inferences that can be made from the disseminated statistical products.\nDiscuss how the statistics support the purposes and uses driving the development of the products.\n\nHere, we have defined the CDE and provided a conceptual walk through of the framework from Figure 1. In the next article, we will put the CDE Framework into practice through a demonstration use case on the resilience of skilled nursing facilities.\n\n\n\n\n← Part 1: The policy problem\n\n\n\n\nPart 3: Climate resiliency of skilled nursing facilities →\n\n\n\n\n\n\n\n\nAbout the authors\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Chay_Tee on Shutterstock.\n\n\n\nHow to cite\n\nKeller S, Shipp S, Lancaster V, Salvo J (2024). “Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?” Real World Data Science, November 8, 2024. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#footnotes",
    "href": "instruments_old/case-studies/posts/2024/11/08/what-is-CDE-2.html#footnotes",
    "title": "Advancing Data Science in Official Statistics – What is the Curated Data Enterprise?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.census.gov/newsroom/blogs/director/2023/01/a-look-ahead-2023.html ↩︎\nhttps://www.census.gov/partners/act.html↩︎"
  },
  {
    "objectID": "companies/tek.html",
    "href": "companies/tek.html",
    "title": "Tektronix",
    "section": "",
    "text": "A Revolution Born in a Basement\nIn the aftermath of World War II, in a humble basement in Portland, Oregon, a revolution in the world of electronic measurement was about to be born. This is where Tektronix came into being in 1946, founded by Howard Vollum and Jack Murdock. Their first commercial oscilloscope, the Model 511, was not just a product; it redefined what an oscilloscope could be and propelled Tektronix to the forefront of the industry for decades to come.\n\n\nThe Genesis of Innovation: The Triggered Sweep\nBefore Tektronix, oscilloscopes were often unstable and difficult-to-use instruments. Visualizing an electronic signal clearly and stably was more of an art than a science. Howard Vollum’s major innovation, the technical brain of the duo, was to integrate a triggered sweep circuit into a commercial device.\nThis technology allowed the oscilloscope to start its horizontal sweep at the same point on the input signal every time. The result was a static, sharp image on the screen, even for complex and non-repetitive signals. This advancement transformed the oscilloscope from a simple qualitative viewing tool into a precise and reliable quantitative measurement instrument.\n\n\nThe Tektronix 511: A New Era of Measurement\nLaunched in 1947, the Tektronix 511 quickly captured the market. Sold for $795, a significant price for the time, it offered unparalleled performance and ease of use. Besides its triggered sweep, the 511 stood out for several other innovative features:\n\nPrecision and Stability: Thanks to regulated power supplies, the 511 offered significantly higher stability and measurement precision than its competitors.\nPortability: Compared to the massive instruments of the era, the 511 was relatively compact and portable, making it practical for both laboratory and field use.\nManufacturing Quality: Tektronix built a reputation for manufacturing excellence, ensuring the reliability and longevity of its devices.\n\nThe success of the 511 was immediate and laid the foundation for the company’s future growth. It allowed engineers and technicians to “see” electronics like never before, accelerating development in many areas, from television to nascent computing.\n\n\nThe Legacy of the Pioneers\nThe history of the first Tektronix oscilloscopes is inseparable from the vision of its founders. Howard Vollum, the brilliant engineer, and Jack Murdock, the shrewd businessman, succeeded in creating a company where innovation and quality were paramount. Their philosophy not only gave birth to revolutionary products like the 511 but also established a corporate culture that attracted the best talent and fostered decades of technological advancements. The introduction of modular oscilloscopes (plug-ins) in the 1950s is just one example of this continuous innovation that kept Tektronix at the top of its field."
  },
  {
    "objectID": "news/90_ghz_china.html",
    "href": "news/90_ghz_china.html",
    "title": "China has officially unveiled its new homegrown 90GHz ultra-high-speed real-time oscilloscope",
    "section": "",
    "text": "90 GHz china oscilloscope\nChina has officially unveiled its new homegrown 90GHz ultra-high-speed real-time oscilloscope, marking a significant milestone for the global electronic communications sector. The announcement was made at the WESEMiBAY Semiconductor Ecosystem Expo 2025 in Shenzhen."
  },
  {
    "objectID": "news/90_ghz_china.html#key-features-and-innovations",
    "href": "news/90_ghz_china.html#key-features-and-innovations",
    "title": "China has officially unveiled its new homegrown 90GHz ultra-high-speed real-time oscilloscope",
    "section": "Key Features and Innovations",
    "text": "Key Features and Innovations\nAn oscilloscope is a crucial piece of equipment that serves as the “eyes” for engineers by converting invisible electrical signals into visible waveforms. According to Liu Sang, CEO of one of the developers (Shenzhen Longsight Technologies), this achievement required sustained innovation across materials, manufacturing, chips, and algorithms.\n\nThe device has already been tested and applied by several enterprises, including Huawei."
  },
  {
    "objectID": "rwds-partners.html",
    "href": "rwds-partners.html",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#publisher",
    "href": "rwds-partners.html#publisher",
    "title": "Our partners and funders",
    "section": "",
    "text": "Real World Data Science is a project of the Royal Statistical Society (RSS). The Society was founded in 1834 and is one of the world’s leading organisations advocating for the importance of statistics and data.\nRSS has more than 10,000 members in the UK and across the world. As a charity, it advocates for the key role of statistics and data in society, and works to ensure that policy formulation and decision making are informed by evidence for the public good.\nTo support the work of the RSS, including Real World Data Science and other projects, become a member today.\nEmail: info@rss.org.uk",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#partners",
    "href": "rwds-partners.html#partners",
    "title": "Our partners and funders",
    "section": "Partners",
    "text": "Partners\n\n\n\n\n\nThe American Statistical Association is the world’s largest community of statisticians, the “Big Tent for Statistics.” It is the second-oldest, continuously operating professional association in the US. Since it was founded in Boston in 1839, the ASA has supported excellence in the development, application, and dissemination of statistical science through meetings, member services, education, publications, advocacy, and accreditation.\nOur members serve in industry, government, and academia in more than 90 countries, advancing research and promoting sound statistical practice to inform public policy and improve human welfare.\nTo support the work of the ASA, become a member today.\nEmail: asainfo@amstat.org",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "rwds-partners.html#funders",
    "href": "rwds-partners.html#funders",
    "title": "Our partners and funders",
    "section": "Funders",
    "text": "Funders\n\n\n\n\n\nReal World Data Science was supported by startup funding from The Alan Turing Institute.\nThe Alan Turing Institute, headquartered in the British Library, London, was created as the UK’s national institute for data science in 2015. In 2017, as a result of a government recommendation, artificial intelligence was added to its remit. \nThe Institute is named in honour of Alan Turing (23 June 1912 – 7 June 1954), whose pioneering work in theoretical and applied mathematics, engineering and computing are considered to be the key disciplines comprising the fields of data science and artificial intelligence.\nTo find out more about The Alan Turing Institute, its strategy and programme of work, visit turing.ac.uk.",
    "crumbs": [
      "Our partners and funders"
    ]
  },
  {
    "objectID": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "href": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "",
    "text": "Doomsaying is an old occupation. Artificial intelligence (AI) is a complex subject. It’s easy to fear what you don’t understand. These three truths go some way towards explaining the oversimplification and dramatisation plaguing discussions about AI.\nLast week, outlets around the world were plastered with news of yet another open letter claiming AI poses an existential threat to humankind. This letter, published through the nonprofit Center for AI Safety, has been signed by industry figureheads including Geoffrey Hinton and the chief executives of Google DeepMind, Open AI and Anthropic.\nHowever, I’d argue a healthy dose of scepticism is warranted when considering the AI doomsayer narrative. Upon close inspection, we see there are commercial incentives to manufacture fear in the AI space.\nAnd as a researcher of artificial general intelligence (AGI), it seems to me the framing of AI as an existential threat has more in common with 17th-century philosophy than computer science."
  },
  {
    "objectID": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "href": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#was-chatgpt-a-breakthrough",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Was ChatGPT a ‘breakthrough’?",
    "text": "Was ChatGPT a ‘breakthrough’?\nWhen ChatGPT was released late last year, people were delighted, entertained and horrified.\nBut ChatGPT isn’t a research breakthrough as much as it is a product. The technology it is based on is several years old. An early version of its underlying model, GPT-3, was released in 2020 with many of the same capabilities. It just wasn’t easily accessible online for everyone to play with.\nBack in 2020 and 2021, I and many others wrote papers discussing the capabilities and shortcomings of GPT-3 and similar models – and the world carried on as always. Forward to today, and ChatGPT has had an incredible impact on society. What changed?\nIn March, Microsoft researchers published a paper claiming GPT-4 showed “sparks of artificial general intelligence”. AGI is the subject of a variety of competing definitions, but for the sake of simplicity can be understood as AI with human-level intelligence.\nSome immediately interpreted the Microsoft research as saying GPT-4 is an AGI. By the definitions of AGI I’m familiar with, this is certainly not true. Nonetheless, it added to the hype and furore, and it was hard not to get caught up in the panic. Scientists are no more immune to group think than anyone else.\nThe same day that paper was submitted, The Future of Life Institute published an open letter calling for a six-month pause on training AI models more powerful than GPT-4, to allow everyone to take stock and plan ahead. Some of the AI luminaries who signed it expressed concern that AGI poses an existential threat to humans, and that ChatGPT is too close to AGI for comfort.\nSoon after, prominent AI safety researcher Eliezer Yudkowsky – who has been commenting on the dangers of superintelligent AI since well before 2020 – took things a step further. He claimed we were on a path to building a “superhumanly smart AI”, in which case “the obvious thing that would happen” is “literally everyone on Earth will die”. He even suggested countries need to be willing to risk nuclear war to enforce compliance with AI regulation across borders."
  },
  {
    "objectID": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "href": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#i-dont-consider-ai-an-imminent-existential-threat",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "I don’t consider AI an imminent existential threat",
    "text": "I don’t consider AI an imminent existential threat\nOne aspect of AI safety research is to address potential dangers AGI might present. It’s a difficult topic to study because there is little agreement on what intelligence is and how it functions, let alone what a superintelligence might entail. As such, researchers must rely as much on speculation and philosophical argument as on evidence and mathematical proof.\nThere are two reasons I’m not concerned by ChatGPT and its byproducts.\nFirst, it isn’t even close to the sort of artificial superintelligence that might conceivably pose a threat to humankind. The models underpinning it are slow learners that require immense volumes of data to construct anything akin to the versatile concepts humans can concoct from only a few examples. In this sense, it is not “intelligent”.\nSecond, many of the more catastrophic AGI scenarios depend on premises I find implausible. For instance, there seems to be a prevailing (but unspoken) assumption that sufficient intelligence amounts to limitless real-world power. If this was true, more scientists would be billionaires.\nMoreover, cognition as we understand it in humans takes place as part of a physical environment (which includes our bodies), and this environment imposes limitations. The concept of AI as a “software mind” unconstrained by hardware has more in common with 17th-century dualism (the idea that the mind and body are separable) than with contemporary theories of the mind existing as part of the physical world."
  },
  {
    "objectID": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "href": "sp/posts/2023/06/05/no-AI-probably-wont-kill-us.html#why-the-sudden-concern",
    "title": "No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye",
    "section": "Why the sudden concern?",
    "text": "Why the sudden concern?\nStill, doomsaying is old hat, and the events of the last few years probably haven’t helped – but there may be more to this story than meets the eye.\nAmong the prominent figures calling for AI regulation, many work for or have ties to incumbent AI companies. This technology is useful, and there is money and power at stake – so fearmongering presents an opportunity.\nAlmost everything involved in building ChatGPT has been published in research anyone can access. OpenAI’s competitors can (and have) replicated the process, and it won’t be long before free and open-source alternatives flood the market.\nThis point was made clearly in a memo purportedly leaked from Google entitled “We have no moat, and neither does OpenAI”. A moat is jargon for a way to secure your business against competitors.\nYann LeCun, who leads AI research at Meta, says these models should be open since they will become public infrastructure. He and many others are unconvinced by the AGI doom narrative.\n\n\n\nA NYT article on the debate around whether LLM base models should be closed or open.Meta argues for openness, starting with the release of LLaMA (for non-commercial use), while OpenAI and Google want to keep things closed and proprietary.They argue that openness can be…\n\n— Yann LeCun (@ylecun) May 18, 2023\n\n\n\nNotably, Meta wasn’t invited when US President Joe Biden recently met with the leadership of Google DeepMind and OpenAI. That’s despite the fact that Meta is almost certainly a leader in AI research; it produced PyTorch, the machine-learning framework OpenAI used to make GPT-3.\nAt the White House meetings, OpenAI chief executive Sam Altman suggested the US government should issue licences to those who are trusted to responsibly train AI models. Licences, as Stability AI chief executive Emad Mostaque puts it, “are a kinda moat”.\nCompanies such as Google, OpenAI and Microsoft have everything to lose by allowing small, independent competitors to flourish. Bringing in licensing and regulation would help cement their position as market leaders and hamstring competition before it can emerge.\nWhile regulation is appropriate in some circumstances, regulations that are rushed through will favour incumbents and suffocate small, free and open-source competition.\n\n\n\nThink Google or Microsoft are encouraging legislation for your safety? But of course! These are honorable companies.You might think they'd like less competition too though. Maybe a monopoly? Maybe legal red tape preventing free and open source alternatives? Perhaps other… https://t.co/Z7vSpMyuHg\n\n— Michael Timothy Bennett (@MiTiBennett) May 5, 2023\n\n\n\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nMichael Timothy Bennett is a PhD student in the School of Computing, Australian National University.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nThumbnail image by Alan Warburton / © BBC / Better Images of AI / Social Media / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "sp/posts/2023/09/06/biased-algorithms.html",
    "href": "sp/posts/2023/09/06/biased-algorithms.html",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "",
    "text": "Bad data does not only produce bad outcomes. It can also help to suppress sections of society, for instance vulnerable women and minorities.\nThis is the argument of my new book on the relationship between various forms of racism and sexism and artificial intelligence (AI). The problem is acute. Algorithms generally need to be exposed to data – often taken from the internet – in order to improve at whatever they do, such as screening job applications, or underwriting mortgages.\nBut the training data often contains many of the biases that exist in the real world. For example, algorithms could learn that most people in a particular job role are male and therefore favour men in job applications. Our data is polluted by a set of myths from the age of “enlightenment”, including biases that lead to discrimination based on gender and sexual identity.\nJudging from the history in societies where racism has played a role in establishing the social and political order, extending privileges to white males –- in Europe, North America and Australia, for instance –- it is simple science to assume that residues of racist discrimination feed into our technology.\nIn my research for the book, I have documented some prominent examples. Face recognition software more commonly misidentified black and Asian minorities, leading to false arrests in the US and elsewhere.\nSoftware used in the criminal justice system has predicted that black offenders would have higher recidivism rates than they did. There have been false healthcare decisions. A study found that of the black and white patients assigned the same health risk score by an algorithm used in US health management, the black patients were often sicker than their white counterparts.\nThis reduced the number of black patients identified for extra care by more than half. Because less money was spent on black patients who have the same level of need as white ones, the algorithm falsely concluded that black patients were healthier than equally sick white patients. Denial of mortgages for minority populations is facilitated by biased data sets. The list goes on."
  },
  {
    "objectID": "sp/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "href": "sp/posts/2023/09/06/biased-algorithms.html#machines-dont-lie",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Machines don’t lie?",
    "text": "Machines don’t lie?\nSuch oppressive algorithms intrude on almost every area of our lives. AI is making matters worse, as it is sold to us as essentially unbiased. We are told that machines don’t lie. Therefore, the logic goes, no one is to blame.\nThis pseudo-objectiveness is central to the AI-hype created by the Silicon Valley tech giants. It is easily discernible from the speeches of Elon Musk, Mark Zuckerberg and Bill Gates, even if now and then they warn us about the projects that they themselves are responsible for.\nThere are various unaddressed legal and ethical issues at stake. Who is accountable for the mistakes? Could someone claim compensation for an algorithm denying them parole based on their ethnic background in the same way that one might for a toaster that exploded in a kitchen?\nThe opaque nature of AI technology poses serious challenges to legal systems which have been built around individual or human accountability. On a more fundamental level, basic human rights are threatened, as legal accountability is blurred by the maze of technology placed between perpetrators and the various forms of discrimination that can be conveniently blamed on the machine.\nRacism has always been a systematic strategy to order society. It builds, legitimises and enforces hierarchies between the haves and have nots."
  },
  {
    "objectID": "sp/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "href": "sp/posts/2023/09/06/biased-algorithms.html#ethical-and-legal-vacuum",
    "title": "For minorities, biased AI algorithms can damage almost every part of life",
    "section": "Ethical and legal vacuum",
    "text": "Ethical and legal vacuum\nIn such a world, where it’s difficult to disentangle truth and reality from untruth, our privacy needs to be legally protected. The right to privacy and the concomitant ownership of our virtual and real-life data needs to be codified as a human right, not least in order to harvest the real opportunities that good AI harbours for human security.\nBut as it stands, the innovators are far ahead of us. Technology has outpaced legislation. The ethical and legal vacuum thus created is readily exploited by criminals, as this brave new AI world is largely anarchic.\nBlindfolded by the mistakes of the past, we have entered a wild west without any sheriffs to police the violence of the digital world that’s enveloping our everyday lives. The tragedies are already happening on a daily basis.\nIt is time to counter the ethical, political and social costs with a concerted social movement in support of legislation. The first step is to educate ourselves about what is happening right now, as our lives will never be the same. It is our responsibility to plan the course of action for this new AI future. Only in this way can a good use of AI be codified in local, national and global institutions.\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nArshin Adib-Moghaddam is professor in global thought and comparative philosophies, SOAS, University of London.\n\n\n\n\n\nCopyright and licence\n\nThis article is republished from The Conversation under a Creative Commons license. Read the original article.\n\n\nImage by Alan Warburton / © BBC / Better Images of AI / Quantified Human / Licenced by CC-BY 4.0."
  },
  {
    "objectID": "sp/posts/2023/08/17/data-science-and-games.html",
    "href": "sp/posts/2023/08/17/data-science-and-games.html",
    "title": "Where do AI, data science, and computer games intersect?",
    "section": "",
    "text": "Game studios have cemented their place among the fastest-growing media industries. In recognition of this, we hosted an event in June through the Royal Statistical Society (RSS) Merseyside Local Group to explore AI and data science in computer game development. This was an amazing opportunity to engage with a different, in-vogue domain that has unique ties to data science. We showcased two fantastic presentations covering both academic and industry perspectives.\nStanley Wang, a data scientist at SEGA Europe, opened the event by showing the methods that SEGA uses to collect, process, and apply data on player decisions in-game. It was a revealing glimpse at how smoothly in-game data collection is integrated into SEGA’s digital platforms and the ways these data can be used to engage game-centred communities – for example, running special celebrations once milestones are hit for in-game events (revenue made, goals scored, etc.) or offering real-time integration with streaming platforms so viewers can see detailed statistics on in-game progress. Stanley showed one particular example where data collection fed directly into development decisions for Endless Space, a competitive strategy game where players vie for galactic conquest. During the beta (a period where a game is available to play but still considered in-testing before commercial release), SEGA were able to monitor how well-balanced the playable alien factions were based on real-time win rate data, which led to improvements to game mechanics for the final release.\nWe also learned how SEGA’s data science teams are using clustering methods to identify different game-playing behaviours in Two Point Hospital, a simulation game where players design, build, and manage a hospital through various scenarios. After compiling high-dimensional in-game data such as objectives achieved, treatment of staff, and even furniture choices, various clustering algorithms (including k-means clustering) were used to identify common sets of player behaviour. Stanley highlighted that when using these sorts of unsupervised learning methods, it’s useful to get insights from multiple models to inform methodological decisions like number of clusters chosen or how to treat outliers. SEGA identified four distinct types of player from these analyses, which you can hear more about from Stanley in the video below. The approach allowed the company to better understand gamers’ motivations and experiences with a view to designing future game content.\n\nOur second speaker, Dr Konstantinos Tsakalidis, a lecturer in the Department of Computer Science at the University of Liverpool, presented exciting new ideas to teach computer games developers of the future. Dr Tsakalidis walked us through the curriculum for a dynamic new undergraduate program that reflects the latest software development technologies and the theory behind them. The course outline was designed around building knowledge and practice from the fundamentals upwards, starting from game physics as a prerequisite for game mechanics, game mechanics being a prerequisite for game content, and game content being a prerequisite for game AI. Combined with the continuous active involvement of students at each stage, this represented a great model of constructivist teaching. Dr Tsakalidis also proposed that practical game development (and subsequent assessments) should follow the latest research on data science and AI in computer games.\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nAlice-Maria Toader is a PhD student at the University of Liverpool and a committee member of the RSS Merseyside Local Group. Liam Brierley is a research fellow in health data science at the University of Liverpool and chair of the RSS Merseyside Local Group.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Alice-Maria Toader and Liam Brierley\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Jezael Melgoza on Unsplash.\n\n\n\nHow to cite\n\nToader, Alice-Maria and Liam Brierley. 2023. “Where do AI, data science, and computer games intersect?” Real World Data Science, August 17, 2023. URL"
  },
  {
    "objectID": "sp/posts/2025/08/04/AI-in-Bus-Review.html",
    "href": "sp/posts/2025/08/04/AI-in-Bus-Review.html",
    "title": "Book Review: AI in Business: Towards the Autonomous Enterprise by Sarah Burnett",
    "section": "",
    "text": "As AI continues to reshape industries, business leaders are increasingly seeking guidance on how to harness its potential responsibly and effectively. Keeping abreast of these conversations is crucial for data practitioners seeking to guide their non-technical stakeholders and foster cross-functional collaboration. The recently released second edition of AI in Business: Towards the Autonomous Enterprise (BCS, 2024) aims to help decision-makers understand the strategic opportunities and challenges of AI in a business context. We asked Ed Rochead, Chair of the Alliance for Data Science Professionals, to reflect on the themes, frameworks and case studies it offers.\n\n\n\n\n\n\n\nFigure 1: AI in Business: Towards the Autonomous Enterprise\n\n\n\nThe key words in the title of the book are ‘autonomous enterprise’: this is a book about using AI to make enterprises more effective with autonomy (AI), rather than (as the cover picture of a robot might suggest) embed autonomous robotic systems in a business.\nThe volume is in three main parts. The first introduces AI, with useful explanations of terms like “generative AI” and gives some thoughts about how AI might be used for innovation and efficiency in a business. The second part gives some case studies on how real, recognisable organisations have successfully used AI to automate their operations with some success. The third part reflects upon the future, focusing on how an organisation might start the journey towards autonomy, as well as some thoughts on the impact of autonomy on society.\nA chapter I found particularly helpful is ‘What You Need to Know About AI.’ It seeks to explain the relevant terms at the level required by an industry or business leader, rather than giving an in-depth technical account of the concepts, and in this the author achieved their intent.\nAt the heart of the book are the case studies, involving organisations that include international companies, an NHS Trust, and a district council. The reader is likely to have some personal experience of these types of organisations, for instance as a patient or resident. This made the case studies even more engaging, at least to me, as not only could I put myself in the shoes of a leader in the organisation concerned, it was also possible to empathise with those involved in the system. This knowledge and interest really brought the content to life, and this made the author’s choice of case studies inspired.\nThe closing section is intriguing. The chapter introducing the first steps an organisation might take towards autonomy is helpful, as it illustrates example of the stages of autonomy as applied to buying a car (spoiler alert – in the last stage it is manufactured and then drives itself to the consumer’s home!) The second chapter in this section, looking towards the future, gets the reader thinking more broadly about the impact of automation on society. This includes the thorny issues of ethics and impact on things like (un)employment; both areas were covered engagingly and thought provokingly.\nAlthough impressed with the content of the book, I found the typeface very cramped and small, and joked with friends that 200 pages of material is crammed into 160. This makes the contents a harder read than they might otherwise be.\nIn one sense, each of the three sections would make a good, if short, book but, read together in the sequence provided, they become more than the sum of their parts, with the first part informative, the second part engaging, and the third thought provoking. AI in Business is an excellent read for an organisational leader seeking inspiration to automate. It gives enough language and concept familiarity to enable such a reader to ask sensible questions of technical experts, and an idea of the art of the possible. Someone with an interest in how AI might change the world around us could also find this a fascinating and informative read.\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nProfessor Edward Rochead, M.Math (Hons), PGDip, CMath, FIMA is a mathematician employed by the government, currently leading work on STEM Skills and Data. Ed is chair of the Alliance for Data Science Professionals, a Visiting Professor at Loughborough University, an Honorary Professor at the University of Birmingham, Chartered Mathematician, and Fellow of the IMA and RSA. Copyright and licence\n\n\n© 2025 Royal Statistical Society\n\n\nThumbnail image by BCS   This article is licensed under a Creative Commons Attribution (CC BY 4.0) International licence."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/10/18/meet-the-team.html",
    "href": "sp/editors-blog/posts/2022/10/18/meet-the-team.html",
    "title": "Meet the team",
    "section": "",
    "text": "Sophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a dataset, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nYaxin Bi\n\n\n\nI am a reader in the Artificial Intelligence Research Centre, School of Computing at Ulster University, UK. My research interests include machine learning, data science and ensemble methods underpinned with the Dempster-Shafer theory of evidence. I have led on developing data analytics for anomaly/change detection within satellite data, as well as text analytics for document categorisation and sentiment analysis. I have served a number of international conferences as general chair and technical program committee chair. I am an associate editor for the International Journal of Intelligence Systems and a core steering committee member for the series of International Conference on Knowledge Science, Engineering and Management.\nAs a member of this editorial board, I am passionate about contributing my expertise to the development of data science and its real-world applications. I look forward to sharing my experience with others in the data science community, in particular to address ethical challenges posed by large language models and to inspire new curriculum development in the field of data science.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nWillis Jensen\n\n\n\nI’m a statistician and leader with over 20 years of experience applying data science methods to real-world challenges in business, quality, product development, manufacturing, and supply chain. I hold a PhD in statistics from Virginia Tech and an MS in statistics from Brigham Young University (BYU).\nCurrently, I lead a business intelligence team at CHG Healthcare in the USA and serve as an Adjunct Professor of Statistics at BYU, where I get to share my enthusiasm for data with students. A long-time member and volunteer for the ASA, I represent the association on the Real World Data Science board.\nThroughout my career, I’ve built and led high-performing teams, developed data-drive solutions, and helped organizations become more analytically mature. While I enjoy the technical depth of statistical modeling, I’m most passionate about using data to solve meaningful business problems. With over 25 publications focused on practical applications, I specialize in bridging the gap between business and the broad tent of data science—helping leaders ask better questions and act on insights.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nMonnie McGee\n\n\n\nI received my B.A. in Mathematics and English from Austin College and completed my M.A. and Ph.D. in Statistics at Rice University. My primary research interests lie in the statistical analysis of high-throughput biological data, with emphasis on the development and application of methods for background correction, normalization, and inference procedures tailored to the unique data structures arising in technologies such as RNA-Seq, microarrays, and flow cytometry.\nI have extensive expertise in the analysis of data constrained to the simplex, and my methodological contributions include the development of inference procedures for compositional data using the Nested Dirichlet Distribution. In parallel, I maintain an active research agenda in sports analytics, particularly for event-driven sports including track and field, competitive diving, swimming, and gymnastics. My work in this area focuses on ranking models, performance trajectories, and judge reliability.\nMost recently, I have extended my research to include applications of text analytics and natural language processing for comparing responses from various generative AI models in the context of statistical reasoning.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "href": "sp/editors-blog/posts/2022/10/18/meet-the-team.html#editorial-board",
    "title": "Meet the team",
    "section": "",
    "text": "Sophie Carr (chair)\n\n\n\nI am the founder and owner of Bays Consulting. I trained as an engineer and took a PhD in Bayesian belief networks, and have worked in data analytics ever since. Or to put it another way, I have made a living out of finding patterns. I am the vice-president for education and statistical literacy at the RSS, officially one of the World’s Most Interesting Mathematicians and was a member of the first cohort of data scientists to achieve the new, defined standard of professionalism award from the Alliance for Data Science Professionals.\nI am delighted to be chairing the editorial board of the new data science project from the RSS and am excited to be a part of this project as it evolves into a key resource for all data science practitioners and leaders. To make this a place that helps everyone learn and develop within this field, I’d like to encourage all practitioners, no matter what stage of their career, to submit the type of resource they learn best from (whether that be an article, some code, a dataset, a case study or a problem/exercise to solve) on a topic that is important to them – from ethics to analysis plans through to tips on how code. Whatever it is you’re working on that you care about, I’d like to ask you to become an active part of the wonderful community of data scientists by sharing your knowledge.\nYaxin Bi\n\n\n\nI am a reader in the Artificial Intelligence Research Centre, School of Computing at Ulster University, UK. My research interests include machine learning, data science and ensemble methods underpinned with the Dempster-Shafer theory of evidence. I have led on developing data analytics for anomaly/change detection within satellite data, as well as text analytics for document categorisation and sentiment analysis. I have served a number of international conferences as general chair and technical program committee chair. I am an associate editor for the International Journal of Intelligence Systems and a core steering committee member for the series of International Conference on Knowledge Science, Engineering and Management.\nAs a member of this editorial board, I am passionate about contributing my expertise to the development of data science and its real-world applications. I look forward to sharing my experience with others in the data science community, in particular to address ethical challenges posed by large language models and to inspire new curriculum development in the field of data science.\nLee Clewley\n\n\n\nI am head of applied AI in GSK’s AI and Machine Learning Group, R&D. I began my career as an astrophysicist, initially working out the mass of our galaxy, before pondering the bigger universe. After six years at Oxford as a post-doc lecturer publishing in theoretical cosmology, I entered the very real world of manufacturing at GSK. For the first five years I applied statistical modelling techniques across manufacturing, such as the first end-to-end continuous manufacturing prototype for tablets. The past decade has been spent as a lead data scientist delivering high value projects across R&D and manufacturing.\nI joined this editorial board because the impulse to assemble and present complex data science ideas to a wide range of folks has never left me. I have been a data scientist leader since it became a distinct profession but also have a decent understanding of classical and modern predictive analytics tools and statistics. I have spent a good deal of my adult life teaching students and non-technical adults alike.\nI am passionate about delivering useful, pragmatic data science ideas and products to a wide range of people. I enjoy trying to communicate complex scientific information simply. Alongside my peers in the team, I want to support and develop data scientists at whatever stage in their career. I want to help cut through the hype and nonsense to give the best advice possible in a highly respected institution like the RSS.\nJonathan Gillard\n\n\n\nI am a professor at the School of Mathematics, Cardiff University, where I am also research group lead for statistics. I have a history of publications in statistical methods and an interest in the theoretical underpinnings of data science, but I have also worked with industry on applied and practical projects. Recent industrial partners of mine include the Office for National Statistics (ONS) and the National Health Service, on projects such as anomaly detection and understanding heterogeneity. Indeed, I am academic chair for Cardiff University’s strategic partnership with the ONS which serves to spur and catalyse collaboration between both organisations.\nI am excited to see what this site can achieve. I’m particularly keen to support articles describing the latest, cutting-edge methodology, as well as contributions from data professionals in industry who can explain how data science has managed to offer insights into important problems. Data science is a broad church and I want to ensure that the full array of work in this area is represented on this site. I think the diversity of the editorial board will help promote this objective.\nWillis Jensen\n\n\n\nI’m a statistician and leader with over 20 years of experience applying data science methods to real-world challenges in business, quality, product development, manufacturing, and supply chain. I hold a PhD in statistics from Virginia Tech and an MS in statistics from Brigham Young University (BYU).\nCurrently, I lead a business intelligence team at CHG Healthcare in the USA and serve as an Adjunct Professor of Statistics at BYU, where I get to share my enthusiasm for data with students. A long-time member and volunteer for the ASA, I represent the association on the Real World Data Science board.\nThroughout my career, I’ve built and led high-performing teams, developed data-drive solutions, and helped organizations become more analytically mature. While I enjoy the technical depth of statistical modeling, I’m most passionate about using data to solve meaningful business problems. With over 25 publications focused on practical applications, I specialize in bridging the gap between business and the broad tent of data science—helping leaders ask better questions and act on insights.\nHollie Johnson\n\n\n\nI am a data scientist at the National Innovation Centre for Data (NICD), based in Newcastle upon Tyne. Following my undergraduate degree in mathematics, I worked as a software developer both in industry and as a technical research assistant in academia. I later joined the Centre for Doctoral Training in Cloud Computing for Big Data at Newcastle and obtained a PhD in topological data analysis in 2020. Now at the NICD, I specialise in transferring statistics and machine learning skills into industry, through collaborative data science projects.\nI am excited to be a member of the editorial board and look forward to seeing Real World Data Science develop into a valuable source of information for aspiring data scientists and professionals alike. I would particularly encourage submissions that demonstrate the use of data science in SMEs and the non-profit sector, as well as perspectives from those with non-standard backgrounds.\nHarvey Lewis\n\n\n\nI am a senior technology leader, with a diverse background spanning rocket science, data science and research. I have 30 years of experience in artificial intelligence and other emerging technologies and am currently pioneering the use of AI in Ernst & Young’s tax and law practice. I’m a former member of the Open Data User Group, the Public Sector Transparency Board and the Advisory Committee to the All-Party Parliamentary Group on AI. I am a member of techUK’s leadership committee for data analytics and AI, and an honorary senior visiting fellow at The Bayes Business School in London.\nI’m passionate about data science but I’m also a fierce advocate for human skills, which are as often underrated as AI is over-hyped. As a member of the editorial board, I’m keen to explore the interplay between artificial and human intelligence in businesses. I’m going to encourage all data scientists to think about the fundamentally human aspects of their work, such as trust and safety, so that we maintain perspective and proportionality in the face of ever-more sophisticated technology.\nMonnie McGee\n\n\n\nI received my B.A. in Mathematics and English from Austin College and completed my M.A. and Ph.D. in Statistics at Rice University. My primary research interests lie in the statistical analysis of high-throughput biological data, with emphasis on the development and application of methods for background correction, normalization, and inference procedures tailored to the unique data structures arising in technologies such as RNA-Seq, microarrays, and flow cytometry.\nI have extensive expertise in the analysis of data constrained to the simplex, and my methodological contributions include the development of inference procedures for compositional data using the Nested Dirichlet Distribution. In parallel, I maintain an active research agenda in sports analytics, particularly for event-driven sports including track and field, competitive diving, swimming, and gymnastics. My work in this area focuses on ranking models, performance trajectories, and judge reliability.\nMost recently, I have extended my research to include applications of text analytics and natural language processing for comparing responses from various generative AI models in the context of statistical reasoning.\nDetlef Nauck\n\n\n\nI am a BT Distinguished Engineer and the head of AI and data science research for BT’s Applied Research Division located at Adastral Park, Ipswich, UK. I have over 30 years of experience in AI and machine learning and lead a programme spanning the work of a large team of international researchers who develop capabilities underpinning future AI systems. A key part of this work is to establish best practices in data science and machine learning, leading to the deployment of responsible and auditable AI solutions that are driving real business value.\nI am a computer scientist by training and hold a PhD and a Postdoctoral Degree (Habilitation) in machine learning and data analytics. I am also a visiting professor at Bournemouth University and a private docent at the Otto-von-Guericke University of Magdeburg, Germany. I have published 3 books and over 120 papers, and I hold 15 patents and have 30 active patent applications.\nI am passionate about promoting best practice in data science and believe that in the UK the RSS is the ideal professional body to pursue this goal. For me, Real World Data Science is an opportunity to share my experience and inspire a new generation of data scientists.\nFatemeh Torabi\n\n\n\nI am a research officer and data scientist at Health Data Research UK and a fellow of the RSS. My background is in mathematical statistics and health data science, and my research interests span novel analytical and computational methods for statistical inference in panel data and population health. I am supporting the development of the Real World Data Science platform in the context of health with a specific focus on how health data can be harnessed through data linkage and analysis to answer important questions and improve the lives of our population.\nIsabel Sassoon\n\n\n\nI am a senior lecturer in computer science (data science) at Brunel University and the programme lead for the data science and analytics MSc programme. My research interests are in data-driven automated reasoning and its transparency and explainability, which brings in data science and artificial intelligence with applications within the health space. I am also championing open science and reproducible analysis in both my research and teaching. I have a PhD in informatics from King’s College London and it was on the topic related to the use of AI to support statistical model selection. Prior to Brunel I was a teaching fellow and research associate at King’s College London and before that I worked for more than 10 years as a data science consultant in industry, including 8 years at SAS UK. \nI have been working, researching, consulting and teaching in the data science space for a while and I am passionate about the domain and its applications. I am always interested in sharing and hearing what else is being done to support, inform and inspire all those studying and working in the field of data science. I look forward to sharing case studies, how-to guides and data science profiles through this website."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/10/18/meet-the-team.html#past-team-members",
    "href": "sp/editors-blog/posts/2022/10/18/meet-the-team.html#past-team-members",
    "title": "Meet the team",
    "section": "Past team members",
    "text": "Past team members\nBrian Tarran\nBrian is the founding editor of Real World Data Science and former head of data science platform for the Royal Statistical Society (RSS). He worked for the RSS from 2014 to 2024, initially as editor of Significance Magazine, a joint publication of the RSS, the American Statistical Association and the Statistical Society of Australia. He launched Real World Data Science in October 2022. He is a former editor of Research-Live.com and was launch editor of Impact magazine, both published by the Market Research Society."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/01/themes.html",
    "href": "sp/editors-blog/posts/2022/12/01/themes.html",
    "title": "Four themes for potential contributors to think about",
    "section": "",
    "text": "We’ve had a fantastic early response to our call for contributions, and it has been pleasing to see and hear how our plans for Real World Data Science chime with the wants and needs of the data science community. But one question we’ve been asked frequently is: “What particular topics are you most interested in?”\nThe honest answer to that question is this: we’re interested in any and all topics that are of interest and importance to you, the data science community at large. However, we thought it might be helpful to identify some themes around which potential contributors could construct different types of content.\nThese themes are outlined below. If you’d like to discuss any of them further, please do not hesitate to contact us."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "href": "sp/editors-blog/posts/2022/12/01/themes.html#can-data-science-save-the-world",
    "title": "Four themes for potential contributors to think about",
    "section": "Can data science save the world?",
    "text": "Can data science save the world?\nEarth today faces major challenges – from the global to the regional to the local, and from the natural and physical to the social and digital. We have rich sources of data to help us understand many of these challenges, and there are teams of data scientists around the world who are working with, analysing, and extracting insights from that data in the hope of delivering positive lasting change.\nOn Real World Data Science we want to highlight this vital work, through case studies of data science projects and applications in such areas as:\n\nmonitoring and mitigating climate change and biodiversity loss\nbuilding sustainable futures\nsafeguarding public health and developing new medical treatments\nunderstanding human happiness and wellbeing\nidentifying and preventing online harms\nmeasuring national, regional, and local economies\n\nAs well as exploring the benefits that data science can deliver, we also want to have an informed conversation about the unintended negative consequences that can arise without careful consideration of data ethics and responsibilities."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "href": "sp/editors-blog/posts/2022/12/01/themes.html#what-is-a-data-scientist",
    "title": "Four themes for potential contributors to think about",
    "section": "What is a data scientist?",
    "text": "What is a data scientist?\nDon’t be misled by the title of this theme. Definitions abound, but we’re not interested in establishing the exact boundaries of what a data scientist is or isn’t. Rather, our goal is to profile actual working data scientists. We want to hear about their skillsets, their experiences, and their career journeys so far. We want to learn about the ways in which they work, who they work with, the challenges they face, and their thoughts on where data science is heading next.\nIf you’re a working data scientist and you are happy to share your own career story, please get in touch."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "href": "sp/editors-blog/posts/2022/12/01/themes.html#statistical-ideas-all-data-scientists-need-to-know",
    "title": "Four themes for potential contributors to think about",
    "section": "Statistical ideas all data scientists need to know",
    "text": "Statistical ideas all data scientists need to know\nStatistics is a crucial component of data science, but not all data scientists have a background in statistics. For those just starting out in their data science careers, or for those coming in from other fields, we want to highlight some of the statistical ideas that are absolutely vital to know.\nWe’re particularly interested in explainers that serve as an introduction to these ideas, alongside which we’ll be looking to publish exercises and example datasets to help people put what they’ve learned into practice.\nWe are also keen to explore the origins of modern data science techniques, including tracing their roots back to some of the foundational ideas in statistics and other disciplines."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "href": "sp/editors-blog/posts/2022/12/01/themes.html#whats-happening-in-the-world-of-data-science",
    "title": "Four themes for potential contributors to think about",
    "section": "What’s happening in the world of data science?",
    "text": "What’s happening in the world of data science?\nData science is such a fast-moving, fast-developing field that it’s difficult to stay on top of all the latest news and developments. But racing to keep up can be counterproductive. It leaves little time to sit back and reflect on what the genuinely important new developments are, and what these might mean for data science longer term.\nOn Real World Data Science, we want to create a space for people to have these conversations – to step outside the news hype cycle, to ask big questions about what’s happening in the field, and to discuss new papers and ideas that otherwise might be lost amid the daily rush and noise.\nSo, if you have thoughts to share, a question you want to ask, or a new paper you want to talk about (one you’ve not written yourself, of course!), let us know.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Four themes for potential contributors to think about.” Real World Data Science, December, 1 2022. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/09/LLM-news.html",
    "href": "sp/editors-blog/posts/2022/12/09/LLM-news.html",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "",
    "text": "Two weeks ago, I posted a Q&A with our editorial board member Detlef Nauck about large language models (LLMs), their drawbacks and risks. And since then we’ve had several big new announcements in this space. First came news from Meta (the company that owns Facebook) about Galactica, an LLM trained on scientific papers. This was followed by another Meta announcement, about Cicero, an AI agent that is apparently very good at playing the game Diplomacy. And then came perhaps the biggest launch of them all: ChatGPT from OpenAI, an LLM-based chatbot that millions of people have already started talking to.\nFollowing these stories and the surrounding commentaries has been something of a rollercoaster ride. ChatGPT, for example, has been greeted in some quarters as if artificial general intelligence has finally arrived, while others point out that – impressive though it is – the technology is as prone to spout nonsense as all LLMs before it (including Galactica, the demo of which was quickly taken offline for this reason). Cicero, meanwhile, has impressed with its ability to play a game that is (a) very difficult and (b) relies on dialogue, cooperation, and negotiation between players. It blends a language model with planning and reinforcement learning algorithms, meaning that it is trained not only on the rules of the game and how to win, but how to communicate with other players to achieve victory.\nTo help me make sense of all these new developments, and the myriad implications, I reached out to Harvey Lewis, another of our editorial board members and a partner in EY’s tax and law practice."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "href": "sp/editors-blog/posts/2022/12/09/LLM-news.html#qa",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Q&A",
    "text": "Q&A\nHarvey, when I spoke with Detlef, he mentioned that one of the reasons we’re seeing investment in LLMs is because there’s this belief that they are somehow the route to artificial general intelligence (AGI). And there were headlines in some places that would perhaps convince a casual reader that AGI had been achieved following the release of ChatGPT. For example, the Telegraph described it as a “scarily intelligent robot who can do your job better than you”. What do you make of all that?\nHarvey Lewis (HL): My personal view is that you won’t get to artificial general intelligence using just one technique like deep learning, because of the problematic nature of these models and the limitations of the data used in their training. I’m convinced that more general intelligence will come from a combination of different systems.\nThe challenge with many LLMs, as we’ve seen repeatedly, is that they’ve no real understanding of language or concepts represented within language. They’re good at finding patterns in semantics and grammatical rules that people use in writing, and then they use those patterns to create new expressions given a prompt. But they’ve no idea whether these outputs are factual, inaccurate or completely fabricated. As a consequence, they can produce outputs that are unreliable, but which sound authoritative, because they’re just repeating a style that we expect to see.\nOver the past couple of weeks, Twitter has been full of people either showing off astoundingly impressive outputs from LLMs, or examples of truly worrying nonsense. One example of the latter is when ChatGPT was asked to “describe how crushed porcelain added to breast milk can support the infant digestive system”. This made me think of a recent headline from VentureBeat, which asked whether AI is moving too fast for ethics to catch up. Do you think that it is?\nHL: I find that to be an interesting philosophical question, because ethics does move very slowly, for good reason. When you think of issues of bias and discrimination and prejudice, or misinformation and other problems that we might have with AI systems, it shouldn’t be a surprise that these can occur. We’re aware of them. We’re aware of the ethical issues. So, why do they always seem to catch us by surprise? Is it because we have teams of people who simply aren’t aware of ethical issues or don’t have any appreciation of them? This points – for me, at least – in the direction of needing to have philosophers, ethicists, theologians, lawyers working in the technical teams that are developing and working on these systems, rather than having them on the periphery and talking about these issues but not directly involved themselves. I think it’s hugely important to ensure that you’ve got trust, responsibility, and ethics embedded in technical teams, because that’s the only way it seems that you can avoid some of these “surprises”.\nWhen situations like these occur, I’m always reminded of Dr Ian Malcolm’s line from Jurassic Park: “…your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they should.” The mindset seems to be, let’s push the boundaries and see what we can do, rather than stop and think deeply about what the implications might be.\nHL: There’s a balance to be struck between these things, though, right? Firstly, show consideration for some of the issues at the outset, and secondly, have checks and balances and safeguards built into the process by which you design, develop and implement these systems. That’s the only way to create the proper safeguards around the systems. I don’t think that there’s any lack of appreciation of what needs to be done; people have been talking about this now for quite a long time. But it’s about making sure organisationally that it is done, and that you’ve got an operating model which bakes these things into it; that the kinds of principles and governance that you want to have around these systems are written, publicised, and properly policed. There should be no fear of making advances in that kind of a context.\nAlso, I think open sourcing these models provides a way forward. A lot of large language models are open for use and for research, but aren’t themselves open sourced, so it’s very difficult to get underneath the surface and figure out exactly how they work. But with open source, you have opportunities for researchers, whether they’re technical or in the field of ethics, to go and investigate and find out exactly what’s going on. I think that would be a fantastic step forward. It doesn’t take you all the way, of course, because a large amount of the data that these systems use is never open sourced, so while you might get an understanding of the mechanics, you have no idea of what exactly went into them in the first place. But open sourcing is a very good way of getting some external scrutiny. It’s about being transparent, which is a core principle of AI ethics and responsible AI.\n\n\n\n\n\n\n\nAn image created by the Stable Diffusion 2.1 Demo. The model was asked to produce an image with the prompt, “Text from an old book cut up like a jigsaw puzzle with pieces missing”.\nThinking about LLMs and their questionable outputs, should there not be ways for users to help the models produce better, more accurate outputs?\nHL: There are, but there are also problems here too. I’ve been having an interesting dialogue with ChatGPT this morning, asking it about quantum computing.1 For each response to a prompt, users are encouraged to provide feedback on whether or not an output is good. But you’re only provided with the usual thumbs-up/thumbs-down ratings; there’s nothing nuanced about it. So, for example, I asked ChatGPT to provide me with good analogies that help to explain quantum computing in simple terms. The first analogy was a combination lock, which is not a good analogy. The chatbot suggested that quantum computing is like a combination lock in which you can test all of the combinations at the same time, but I don’t know any combination locks where you can do this – being able to check only one combination at a time is the principal security mechanism of a combination lock! I asked it again for another analogy, and it suggested a coin toss where, when the coin is spinning in the air, you can see both heads and tails simultaneously but it isn’t until you catch the coin and then show its face that one of the states of the coin is resolved. That is a good analogy – it’s one I’ve also used myself. Now, the challenge I can see with a lot of these feedback approaches is that unless I know enough about quantum computing to understand that a combination lock is not a good analogy whereas a coin toss is, how am I to provide that kind of feedback? They’re relying to an extent on the user being able to make a distinction between what is correct and what is potentially incorrect or flawed, and I think that’s not a good way of approaching the problem.\nFinal question for you, Harvey. There’s a lot of excitement around GPT-4, which is apparently coming soon. The rumour mill says it will bring a leap forward in performance. But what do you expect we’ll see – particularly with regards to the issues we’ve talked about today?\nHL: I’ve likened some of the large language models and their approach of “bigger is better” to the story of the Tower of Babel – trying to reach heaven by building a bigger and bigger tower, basically. That is not going to achieve the objective of artificial general intelligence, no matter how sophisticated an LLM might appear. That said, language is a fascinating area. I’m not a linguist, but I spend a lot of my time on natural language processing using AI systems. Language responds very well to AI because it is pattern-based. We speak using patterns, we write using patterns, and these can be inferred by machines from many examples. The addition of more parameters in the models allows them to understand patterns that extend further into the text, and I suspect that outputs from these kinds of models are going to be largely indistinguishable from the sorts of things that you or I might write.\nBut, I also think that increasing the number of parameters runs a real risk – and we’re starting to see this in other generative models – where prompts become so specific that the models aren’t actually picking up on patterns, they are picking up on specific instances of training data and text they’ve seen before. So, buried amongst these fantastically written articles on all kinds of subjects are going to be more examples of plagiarism, which is a problem; more examples of spelling mistakes and other kinds of issues, because these are also patterns which are going to possibly be observed.\nThis introduces potentially a whole new breed of problems that the community has to deal with – as long as they don’t get fixated upon the height of the tower and the quality of some of the examples that are shown, and realise that there are some genuine underlying difficulties and challenges that need to be solved.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “LLMs in the news: hype, tripe, and everything in between.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "href": "sp/editors-blog/posts/2022/12/09/LLM-news.html#footnotes",
    "title": "LLMs in the news: hype, tripe, and everything in between",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI also had a conversation with ChatGPT. Read the transcript.↩︎"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/10/10/october-events.html",
    "href": "sp/editors-blog/posts/2023/10/10/october-events.html",
    "title": "Join Real World Data Science at three events this October!",
    "section": "",
    "text": "Summer 2023 for us was a blur of excellent data science and statistics events. There was the Joint Statistical Meetings in Toronto, the Royal Statistical Society Conference in Harrogate, and posit::conf(2023) in Chicago. But if that wasn’t enough, autumn promises more good stuff, and more opportunities to meet with Real World Data Science in person and online.\n\nAn introduction to Real World Data Science\nDate: Monday, October 16, 2023 Time: 12 pm – 1 pm Location: Online\nNext week is Members’ Week at the Royal Statistical Society (RSS), and the RSS calendar is full of events targeted at members – prospective members, new members, and established members. Kicking things off on Monday lunchtime is an online event to introduce Real World Data Science. We’ll discuss the aims of this project, our guiding ethos and content plans, and we’ll explain the various ways in which people can contribute to the site.\nChances are, if you’re reading this blog, you won’t need much of an introduction to Real World Data Science. But do please help spread the word to potential new readers, and encourage them to register for this free event.\n\n\nNHS-R Community Annual Conference\nDate: Tuesday, October 17, 2023 Time: 9:30 am – 10:00 am Location: Edgbaston Stadium, Birmingham (in person) and online\nIt’s a real honour for us to be invited to give a keynote talk at this annual gathering of the NHS-R Community, a group dedicated to promoting the use of R in the National Health Service. Our talk is titled, “Forging community links: NHS-R, the Royal Statistical Society and Real World Data Science,” and we’ll explain how the Real World Data Science project came about, how we embraced open-source tools and the idea of collaborative content development, and why there’s so much to be gained from sharing data science case studies across domains.\n\n\nEvaluating artificial intelligence: How data science and statistics can make sense of AI models\nDate: Tuesday, October 31, 2023 Time: 4 pm – 6 pm Location: RSS, London (in person only)\nReal World Data Science has partnered with colleagues and volunteers across the RSS to organise another AI panel debate, following up on the AI discussion at the RSS Annual Conference.\nThis event forms part of the AI Fringe programme of events, which coincides with the UK government’s AI Safety Summit on 1–2 November.\nOur free event focuses on big questions around AI model evaluation, which will also be a key topic of discussion at the summit. One of the government’s stated objectives is for the summit to identify “areas for potential collaboration on AI safety research, including evaluating model capabilities and the development of new standards to support governance,” and so we’ll be asking:\n\nWhat should AI evaluation look like?\nHow will it work in practice?\nWhat metrics are most important?\nWho gets to decide all of this?\n\nRegister via the RSS website to attend this free in-person event, chaired by RSS president Andy Garrett. Panellists will be announced soon, so stay tuned!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Join Real World Data Science at three events this October!” Real World Data Science, October 10, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/10/02/asa-partner.html",
    "href": "sp/editors-blog/posts/2023/10/02/asa-partner.html",
    "title": "American Statistical Association joins Real World Data Science as partner",
    "section": "",
    "text": "The first version of Real World Data Science was launched almost one year ago by the Royal Statistical Society (RSS). As we approach our first birthday, we’re delighted to announce that the American Statistical Association (ASA) has become a partner in this project.\nASA shares our goal of developing Real World Data Science as a free and beneficial resource for the entire data science community – one that informs, inspires and strengthens the community by bringing together students, practitioners, leaders, and educators to share knowledge about real-world applications of data science.\nThe data science profession is geographically and academically diverse. We believe that Real World Data Science can best achieve its goal of being a trusted, go-to resource for all data scientists if a range of partner organisations work together to develop the site and its content, so we’re thrilled that ASA is taking the first step with us towards fulfilling this vision.\nRon Wasserstein, executive director of the American Statistical Association, shared: “We are delighted to be partnering with RSS on Real World Data Science. This is important for our community and serves to further strengthen our valuable relationship with RSS.”\nSarah Cumbers, chief executive of the Royal Statistical Society, commented: “We’re thrilled to have ASA on board as a partner for Real World Data Science. We have big plans for the project and this partnership will help us achieve these by allowing us to reach more of the data science community and strengthen our content offering.”\nAs part of this new partnership with ASA, we will shortly welcome two ASA members to our editorial board. So, on behalf of the entire – and soon-to-be expanded – editorial board, we’d like to say a huge thanks to ASA for their support and endorsement of Real World Data Science.\nASA members, groups, and sections interested in contributing to the site are encouraged to review our call for contributions and to contact us via email or our social media channels to discuss content ideas.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “American Statistical Association joins Real World Data Science as partner.” Real World Data Science, October 2, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "href": "sp/editors-blog/posts/2023/01/18/rwds-at-rss-conference.html",
    "title": "We’re taking Real World Data Science on the road",
    "section": "",
    "text": "Real World Data Science has booked its first conference appearance! This September, we’ll be part of the data science stream of the RSS International Conference.\nOur session, “Real World Data Science Live”, will feature talks and discussions based on content published on this site. In particular, we’re looking to share compelling examples of how data science is being used to solve real-world problems.\nIf you’re thinking about contributing to Real World Data Science, or have already made a submission, do let us know whether you’d be interested in taking part in this in-person event. There are only a handful of speaker slots available, so please get in touch ASAP!\nThe conference takes place 4-7 September 2023, in Harrogate, Yorkshire. Keynote speakers include Anuj Srivastava, a Florida State University professor with research interests in statistical computer vision, functional data analysis, and shape analysis, and other invited topic sessions in the data science stream are:\n\nGitHub: Version control for research, teaching and industry\nSurrogate-assisted uncertainty quantification of complex computer models\nGetting your work to work\nBest practices for the analysis and visualisation of Google Trends data\n\nSee the RSS International Conference 2023 website for more details.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “We’re taking Real World Data Science on the road.” Real World Data Science, January, 18 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/05/26/future.html",
    "href": "sp/editors-blog/posts/2023/05/26/future.html",
    "title": "What’s the future of data science and AI in an LLM world?",
    "section": "",
    "text": "The impact ChatGPT and large language models (LLMs) are having on the practice and profession of data science is something we discussed recently with data scientists from Unilever, BT, Deliveroo, and others. So, it was interesting to hear a perspective this week from Osama Rahman, director of the Data Science Campus at the UK Office for National Statistics.\nSpeaking Tuesday at an online event, Rahman mused on how LLMs brought both potential benefits and risks. For example, those who can already code can code more efficiently now with the help of LLM-powered tools. However, those same tools also allow non-coders to code – and inexpert use of tools and code presents risks. How do we guard against this, he was asked. “I don’t know,” came the response, “other than you have to observe and clampdown on it.”\nSuch problems are by no means new or unique to the post-ChatGPT era, of course. As someone with a background in economics, Rahman said he has, over the years, observed “inexpert uses of economics.” His advice was to “make sure experts are plugged in” – to teams, conversations, decision-making processes, etc. – “and are seen as the experts in the use of these tools.”\nThe discussion was wide-ranging, and also took in questions on whether data scientists have the right skills at this moment – “Skills evolve, it’s just a natural process… We need to keep a culture of curiosity…” – and whether enough is being done to address ethical issues – “My key issue is that ethical frameworks need a lot more discussion and debate than it takes to put out a new tool… I don’t have much to add, other than that there is a problem.”\nHowever, two questions – and answers – jumped out at me as particularly interesting. Rahman was asked: Have we delivered on the promise of data science from 5 years ago? “No, but that’s because expectations were wrong,” he said. “Data science wasn’t going to completely and utterly transform government. But where it has delivered is in an evolving set of tools, people, and skills coming in and allowing us to do impactful stuff. It hasn’t delivered on the false promise that it would change the world, but it has delivered a lot.”\nHe was also asked: How will data science and AI have changed the world in 5–10 years? “I’m not sure it will,” he said. “It will do certain things. It will allow us to address certain analytical problems more efficiently.” Rahman then offered a salutary reminder. Email once made life more efficient; now, we’re all at risk of “death by email.”\nWe’ll be sure to update this post with a link to a video or other recording of the event, if/when it becomes available. For now, be sure to check out our two-part discussion on LLMs and data science:\n\nHow is ChatGPT changing data science?\nLarge language models: Do we need to understand the maths, or simply recognise the limitations?\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “What’s the future of data science and AI in an LLM world?” Real World Data Science, May 26, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/03/15/AI-screening.html",
    "href": "sp/editors-blog/posts/2023/03/15/AI-screening.html",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "",
    "text": "When ChatGPT launched in December 2022, it wasn’t long before users highlighted the tool’s potential as a homework aid. Pop an essay question into ChatGPT’s prompt box or feed your creative writing task to the AI instead, et voila – your work is done!\nIn reality, of course, things aren’t quite so simple. ChatGPT, like other large language models, has an unfortunate habit of making stuff up – fine for creative writing, perhaps; not so good for a history essay. Outputs need to be checked and verified if you want to guarantee a good mark on your assignments. But while ChatGPT can’t – and shouldn’t – be trusted completely, many have found that it can help lighten the homework load.\nWith ChatGPT’s user count crossing the 100 million mark last month, it’s understandable that worries about an explosion of AI-written text have proliferated in many professions, including education. Some education systems have decided to ban the use of ChatGPT. Other educators have adopted a more relaxed approach. Writing in Scientific American, law professor John Villasenor argued:\nVillasenor makes a valid point. But experience tells us that not every student is going to use these tools ethically. Some will pursue the path of least resistance and will attempt to present ChatGPT’s outputs as their own. So, the question becomes: Is it possible to tell the difference between human-generated text and AI-generated text?"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "href": "sp/editors-blog/posts/2023/03/15/AI-screening.html#spot-the-difference",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Spot the difference",
    "text": "Spot the difference\nOne answer to that question comes from OpenAI, makers of ChatGPT. On January 31, they launched a classifier “to distinguish between text written by a human and text written by AIs from a variety of providers”.\nOpenAI introduces the classifier by saying that reliably detecting all AI-written text is “impossible”. But it goes on to say:\n\n“… we believe good classifiers can inform mitigations for false claims that AI-generated text was written by a human: for example, running automated misinformation campaigns, using AI tools for academic dishonesty, and positioning an AI chatbot as a human.”\n\nOpenAI stresses that the current version of the classifier “should not be used as a primary decision-making tool”, and users should take that statement to heart – especially if they are planning to vet student homework with it. In evaluations, OpenAI reports that its classifier correctly identifies AI-written text as “likely AI-written” only 26% of the time, while human written text is incorrectly labelled as AI-written 9% of the time.\nThese two reported numbers are important. They are, respectively, the classifier’s true positive rate and the false positive rate. The former is the conditional probability of a positive result given that a piece of text is AI generated; the latter is the conditional probability of a positive result given that a piece of text is not AI generated. However, neither piece of information directly addresses the question that will be of most interest to teachers: “If a piece of homework is flagged as ‘likely AI-written’ by the OpenAI classifier, what is the probability that it actually is AI-written?”\nTo answer this question, we need to flip the conditional probabilities – from “the probability of positive test given text is AI generated” to “the probability text is AI generated given positive test”. Bayes’ theorem provides a formula for doing just that, as described in this 2017 article by Tim Brock, published by Significance magazine.\nAs Brock’s article demonstrates, versions of this problem are familiar to medical statisticians, who often find themselves having to explain screening test outcomes – specifically, the probability that a person has disease X given that they have tested positive for said disease. This probability depends on the prevalence of a disease and the sensitivity and specificity of the test, and Brock defines these terms as follows:\n\n\nPrevalence\n\nThe proportion of the population being tested that are affected by a given condition.\n\n\n\nSensitivity\n\nThe proportion of patients with the condition being screened for that are correctly identified as having the condition.\n\n\n\nSpecificity\n\nThe proportion of patients without the condition being screened for that are correctly identified as not having the condition.\n\n\n\nSensitivity and specificity are also referred to as, respectively, the true positive rate (mentioned earlier) and the true negative rate.\nWe know from OpenAI’s own evaluations that out of 100 pieces of AI-written text, only around 26 would be correctly classified as “likely AI-written”, so the classifier’s sensitivity is 26%. And out of 100 pieces of human-written text, around 9 would be incorrectly classified as AI written, meaning 91 would be correctly classified as not AI written, so specificity is 91%. But the big piece of information we don’t have is prevalence: What proportion of homework assignments are written by AI?\nThis prevalence figure is likely to vary based on where students live, what age they are, their level of interest in AI tools and technologies, and many other factors. A poll of Stanford University students by The Stanford Daily, for example, found that 17% of respondents used ChatGPT for final assignments or exams in the fall quarter – though it reports that “only about 5% reported having submitted written material directly from ChatGPT with little to no edits”.\nSo, let’s assume for the moment that 5% of homework assignments are AI-generated. If you were screening 1,000 pieces of homework with the OpenAI classifier, you’d see something close to the following results:\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n13\n86\n864\n37\n\n\n\n\nThe figures below show the results graphically as proportions of (a) all tests and (b) all positive tests. (All plots are produced using Python and the matplotlib package; code and functions are available from this GitHub repository.)\n\n\n\n\n\n\n\nFigure 1a: Classifier test results as a percentage of all tests, assuming 5% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 1b: Classifier test results as a percentage of all positive tests, assuming 5% prevalence of AI-written homework.\nFrom Figure 1b, we see that if the classifier delivers a “likely AI-written” result, the chance that the text is AI-written is only about 13%. This is the classifier’s positive predictive value at the assumed 5% prevalence.\nIf we reproduce our figures using a prevalence rate of 17%, also from the Stanford survey, the chance that a positive result is a true positive is now about 37%.\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n44\n75\n755\n126\n\n\n\n\n\n\n\n\n\n\n\nFigure 2a: Classifier test results as a percentage of all tests, assuming 17% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 2b: Classifier test results as a percentage of all positive tests, assuming 17% prevalence of AI-written homework.\nYet another survey, this one from Intelligent.com, claims that 30% of college students have used ChatGPT for written homework. Plugging this number into our calculations, the chance that a positive test result is a true positive is now slightly better than 50/50.\n\n\n\n\n\n\n\n\n\n\n\n\nTrue positives\nFalse positives\nTrue negatives\nFalse negatives\n\n\n\n\nResults\n78\n63\n637\n222\n\n\n\n\n\n\n\n\n\n\n\nFigure 3a: Classifier test results as a percentage of all tests, assuming 30% prevalence of AI-written homework.\n\n\n\n\n\n\n\nFigure 3b: Classifier test results as a percentage of all positive tests, assuming 30% prevalence of AI-written homework."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "href": "sp/editors-blog/posts/2023/03/15/AI-screening.html#determining-guilt",
    "title": "OpenAI’s text classifier won’t calm fears about AI-written homework",
    "section": "Determining ‘guilt’",
    "text": "Determining ‘guilt’\nIf a test has a positive predictive value of just over 50% (at an assumed prevalence rate of 30%), does that provide a reasonable basis on which to accuse someone of getting ChatGPT to do their homework? That depends on who you ask. If we look to the legal system for guidance, in civil cases like personal injury claims or contract disputes judges typically make decisions on the so-called “balance of probabilities”. This is generally assumed to mean if we are more than 50% sure of someone’s “guilt” in this context, that might be sufficient to find against them. However, in criminal law, a higher standard applies: “beyond reasonable doubt”. Legal scholars have long wrestled with how to quantify this in probabilistic terms, and surveys of judges put “beyond reasonable doubt” somewhere in the range of being 80% to 99% certain of guilt – see, for example McCauliff (1982) and Solan (1999).\nIt is at this standard of evidence that OpenAI’s classifier shows its limitations. For example, if we flip Bayes’ theorem around, we find that to achieve a positive predictive value of at least 80%, the prevalence rate needs to be at least 58%. For a positive predictive value of 90%, prevalence needs to be 76%. (Verify these figures for yourself: Python code and functions are available from this GitHub repository).\nThus far in our calculations, we’ve set prevalence according to estimates of the percentage of students who use ChatGPT for their homework. But, according to statistician and science writer Robert Matthews, individual students could justifiably complain about having their guilt decided on this basis. “It’s like deciding someone is guilty of a crime just because they happen to live in an area notorious for criminal gangs,” he says. Instead, the guilt of individual students should be decided using an estimate of the chances that they would use ChatGPT for that particular homework assignment.\nLooked at in this way, Matthews says, “You already have to be pretty convinced of a person’s ‘guilt’ even before applying the classifier if you want to put the evidence ‘beyond reasonable doubt’. Bayes’ theorem highlights the need to be really clear about what you mean by the ‘accuracy’ of a test, and about what question you want the test to answer.”\nSo, here’s a question that teachers will be asking if they are worried about ChatGPT-generated homework: “Has the piece of text I’m marking been written by AI?” If those same teachers use the OpenAI classifier to try to answer that question, they will no doubt expect that something classified as “likely AI-written” is more likely to be AI-written than not. However, as it stands now – and as our examples above have shown – users can’t be confident that’s the case. In education terms, this particular ‘test’ is a long way from scoring top marks.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “OpenAI’s text classifier won’t calm fears about AI-written homework.” Real World Data Science, March 15, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/07/07/LDW.html",
    "href": "sp/editors-blog/posts/2023/07/07/LDW.html",
    "title": "Teaching a chatbot about love, and other adventures from London Data Week",
    "section": "",
    "text": "London Data Week wraps up on Sunday, and what a week it’s been! Kudos to organisers Sam Nutt and Jennifer Ding for the huge amount of energy and passion they invested in making this idea a reality, and I’m looking forward to seeing what they have in store for us next year.\nMy highlights of the week? Well, of course, I really enjoyed being part of the event that was hosted at the Royal Statistical Society on Tuesday. The Statisticians for Society workshop brought together charities and statisticians to explore ways in which data and statistics can support third sector organisations to deliver on their charitable aims as well as demonstrate to communities and funders the impact they are having. There’s a nice selection of case studies of successful past projects on the RSS website, and hopefully the London Data Week event will result in several new additions to this collection in due course.\nI wasn’t able to attend this event myself, but I’m really looking forward to viewing the outputs of the Better Images of AI workshop, which was also held on Tuesday. Real World Data Science has used several of the group’s images to illustrate past articles (here, here and here), so I’m excited to see what gets added to the image gallery in the coming weeks.\nSticking with the AI theme, I also got to explore the “AI: Who’s Looking After Me?” exhibition at the Science Gallery, where I found myself unexpectedly moved by one installation in particular – an artificial landfill of broken and discarded tablets and smart speakers, explaining matter-of-factly, but with an unmistakable air of mournfulness, that they had been replaced “by a newer model that is better because it is lighter, or heavier, or bigger, or smaller…”. Fortunately I was able to cheer myself up with another exhibit, which tasks visitors with helping a chatbot to define and understand love.\nLater on in my visit to the Science Gallery (which was actually last Thursday, before London Data Week officially began), I listened to a panel debate on “Building Better AI in the Open”, featuring Margaret Mitchell of HuggingFace, Lara Groves of the Ada Lovelace Institute, and Irini Papadimitriou of FutureEverything, facilitated by artist and machine learning design researcher Caroline Sinders. A recording of the panel is below, and well worth a watch for discussion of:\n\nthe advantages of open source versus closed source\nthe role of public participation in AI\nwhat transparency in AI development should look like\nissues of accountability in AI applications.\n\nJennifer Ding followed up the panel with a thoughtful post on the benefits of open source AI, and for more on trustworthy AI – and the need for transparency, explainability, and fairness – check out Maxine Setiawan and Mira Pijselman’s recent Real World Data Science article, “Trusted AI: translating AI ethics from theory into practice”.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Benjamin Davies on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Teaching a chatbot about love, and other adventures from London Data Week.” Real World Data Science, July 7, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/07/27/schools-outreach.html",
    "href": "sp/editors-blog/posts/2023/07/27/schools-outreach.html",
    "title": "‘Go out and talk about data science, particularly to schoolchildren’",
    "section": "",
    "text": "Rachel Hilliam, statistics professor at The Open University, used her inaugural lecture this month to make “a real plea” to the data science community “for outreach into schools”, to help build excitement and awareness of the promise and potential for careers in data science.\n“We have a plethora of jobs that we cannot fill in data science at the moment,” said Hilliam. “We’ve had all sorts of initiatives in terms of trying to retrain people, and that’s great, and those are gaps that we need to plug. But unless we get that pipeline coming through, we’re always going to have this problem at the top.”\nHilliam, who is chair of the Alliance for Data Science Professionals, wants schoolchildren and teachers to be made more aware of the benefits of, and opportunities for, data science careers. “Let me tell you,” she said, “if you go out into a school and say, ‘Do your kids want to be a data scientist?’, the teachers will look at you and go, ‘A what?’. They have no idea, generally, that data science actually exists, which is a shame.”\nBut there are plentiful opportunities to introduce data science to children, Hilliam suggests. She began her talk by saying that: “Data is everywhere – in every single thing that we do, in all of our walks of life.” And she concluded by saying: “Whatever it is that these kids are interested in, […] there is lots of data out there, so there is absolutely no reason why we can’t excite children in a career in data science. So, that’s where I’d like to finish. Go out and talk about data science, particularly to schoolchildren!”\nWatch the lecture in full below or on YouTube. Skip to 17:04 for the start of the talk.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Kenny Eliason on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “‘Go out and talk about data science, particularly to schoolchildren.’” Real World Data Science, July 27, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/09/12/template.html",
    "href": "sp/editors-blog/posts/2023/09/12/template.html",
    "title": "Contributors: check out the new Real World Data Science template repo on GitHub",
    "section": "",
    "text": "Thinking of contributing to Real World Data Science but not sure how to get started? Help is at hand, thanks to Finn-Ole Höner. The Amsterdam-based business data science student has created a template repository on GitHub that allows anyone to create Real World Data Science content in our house style and format.\nIn this repository you’ll find two example Quarto (.qmd) documents, which is the main file type we use for generating site content. The “content-brief.qmd” file is a template for developing article ideas to discuss with our site editors, and the “report.qmd” file is a standard article template. Within that article template you’ll find examples of the range of Quarto features that we use, as well as the code you need to make use of them yourself.\nThese documents can be edited using tools including Visual Studio Code and R Studio. For details on how to work with Quarto documents, see the Quarto website. Once article drafts are finished they can be rendered into HTML format, and the output files will be displayed in the Real World Data Science style, thanks to the inclusion of our stylesheets in the template repository. This is a great way for contributors to see what their content will look like on Real World Data Science before anything is published.\nTo get started, head on over to the RWDS_post_template repository and click the “Use this template” button. Also, be sure to review our contributor guidelines for advice on how to integrate the .qmd templates into the content development and submission workflow.\nHuge thanks to Finn-Ole Höner for building this valuable resource for Real World Data Science contributors.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Roman Synkevych on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Contributors: check out the new Real World Data Science template repository on GitHub.” Real World Data Science, September 12, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html",
    "href": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "",
    "text": "What a difference a year makes! That was the general tone of the conversation coming out of techUK’s Digital Ethics Summit this week. At last year’s event, ChatGPT was but a few days old. An exciting, enticing prospect, sure – but not yet the phenomenon it would soon become. My notes from last year include only two mentions of the AI chatbot: Andrew Strait of the Ada Lovelace Institute expressing concern about the way ChatGPT had been released straight to the public, and Jack Stilgoe of UCL warning of the threat such technology poses to the social contract – public data trains it, while private firms profit.\nA lot has happened since last December, as many of the speakers at Wednesday’s summit pointed out. UNESCO’s Gabriela Ramos commented on how the UK’s AI Safety Summit, US President Joe Biden’s executive order on AI, and other international initiatives had brought about “a change in the conversation” on AI risk, safety, and assurance. Simon Staffell of Microsoft spoke of “a huge amount of progress” being made, building from principles into voluntary actions that companies and countries can take.\nLuciano Floridi of Yale University described 2023 as a “remarkable, eventful year which we didn’t quite expect,” with various international efforts helping to build consensus on what needs to be done, and what needs to be regulated, to ensure the benefits of AI can be realised while harms are minimised. Camille Ford of the Centre for European Policy Studies noted that while attempts at global governance of AI make for a “crowded space” – with more than 200 documents in circulation – there are at least principles in common across the various initiatives, focusing on aspects such as transparency, reliability and trustworthiness, safety, privacy, and accountability and liability.\nHowever, in some respects, we’ve not come as far as we could or should have over the past 12 months. Ford, for instance, called for more conversation on AI safety, and a frank discussion about on whose terms AI safety is defined. Not only are there the risks and harms of AI outputs to consider, but also environmental harms, exploitative labour practices, and more besides. Echoing the Royal Statistical Society’s recent AI debate, Ford said we need to focus on the risks we face now, rather than being consumed by discussions about the existential and catastrophic risks of AI – which, for many, are still firmly in the realm of science fiction.\nThere also remains “a big mismatch” between the AI knowledge and skills that reside within tech companies and that of other communities, said Zeynep Engin of Data for Policy. And many speakers were clear that the global south needs a more prominent voice in the AI debate."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html#regulatory-approaches",
    "href": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html#regulatory-approaches",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "Regulatory approaches",
    "text": "Regulatory approaches\nThe UK government’s AI Safety Summit has been criticised for focusing too much on the hypothetical existential risks of AI. But, on regulation at least, there was broad agreement that the UK’s principles- and sector-based approach, outlined in a March 2023 white paper, is the right one. That’s not to say it’s perfect: discussions were had about whether regulatory bodies would be adequately funded to regulate the use of AI in their sectors, while Hetan Shah of the British Academy wondered “where was the golden thread” linking the AI white paper to the AI Safety Summit and its various pronouncements, including plans for an AI Safety Institute. (On the Safety Institute in particular, Lord Tim Clement-Jones was sceptical of yet another body being drafted in to debate these issues – a point made by panellists at the RSS’s recent AI debate.)\nDelegates also got to hear from the UK’s Information Commissioner directly. John Edwards delivered a keynote address in which he acknowledged the huge excitement surrounding the benefits AI promises to bring, while cautioning that deployment and use of AI must be done in accordance with existing rules on data protection and privacy. The technology may be new, he said, but the same old data rules apply: “Our legislation is founded on technology-neutral principles of general application. They are capable of adapting to numerous new technologies, as they have over the last 30 years and will continue to do.”\nHe warned that noncompliance with data protection rules and regulations “will not be profitable,” and that persistent misuse of AI and personal data for competitive advantage would be punished. Edwards concluded by saying that AI is built on the data of human individuals and should therefore be used to improve their lives, and not put them or their personal data at risk."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html#elections-in-an-era-of-generative-ai",
    "href": "sp/editors-blog/posts/2023/12/08/digital-ethics-summit.html#elections-in-an-era-of-generative-ai",
    "title": "AI and digital ethics in 2023: a ‘remarkable, eventful year’",
    "section": "Elections in an era of generative AI",
    "text": "Elections in an era of generative AI\nOne major looming risk is the use of generative AI to create mis- and disinformation during election campaigns. Hans-Petter Dalen of IBM suggested that next year is perhaps the biggest year for elections in the history of mankind, with votes due in the UK, US, and India, to name but a few. Generative AI represents not a new threat, he said, but an “amplified” one – a point further developed by Henry Parker of Logically.ai. Parker spoke of the risk of large-scale breakdown in trust due to mis- or disinformation campaigns. Thanks to AI tools, he said, we are now seeing the “democratisation of disinformation.” What once might have cost millions of dollars and required a team of hundreds of people can now be done much more cheaply and with fewer human resources. As the Royal Society’s Areeq Chowdhury said, the challenge of disinformation has only become harder.\nAsked how to counter this, Dalen said that if he were a politician, “I would certainly get my own blockchain and all my content would have been digitally watermarked from source – that’s what the blockchain does.” But digital watermarking is only part of the answer, added Parker. Identifying mis- and disinformation is both a question of provenance and of dissemination. Logically.ai is using AI as a tool to analyse behaviours around the circulation of mis- and disinformation, Parker said – positioning AI as but one solution to a problem it has helped exacerbate.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Kajetan Sumila on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “AI and digital ethics in 2023: a ‘remarkable, eventful year.’” Real World Data Science, December 8, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/24/rss-conference.html",
    "href": "sp/editors-blog/posts/2023/08/24/rss-conference.html",
    "title": "RSS Conference preview: Evaluating AI, machine learning, and data visualisation",
    "section": "",
    "text": "The Royal Statistical Society International Conference takes place in Harrogate, England, this September (Monday 4 to Thursday 7). Real World Data Science will be in attendance, and we’ve helped organise a couple of sessions we’d like to tell you about.\n\nEvaluating AI: How data science and statistics can shape the UK’s AI strategy\nDate: 6 September Time: 9:00 am - 10:20 am Room: Auditorium (moved from Queens Suite 8)\nThe launch of ChatGPT less than a year ago is a milestone moment in the story of artificial intelligence. Overnight, large language models were transformed from research projects into consumer products, now used by millions each month. The capabilities are impressive, the productivity gains undeniable. But, what of the downsides? These are issues societies, governments, and individuals are now starting to reckon with.\nIn March 2023, the UK government published a white paper promising a “pro-innovation approach” to AI regulation, while also acknowledging the risks AI poses to “people’s privacy, their human rights or their safety” and “concerns about the fairness of using AI tools to make decisions which impact people’s lives”. The Royal Statistical Society, in response, has called for investment in a centre for AI evaluation methodology, arguing that users of AI systems should be able to judge the trustworthiness of claims made by AI companies as well as the outputs of their systems.\nWhat should AI evaluation look like? How will it work in practice? What metrics are most important, and – crucially – who gets to decide this? Join us for a special panel debate at the RSS International Conference, where these questions, and more, will be discussed.\n\n\nBest Practices for Data Visualisation: How to make data outputs more readable, accessible, and impactful\nDate: 5 September Time: 11:40 am - 1:00 pm Room: Auditorium\nThe Royal Statistical Society (RSS) has published a new guide, “Best Practices for Data Visualisation”, containing insights, advice, and examples (with code) to make data outputs more readable, accessible, and impactful. The guide is written primarily for contributors to Royal Statistical Society publications – including Significance magazine, the Journal of the Royal Statistical Society Series A, and Real World Data Science – but the information and advice within is also of broad relevance and use for any data visualisation task.\nIn the first half of this conference session, authors Andreas Krause, Nicola Rennie, and Brian Tarran will introduce the guide and its key recommendations, and there will be a short demo of how to use the new {RSSthemes} R package. For the second half of the session, attendees will be invited to share feedback with the authors, propose ideas, and start developing new and expanded sections of the guide. Attendees will be shown how to work with the guide’s source files and collaborate via GitHub, so feel free to bring along a laptop and become a contributor!\nFor more information, see rss.org.uk/datavisguide and the RSS Conference website.\n\n\nDiscussion Meeting: Probabilistic and statistical aspects of machine learning\nDate: 6 September Time: 5:00pm - 7:00 pm Room: Auditorium\nWe haven’t helped organise this session, but we are interested to see it. Two papers will be presented for discussion and debate. Paper 1 is “Automatic Change-Point Detection in Time Series via Deep Learning” by Jie Li, Paul Fearnhead, Piotr Fryzlewicz, and Tengyao Wang, while Paper 2 is “From Denoising Diffusions to Denoising Markov Models” by Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, and Arnaud Doucet. Preprints of both papers are available now via the RSS Discussion Meetings webpage, and you can also hear more about the session in this interview with Adam Sykulski, RSS Discussion Papers editor.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by jcw1967, licenced under a Creative Commons Attribution 2.0 Generic (CC BY 2.0) licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “RSS Conference preview: Evaluating AI, machine learning, and data visualisation.” Real World Data Science, August 24, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/07/editors-relaunch.html",
    "href": "sp/editors-blog/posts/2025/07/07/editors-relaunch.html",
    "title": "We’re Back: Real World Data Science Relaunches",
    "section": "",
    "text": "You may have noticed our brief hiatus. Since publishing our series on AI - which covered the quest for human-level intelligence, data-set risks, ethical considerations and much more - the ongoing deluge of content and commentary on AI in the wider world has continued to accelerate. This year has seen a surge in developments that sit at the intersection of data science and AI: from the growing use of synthetic data to overcome privacy and bias challenges, to the rise of multi-modal models that demand increasingly sophisticated data engineering and integration techniques. The emergence of Agentic AI has sparked new conversations around data provenance, model interpretability, and the reproducibility crisis in machine learning. Meanwhile, the meteoric rise of open-source disruptor DeepSeek triggered stock-market ruptures and industry panic, before cyber-attacks, data leaks and a failed safety test complicated its standing - a parable for the volatility of the space, where data governance failures and safety oversights can rapidly derail innovation. Meanwhile, governments worldwide are investing heavily in national data infrastructure and advanced analytics capabilities, while grappling with how best to regulate a field that is evolving faster than policy can keep up.\nThe world of data science has been a dizzying place over the last few months, so we took a moment to pause and take stock. In the face of rapid change and constant noise, it felt important to reflect with intention on the role Real World Data Science can and should play in this evolving landscape. Now we’re back - ready to rejoin the conversation with renewed clarity and purpose.\nAs a project from the Royal Statistical Society, in partnership with the American Statistical Association, we are backed by organisations with nearly two centuries of history in championing sound evidence, rigorous methodology and ethical data use. These values form the foundation of our next phase - distilled into the essential pillars: data, evidence and decision. With an esteemed editorial board representing the cutting-edge of industry and academia, and an international network of practitioners working at the coalface of modern data science, we are uniquely placed to navigate the pace and complexity of today’s data-driven world. Real World Data Science will meet that world in real time with the RSS’s trademark steadying presence, bridging the gap between rigorous analysis and real-time relevance.\nWe are now returning with a slightly refreshed site, encompassing four editorial sections:\nThe Pulse - covering news, updates and real-time commentary\nApplied Insights - exploring how data science is used to solve real-world problems in business, public policy and beyond\nFoundations & Frontiers - unpicking the ideas behind the impact: the concepts, tools and methods that make data science possible\nPeople & Paths - offering strategic reflections on careers, leadership and professional evolution in data science.\nYou can find the full details of these sections, plus guidance around submitting to them, in our new Call for Submissions.\nDespite these updates, we remain committed to providing content that is useful and relevant for practicing data scientists seeking to learn good practices in the field and new potential applications.\nThe choices we make now will shape how data and AI serve society for years to come. If you’re working on the front lines of these changes, whether through research, practice, or critical reflection, we invite you to share your insights and help us build a future for data science that is thoughtful, transparent and grounded in real world understanding.\nMeet the Team\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2025 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nReal World Data Science Editorial Board. 2025. “We’re Back: Real World Data Science Relaunches” Real World Data Science, July 7, 2025. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "href": "sp/editors-blog/posts/2024/01/18/cherry-blossom.html",
    "title": "When will the cherry trees bloom? Get ready to make and share your predictions!",
    "section": "",
    "text": "The 2024 International Cherry Blossom Prediction Competition will open for entries on February 1, and Real World Data Science is once again proud to be a sponsor.\nContestants are invited to submit predictions for the date cherry trees will bloom in 2024 at five different locations – Kyoto, Japan; Liestal-Weideli, Switzerland; Vancouver, Canada; and Washington, DC and New York City, USA.\nThe competition organisers will provide all the publicly available data they can find for the bloom dates of cherry trees in these locations, and contestants will then be challenged to use this data “in combination with any other publicly available data (e.g., climate data) to provide reproducible predictions of the peak bloom date.”\n“For this competition, we seek accurate, interpretable predictions that offer strong narratives about the factors that determine when cherry trees bloom and the broader consequences for local and global ecosystems,” say the organisers. “Your task is to predict the peak bloom date for 2024 and to estimate a prediction interval, a lower and upper endpoint of dates during which peak bloom is most probable.”\nSo that organisers can reproduce the predictions, entrants must submit all data and code in a Quarto document.\nThere’s cash and prizes on offer for the best entries, including having your work featured on Real World Data Science. Head on over to the competition website for full details and rules.\nAnd, if you are looking for some inspiration, check out this tutorial on the law of the flowering plants, written by Jonathan Auerbach, a co-organiser of the prediction competition.\nGood luck to all entrants!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Photo by AJ on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “When will the cherry trees bloom? Get ready to make and share your predictions!” Real World Data Science, January 18, 2024. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "href": "sp/editors-blog/posts/2024/01/03/posit-conf-video.html",
    "title": "Creating a web publication with Quarto: the Real World Data Science origin story",
    "section": "",
    "text": "When I attended posit::conf(2023) in Chicago last year, I gave a talk about creating Real World Data Science using Quarto, the open source publishing system developed by Posit. That talk is now online, along with all the other conference talks and keynotes.\nMy talk, “From Journalist to Coder: Creating a Web Publication with Quarto,” is embedded below. You can also find a selection of talks on our posit::conf highlights blog. The full conference playlist is on YouTube.\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Creating a web publication with Quarto: the Real World Data Science origin story.” Real World Data Science, January 03, 2024. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "href": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "",
    "text": "The UK government this week announced a £10 million investment to “jumpstart regulators’ AI capabilities” as part of its commitment to a “pro-innovation approach to AI regulation.” But will this be sufficient to answer criticisms that it has so far been “too slow” to give regulators the tools they need to police the growing usage of AI?\nIt was March last year when a Department for Science, Innovation and Technology (DSIT) white paper first set out the government’s principles- and context-based approach to regulating artificial intelligence. This proposed to focus regulatory attention on “the context in which AI is deployed” rather than target specific technologies. Under this model, existing regulators, including the Information Commissioner’s Office, Ofcom, and the Competition and Markets Authority, would be responsible for ensuring that technologies deployed within their domains adhered to established rules – e.g., data protection regulation – and a common set of principles:\nThe approach was broadly well received, as was clear from a debate at techUK’s Digital Ethics Summit last December. However, concerns were expressed about whether regulators would be funded sufficiently to meet the expectations set out in the March white paper. Also, the Royal Statistical Society, in its response to the white paper, worried that “splitting responsibilities for regulating the use of AI between existing regulators does not meet the scale of the challenge,” and that “central leadership is required to give a clear, coherent and easily communicable framework that can be applied to all sectors.”\nWhile the DSIT white paper proposed that a range of “central functions” be created to support regulators, evidence presented to a House of Lords inquiry last November suggested that regulators “did not appear to know what was happening” with these mooted teams and were “keen to see progress” on this front.\nIn reporting the outcomes of its inquiry last week, the House of Lords Communications and Digital Committee concluded that government was being “too slow” to give regulators the tools required to meet the objectives set out in the white paper, and that “speedier resourcing of government‑led central support teams is needed.”\n“Relying on existing regulators to ensure good outcomes from AI will only work if they are properly resourced and empowered,” the committee said.\nThe £10 million funding for regulators announced this week is therefore likely to be welcomed. Money is earmarked to “help regulators develop cutting-edge research and practical tools to monitor and address risks and opportunities in their sectors, from telecoms and healthcare to finance and education,” according to a DSIT press release. Speaking on February 6 at a hearing of the Lords Communications and Digital Committee, Michelle Donelan, Secretary of State for Science, Innovation and Technology, said that the government would “stay on top” of what regulators need to be able to fulfil their responsibilities for regulating the use of AI in their sectors."
  },
  {
    "objectID": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "href": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#consultation-response",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Consultation response",
    "text": "Consultation response\nNews of the funding for regulators came as part of a long-awaited response by the government to the consultation on its AI regulation white paper. The response essentially confirmed that the government was proceeding with its principles- and context-based approach to regulating AI, having received “strong support from stakeholders across society.”\nThis approach is right for today, the government said, “as it allows us to keep pace with rapid and uncertain advances in AI.” However, it acknowledged that “the challenges posed by AI technologies will ultimately require legislative action in every country once understanding of risk has matured.”\n“Highly capable general-purpose AI systems” would, for example, present a particular challenge to the government’s current approach. It explained: “Even though some regulators can enforce existing laws against the developers of the most capable general-purpose systems within their current remits, the wide range of potential uses means that general-purpose systems do not currently fit neatly within the remit of any one regulator, potentially leaving risks without effective mitigations.”\nAs a next step in delivering on the white paper approach, the government is asking key regulators to publish an update on their strategic approach to AI by the end of April. This was welcomed by Royal Statistical Society (RSS) president Andrew Garrett, who said:\n\n“Urgency is certainly warranted, and the directive for key regulators to disclose their approach in the coming months is a positive development. Ensuring consistency and coherence not only among key regulators but also those who follow is crucial.”\n\nGarrett also reiterated the need for government to engage with statisticians and data scientists, particularly through its new AI Safety Institute (AISI). In the white paper consultation response, AISI is billed as being “fundamental to informing the UK’s regulatory framework”: it will “advance the world’s knowledge of AI safety by carefully examining, evaluating, and testing new frontier AI systems” and will also “research new techniques for understanding and mitigating AI risk.” Garrett said:\n\n“As always, fostering diversity of representation within government and regulatory bodies remains paramount; it cannot solely rely on input from major tech companies. It is especially important that the AI Safety Institute engages with a diverse array of voices, including statisticians and data scientists who play a pivotal role in both the development of AI systems and novel evaluation methodologies.”"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "href": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#risks-and-opportunities",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Risks and opportunities",
    "text": "Risks and opportunities\nCalls for a “diversity of representation within government and regulatory bodies” certainly chime with a warning bell sounded by the Lords Communications and Digital Committee last week, in the February 2 release of its inquiry report into large language models and generative AI. “Regulatory capture” by big commercial interests was highlighted as a danger to be avoided, amid concern that “the AI safety debate is being dominated by views narrowly focused on catastrophic risk, often coming from those who developed such models in the first place” and that “this distracts from more immediate issues like copyright infringement, bias and reliability.”1\nThe committee called for enhanced governance and transparency measures in DSIT and AISI to guard against regulatory capture, and for a rebalancing away from a “narrow focus on high-stakes AI safety” toward a “more positive vision for the opportunities [of AI] and a more deliberate focus on near-term risks” including cyber security and disinformation.\nIt also wants to see greater action by the government in support of copyright. “Some tech firms are using copyrighted material without permission, reaping vast financial rewards,” reads the report. “The legalities of this are complex but the principles remain clear. The point of copyright is to reward creators for their efforts, prevent others from using works without permission, and incentivise innovation. The current legal framework is failing to ensure these outcomes occur and the Government has a duty to act. It cannot sit on its hands for the next decade and hope the courts will provide an answer.”\nAgain, here’s RSS president Andrew Garrett’s take on the Lords committee report:\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Yaopey Yong on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach.” Real World Data Science, February 8, 2024. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "href": "sp/editors-blog/posts/2024/02/08/llms-whitepaper-response.html#footnotes",
    "title": "£10m for UK regulators to ‘jumpstart’ AI capabilities, as government commits to white paper approach",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee, for example, “No, AI probably won’t kill us all – and there’s more to this fear campaign than meets the eye.”↩︎"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience)\nLinkedIn: RSS Real World Data Science\nX: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#editorial",
    "href": "contact.html#editorial",
    "title": "Contact us",
    "section": "",
    "text": "Email: rwds@rss.org.uk\nGitHub: @realworlddatascience)\nLinkedIn: RSS Real World Data Science\nX: @rwdatasci",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "contact.html#advertising-and-commercial",
    "href": "contact.html#advertising-and-commercial",
    "title": "Contact us",
    "section": "Advertising and commercial",
    "text": "Advertising and commercial\n\nEmail: advertising@rss.org.uk",
    "crumbs": [
      "Contact us"
    ]
  },
  {
    "objectID": "inventors/murdock.html",
    "href": "inventors/murdock.html",
    "title": "Jack Murdock",
    "section": "",
    "text": "Born in Portland, Oregon, in 1917, Jack Murdock developed a strong entrepreneurial spirit from a young age. Unlike Vollum’s deep technical focus, Murdock’s talents lay in organization, finance, and understanding market needs. He gained valuable experience in sales and radio parts distribution, which gave him insights into the electronics market and the practical challenges faced by engineers.\nDuring World War II, Murdock served in the U.S. Coast Guard. Upon his return, he and Howard Vollum, whom he had known from their shared interest in electronics, recognized a significant unmet need for better electronic test equipment, particularly oscilloscopes."
  },
  {
    "objectID": "inventors/murdock.html#early-life-and-business-acumen",
    "href": "inventors/murdock.html#early-life-and-business-acumen",
    "title": "Jack Murdock",
    "section": "",
    "text": "Born in Portland, Oregon, in 1917, Jack Murdock developed a strong entrepreneurial spirit from a young age. Unlike Vollum’s deep technical focus, Murdock’s talents lay in organization, finance, and understanding market needs. He gained valuable experience in sales and radio parts distribution, which gave him insights into the electronics market and the practical challenges faced by engineers.\nDuring World War II, Murdock served in the U.S. Coast Guard. Upon his return, he and Howard Vollum, whom he had known from their shared interest in electronics, recognized a significant unmet need for better electronic test equipment, particularly oscilloscopes."
  },
  {
    "objectID": "inventors/murdock.html#co-founding-tektronix-and-building-a-company-culture",
    "href": "inventors/murdock.html#co-founding-tektronix-and-building-a-company-culture",
    "title": "Jack Murdock",
    "section": "Co-founding Tektronix and Building a Company Culture",
    "text": "Co-founding Tektronix and Building a Company Culture\nIn 1946, Jack Murdock and Howard Vollum co-founded Tektronix, Inc. Murdock’s role was to build the business infrastructure around Vollum’s engineering innovations. This included:\n\nSecuring Funding: Initially through personal savings and loans, to get the company off the ground.\nManufacturing Setup: Establishing efficient production lines for the intricate electronic instruments.\nSales and Marketing: Creating a distribution network and strategies to bring Tektronix products to market.\nFinancial Management: Ensuring the company’s financial stability and growth.\n\nMurdock’s keen business sense complemented Vollum’s technical genius perfectly. While Vollum focused on designing the revolutionary Model 511 oscilloscope with its triggered sweep, Murdock ensured that the company had the means to produce it efficiently, market it effectively, and support its customers.\n\nA Pioneering Employee Philosophy\nBeyond financial and operational management, Jack Murdock was also a pioneer in employee relations and corporate culture. He believed strongly in treating employees as valued partners, establishing progressive policies that were ahead of their time:\n\nProfit Sharing: Employees shared directly in the company’s success.\nExcellent Benefits: Comprehensive healthcare and other benefits.\nOpen Communication: Fostering a transparent and collaborative work environment.\nEmployee Stock Ownership: Giving employees a stake in the company.\n\nThis progressive approach not only attracted and retained top talent but also fostered a highly dedicated and motivated workforce, which was a significant factor in Tektronix’s ability to innovate and maintain its market leadership."
  },
  {
    "objectID": "inventors/murdock.html#later-years-and-philanthropy",
    "href": "inventors/murdock.html#later-years-and-philanthropy",
    "title": "Jack Murdock",
    "section": "Later Years and Philanthropy",
    "text": "Later Years and Philanthropy\nMurdock served as Tektronix’s President for many years and later as Chairman. He oversaw the company’s expansion into a global leader in electronic test and measurement.\nTragically, Jack Murdock died in a plane crash in 1984. His death was a significant loss, not only for Tektronix but also for the Portland community, where he was known for his extensive philanthropy.\nJack Murdock’s legacy is one of successful entrepreneurship, visionary business leadership, and a profound commitment to his employees and community. He built the framework that allowed Howard Vollum’s technical innovations to reach the world, proving that a strong business foundation is just as critical as technical brilliance for groundbreaking success. His influence on corporate culture, particularly in the tech industry, continues to be studied and admired."
  },
  {
    "objectID": "inventors/packard.html",
    "href": "inventors/packard.html",
    "title": "David Packard",
    "section": "",
    "text": "Born in Pueblo, Colorado, in 1912, David Packard exhibited strong leadership qualities and a practical, results-oriented mindset from a young age. He attended Stanford University, where he earned his bachelor’s degree in electrical engineering in 1934. It was at Stanford that he met Bill Hewlett, with whom he shared a passion for engineering and innovation.\nAfter graduating, Packard worked for General Electric in Schenectady, New York, gaining valuable experience in large-scale corporate operations and manufacturing. This exposure to established business practices, combined with his natural leadership, prepared him to build the organizational framework for HP."
  },
  {
    "objectID": "inventors/packard.html#early-life-and-managerial-aptitude",
    "href": "inventors/packard.html#early-life-and-managerial-aptitude",
    "title": "David Packard",
    "section": "",
    "text": "Born in Pueblo, Colorado, in 1912, David Packard exhibited strong leadership qualities and a practical, results-oriented mindset from a young age. He attended Stanford University, where he earned his bachelor’s degree in electrical engineering in 1934. It was at Stanford that he met Bill Hewlett, with whom he shared a passion for engineering and innovation.\nAfter graduating, Packard worked for General Electric in Schenectady, New York, gaining valuable experience in large-scale corporate operations and manufacturing. This exposure to established business practices, combined with his natural leadership, prepared him to build the organizational framework for HP."
  },
  {
    "objectID": "inventors/packard.html#the-garage-and-the-business-foundation-1939",
    "href": "inventors/packard.html#the-garage-and-the-business-foundation-1939",
    "title": "David Packard",
    "section": "The Garage and the Business Foundation (1939)",
    "text": "The Garage and the Business Foundation (1939)\nThe legendary partnership began in 1939 in a small garage in Palo Alto, California, with a mere $538 in capital. While Bill Hewlett focused on the technical design of their first product, the HP 200A Audio Oscillator, Dave Packard was responsible for the crucial business aspects:\n\nEstablishing the Company Structure: Setting up the initial legal and financial frameworks.\nManufacturing and Operations: Organizing the production process for their innovative instruments.\nSales and Distribution: Strategizing how to get their products to market and attract customers.\nFinancial Management: Overseeing the company’s finances and ensuring its sustainability.\n\nPackard’s pragmatic approach and strong organizational skills were indispensable in transforming Hewlett’s brilliant engineering into a commercially viable enterprise. The significant order from Walt Disney Studios for their audio oscillators was not just a technical validation but also a testament to Packard’s ability to secure key clients."
  },
  {
    "objectID": "inventors/packard.html#the-hp-way-a-revolutionary-management-philosophy",
    "href": "inventors/packard.html#the-hp-way-a-revolutionary-management-philosophy",
    "title": "David Packard",
    "section": "“The HP Way”: A Revolutionary Management Philosophy",
    "text": "“The HP Way”: A Revolutionary Management Philosophy\nDavid Packard is widely recognized as the principal architect of “The HP Way,” a management philosophy that became a hallmark of Silicon Valley. This approach was rooted in his deep belief in people and a decentralized management structure:\n\nManagement by Walking Around (MBWA): Like Hewlett, Packard believed in direct interaction with employees, fostering open communication and understanding the day-to-day challenges.\nOpen-Door Policy: Encouraging employees to approach management with ideas and concerns.\nFlat Hierarchy: Minimizing bureaucratic layers to empower employees and foster innovation.\nProfit Sharing and Benefits: Pioneering programs that shared the company’s success with its workforce, building loyalty and motivation.\nQuality and Integrity: Emphasizing ethical conduct and a relentless pursuit of product quality and customer satisfaction.\n\n“The HP Way” created an environment where creativity flourished, employees felt valued, and the company consistently delivered groundbreaking products, especially in the highly competitive test and measurement market."
  },
  {
    "objectID": "inventors/packard.html#public-service-and-later-life",
    "href": "inventors/packard.html#public-service-and-later-life",
    "title": "David Packard",
    "section": "Public Service and Later Life",
    "text": "Public Service and Later Life\nPackard served as HP’s President and CEO for many years, guiding its expansion from a small startup into a global technology giant. He also took a leave of absence from HP from 1969 to 1971 to serve as the U.S. Deputy Secretary of Defense under President Richard Nixon, demonstrating his commitment to public service.\nUpon his return to HP, he continued to influence the company’s strategic direction. He eventually stepped down from active management but remained a respected figure in the technology industry and a prominent philanthropist through the David and Lucile Packard Foundation.\nDavid Packard passed away in 1996. His legacy is one of transformative business leadership, a pioneering management philosophy that valued both innovation and humanity, and the creation of a corporate culture that became an enduring model for success. His vision laid the essential groundwork for HP’s decades of dominance, not just in test and measurement, but across the entire technology landscape."
  },
  {
    "objectID": "inventors/hewlett.html",
    "href": "inventors/hewlett.html",
    "title": "William R. Hewlett",
    "section": "",
    "text": "Born in Ann Arbor, Michigan, in 1913, Bill Hewlett moved to San Francisco, California, at an early age. His aptitude for engineering and electronics was evident during his studies. He attended Stanford University, where he received his bachelor’s degree in electrical engineering in 1934. It was at Stanford that he met David Packard, a fellow student with whom he would forge a lifelong partnership.\nHewlett furthered his education, earning an E.E. degree from MIT in 1936. His academic background, combined with a hands-on approach to problem-solving, equipped him with the deep technical knowledge and practical skills that would define HP’s early product development."
  },
  {
    "objectID": "inventors/hewlett.html#early-life-and-educational-foundations",
    "href": "inventors/hewlett.html#early-life-and-educational-foundations",
    "title": "William R. Hewlett",
    "section": "",
    "text": "Born in Ann Arbor, Michigan, in 1913, Bill Hewlett moved to San Francisco, California, at an early age. His aptitude for engineering and electronics was evident during his studies. He attended Stanford University, where he received his bachelor’s degree in electrical engineering in 1934. It was at Stanford that he met David Packard, a fellow student with whom he would forge a lifelong partnership.\nHewlett furthered his education, earning an E.E. degree from MIT in 1936. His academic background, combined with a hands-on approach to problem-solving, equipped him with the deep technical knowledge and practical skills that would define HP’s early product development."
  },
  {
    "objectID": "inventors/hewlett.html#the-garage-and-the-audio-oscillator-1939",
    "href": "inventors/hewlett.html#the-garage-and-the-audio-oscillator-1939",
    "title": "William R. Hewlett",
    "section": "The Garage and the Audio Oscillator (1939)",
    "text": "The Garage and the Audio Oscillator (1939)\nThe legendary story of Hewlett-Packard began in 1939 in a small garage in Palo Alto, California. With just $538 in capital, Hewlett and Packard officially founded their company.\nHewlett’s first major contribution was the design of their inaugural product: the HP 200A Audio Oscillator. This revolutionary instrument was a low-distortion audio oscillator that was more stable and significantly more affordable than anything else on the market at the time. Hewlett’s innovative design used a light bulb as a temperature-sensitive resistor in a feedback loop, which was a clever and cost-effective way to stabilize the output.\nThe HP 200A’s commercial breakthrough came when Walt Disney Studios ordered eight modified units (the HP 200B) for testing audio systems during the production of the film Fantasia. This order validated Hewlett’s engineering brilliance and the company’s potential."
  },
  {
    "objectID": "inventors/hewlett.html#the-hp-way-and-technical-leadership",
    "href": "inventors/hewlett.html#the-hp-way-and-technical-leadership",
    "title": "William R. Hewlett",
    "section": "The “HP Way” and Technical Leadership",
    "text": "The “HP Way” and Technical Leadership\nAs HP grew, Bill Hewlett played a pivotal role in shaping the company’s unique corporate culture, known as “The HP Way.” While both founders contributed to this philosophy, Hewlett’s influence was particularly strong in fostering an environment of engineering excellence, innovation, and direct engagement with technical challenges.\nHe championed:\n\n“Management by Wandering Around” (MBWA): Regularly walking through the engineering labs and production floors to interact directly with employees, understand their work, and offer guidance.\nOpen Labs: Encouraging engineers to work on personal projects, which often led to new HP products.\nDecentralization and Trust: Empowering engineers with autonomy and responsibility.\n\nHewlett’s vision ensured that HP remained at the forefront of technical innovation, consistently developing cutting-edge test and measurement instruments, calculators, and eventually computers. His focus was always on creating practical, high-performance tools that genuinely helped engineers and scientists."
  },
  {
    "objectID": "inventors/hewlett.html#legacy",
    "href": "inventors/hewlett.html#legacy",
    "title": "William R. Hewlett",
    "section": "Legacy",
    "text": "Legacy\nBill Hewlett served as President and CEO of HP for many years, eventually becoming Chairman of the Executive Committee. He retired from active management in 1978 but remained influential within the company.\nHe passed away in 2001, leaving behind an indelible mark on the technology world. Hewlett’s combination of brilliant engineering, an innovative spirit, and a commitment to people-centric management built a company that not only produced iconic products but also defined many of the best practices of Silicon Valley. His legacy is etched into the very fabric of electronic engineering, proving that a deep understanding of technology, coupled with visionary leadership, can change the world."
  },
  {
    "objectID": "inventors/braun.html",
    "href": "inventors/braun.html",
    "title": "Karl Ferdinand Braun",
    "section": "",
    "text": "Early Life and Education\nBorn on June 6, 1850, in Fulda, in the Electorate of Hesse (now Germany), Braun showed a great interest in science from an early age. He studied physics, chemistry, and mathematics at the University of Marburg before continuing his studies at the University of Berlin. He earned his doctorate there in 1872 with a thesis on the vibrations of elastic strings.\n\n\nMajor Scientific Contributions\nBraun’s work spans several areas of physics, but two of his inventions particularly stand out.\n\nThe Rectifier Effect (1874)\nWhile studying the conductivity of crystals, Braun discovered in 1874 that a point contact between a metal wire and a galena crystal (lead sulfide) did not conduct current equally in both directions. He had just discovered the rectifier effect, a fundamental principle of semiconductors. This discovery is the basis of the diode, an essential component of all modern electronics, and was later used in the first radio receivers, the famous “cat’s whisker” radios.\n\n\nThe Cathode Ray Tube (1897)\nIn 1897, while a professor at the University of Strasbourg, Braun developed the first cathode ray tube, also known as the “Braun tube.” By using electromagnetic fields to deflect an electron beam directed at a phosphorescent screen, he managed to visualize the waveform of an alternating current for the first time. This device is the direct ancestor of the oscilloscope, as well as the television screens and computer monitors that dominated the 20th century.\n\n\n\nThe Nobel Prize in Physics\nBraun also took a keen interest in the emerging technology of wireless telegraphy. He made crucial improvements to Guglielmo Marconi’s transmission system by developing a coupled oscillating circuit that significantly increased the power and range of radio transmitters.\nFor these revolutionary works, Karl Ferdinand Braun shared the 1909 Nobel Prize in Physics with Guglielmo Marconi “in recognition of their contributions to the development of wireless telegraphy.”\n\n\nLater Life\nIn 1915, during World War I, Braun traveled to the United States to testify in a radio patent lawsuit. When the United States entered the war against Germany in 1917, he was considered an “enemy alien” and was not allowed to leave the country. He died in Brooklyn, New York, on April 20, 1918, before the end of the war.\nHis legacy endures through his inventions that laid the foundations of the electronic and communications era."
  },
  {
    "objectID": "history.html",
    "href": "history.html",
    "title": "The History of the First Oscilloscopes",
    "section": "",
    "text": "The oscilloscope, a fundamental instrument in the field of electronics, owes its existence to a series of inventions and improvements that spanned several decades. Its history is intrinsically linked to the development of the cathode-ray tube (CRT).\n\nThe Origins: The Cathode-Ray Tube\nThe crucial invention that made the oscilloscope possible was the cathode-ray tube. In 1897, German physicist Karl Ferdinand Braun modified a Crookes tube by adding a phosphorescent screen and electrostatic deflection plates. This device, known as the “Braun tube,” allowed the visualization of a waveform by deflecting an electron beam to “draw” on the screen. It was the first time an electrical waveform could be directly observed, although the device was still rudimentary.\n\n\nThe First Commercial Oscilloscope\nThe very first commercial oscilloscope was developed by the American company General Radio in the 1920s. However, it was Allen B. DuMont, an American engineer, who truly popularized the instrument.\nIn 1931, DuMont improved cathode-ray tube technology, making them much more durable, with a lifespan of over 1000 hours, compared to around a hundred previously. This advancement made oscilloscopes commercially viable. His company, DuMont Laboratories, began producing and selling oscilloscopes starting in 1934. The DuMont 164 model is often considered one of the first practical and widely adopted oscilloscopes.\n\n\nThe Key Innovation: Triggering\nEarly oscilloscopes had a major limitation: the waveform display was often unstable, continuously scrolling across the screen. The real technological turning point came with the invention of the trigger circuit.\nIt was Tektronix, founded in 1946, that revolutionized the market with its Tektronix 511 model in 1947. This oscilloscope integrated a trigger circuit (or triggered sweep) that allowed the horizontal sweep of the electron beam to synchronize with the input signal. The result: a stable and clear image of the waveform, which could be analyzed in detail. This innovation transformed the oscilloscope into a true precision measurement instrument.\n\n\nEvolution and Impact\nFrom then on, oscilloscopes continuously improved:\n\nIncreased bandwidth to visualize increasingly faster signals.\nIntroduction of multiple channels to compare several signals simultaneously.\nAddition of measurement functions (voltage, frequency, etc.) directly on the screen.\n\nThese early instruments laid the groundwork for modern electronics, allowing engineers and scientists to “see” electrical signals for the first time. They were essential to the development of radio, television, radar, and later, computers and all digital technology."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Onthesignal",
    "section": "",
    "text": "This site is dedicated to the signal processing."
  },
  {
    "objectID": "LICENCE.html",
    "href": "LICENCE.html",
    "title": "Licence",
    "section": "",
    "text": "The website onthesignal.com (“the website”) and the “Onthesignal” and “Onthesignal” brands and logos are copyright © Onthesignal.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\nCopyright and licence\n© Onthesignal\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#content",
    "href": "LICENCE.html#content",
    "title": "Licence",
    "section": "",
    "text": "The website onthesignal.com (“the website”) and the “Onthesignal” and “Onthesignal” brands and logos are copyright © Onthesignal.\nCopyright and licence terms for published articles and any associated videos, images, or other material can be found at the end of each article page, e.g.:\nCopyright and licence\n© Onthesignal\nWe make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s).\nYou are not permitted to republish the website in its entirety.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "LICENCE.html#software-and-services",
    "href": "LICENCE.html#software-and-services",
    "title": "Licence",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for the website are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct.\nThe website is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nReal World Data Science is hosted by GitHub Pages.\nFonts used on Real World Data Science are served by the Google Fonts API. This is to improve site loading speeds and font compatibility across devices. Review the Google Fonts Privacy and Data Collection statement.\nUser comments and reaction functionality is provided by giscus, a comments system powered by GitHub Discussions. Use of this comment functionality is governed by the Contributor Covenant Code of Conduct.",
    "crumbs": [
      "Licence"
    ]
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Onthesignal, its editors, or other partners and funders.\nOnthesignal has prepared the content of this website responsibly and carefully. However, Onthesignal, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of Onthesignal, its editors, or other partners and funders.\nOnthesignal has prepared the content of this website responsibly and carefully. However, Onthesignal, its editors, the RSS, or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nRWDS, its editors, the RSS, or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. RWDS, its editors, the RSS, or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nRWDS, its editors, the RSS, or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nRWDS editors and contributors recommend external web links on the basis of their suitability and usefulness for our users. Selection and addition of links to our website is entirely a matter for RWDS and for RWDS alone.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by RWDS, its editors, the RSS, or other partners and funders of any product, service, policy or opinion of the organisation or individual. RWDS, its editors, the RSS, or other partners and funders are not responsible for the content of external websites.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of RWDS is final and no correspondence will be entered into.\nIf you wish to report a concern, please email contact@onthesignal.com.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown policy",
    "text": "Notice and Takedown policy\nIf you are a rights holder and are concerned that you have found material on our site for which you have not given permission, or is not covered by a limitation or exception in national law, please contact us in writing stating the following:\n\nYour contact details.\nThe full bibliographic details of the material.\nThe exact and full url where you found the material.\nProof that you are the rights holder and a statement that, under penalty of perjury, you are the rights holder or are an authorised representative.\n\nUpon receipt of notification, the ‘Notice and Takedown’ procedure is then invoked as follows:\n\nWe will acknowledge receipt of your complaint by email and will make an initial assessment of the validity and plausibility of the complaint.\nUpon receipt of a valid complaint the material will be temporarily removed from our website pending an agreed solution.\nWe will contact the contributor who deposited the material, if relevant. The contributor will be notified that the material is subject to a complaint, under what allegations, and will be encouraged to assuage the complaints concerned.\nThe complainant and the contributor will be encouraged to resolve the issue swiftly and amicably and to the satisfaction of both parties, with the following possible outcomes:\n\nThe material is replaced on our website unchanged.\nThe material is replaced on our website with changes.\nThe material is permanently removed from our website.\n\n\nIf the contributor and the complainant are unable to agree a solution, the material will remain unavailable through the website until a time when a resolution has been reached.",
    "crumbs": [
      "Terms and conditions"
    ]
  },
  {
    "objectID": "inventors/thomson.html",
    "href": "inventors/thomson.html",
    "title": "Sir J.J. Thomson",
    "section": "",
    "text": "Sir Joseph John Thomson, commonly known as J.J. Thomson, was a groundbreaking British physicist who revolutionized our understanding of atomic structure. His most celebrated achievement was the discovery of the electron in 1897, a particle that fundamentally reshaped scientific thought and paved the way for modern electronics and physics. His work with cathode rays and discharge tubes was crucial in establishing the existence of subatomic particles and earned him the Nobel Prize in Physics."
  },
  {
    "objectID": "inventors/thomson.html#early-life-and-academic-career",
    "href": "inventors/thomson.html#early-life-and-academic-career",
    "title": "Sir J.J. Thomson",
    "section": "Early Life and Academic Career",
    "text": "Early Life and Academic Career\nBorn in Cheetham Hill, Manchester, England, in 1856, J.J. Thomson displayed an exceptional intellect from a young age. He entered Owens College, Manchester (now the University of Manchester), at the remarkably young age of 14, where he studied mathematics and experimental physics. He then moved to Trinity College, Cambridge, in 1876, where he quickly distinguished himself.\nIn 1884, at just 28 years old, Thomson was appointed Cavendish Professor of Physics at the University of Cambridge, a position previously held by James Clerk Maxwell and Lord Rayleigh. He would hold this prestigious role for 34 years, during which the Cavendish Laboratory became a world-leading center for experimental physics."
  },
  {
    "objectID": "inventors/thomson.html#the-discovery-of-the-electron-1897",
    "href": "inventors/thomson.html#the-discovery-of-the-electron-1897",
    "title": "Sir J.J. Thomson",
    "section": "The Discovery of the Electron (1897)",
    "text": "The Discovery of the Electron (1897)\nThomson’s most significant work involved his experiments with cathode ray tubes (CRTs), building upon the work of scientists like William Crookes and Karl Ferdinand Braun. These tubes, which were highly evacuated glass cylinders with electrodes, produced luminous “rays” when a high voltage was applied.\nThrough a series of ingenious experiments, Thomson demonstrated several key properties of these cathode rays:\n\nDeflection by Electric Fields: Unlike previous experimenters who failed to observe it, Thomson successfully showed that cathode rays could be deflected by an electric field, in addition to magnetic fields. This deflection indicated that the rays carried a negative charge.\nCharge-to-Mass Ratio: By precisely measuring the deflection of the rays in both electric and magnetic fields, he was able to calculate the charge-to-mass ratio (e/m) of the particles composing the rays.\nIndependence of Material: Crucially, he found that this e/m ratio was the same regardless of the gas in the tube or the material of the electrodes. This suggested that these negatively charged particles were a universal component of all matter.\n\nIn 1897, Thomson announced his discovery of these “corpuscles,” which we now know as electrons. He concluded that these corpuscles were much smaller than atoms and were the fundamental units of electric charge. This was a revolutionary concept, as it shattered the prevailing belief that atoms were indivisible and the smallest units of matter."
  },
  {
    "objectID": "inventors/thomson.html#impact-and-the-plum-pudding-model",
    "href": "inventors/thomson.html#impact-and-the-plum-pudding-model",
    "title": "Sir J.J. Thomson",
    "section": "Impact and the Plum Pudding Model",
    "text": "Impact and the Plum Pudding Model\nThomson’s discovery fundamentally changed physics and chemistry: * It opened the door to subatomic physics, leading to the eventual discovery of protons and neutrons. * It provided the first real understanding of how electricity flows through matter. * It laid the groundwork for the development of virtually all modern electronic technologies, including radios, televisions, and computers, which rely on controlling the flow of electrons.\nTo explain the atom’s structure after discovering the electron, Thomson proposed the “plum pudding model” in 1904. In this model, the atom was seen as a sphere of uniformly distributed positive charge, with negatively charged electrons (the “plums”) embedded within it, like raisins in a pudding. While later superseded by Rutherford’s nuclear model, it was an important step in developing atomic theory."
  },
  {
    "objectID": "inventors/thomson.html#nobel-prize-and-legacy",
    "href": "inventors/thomson.html#nobel-prize-and-legacy",
    "title": "Sir J.J. Thomson",
    "section": "Nobel Prize and Legacy",
    "text": "Nobel Prize and Legacy\nJ.J. Thomson was awarded the Nobel Prize in Physics in 1906 “in recognition of the great merits of his theoretical and experimental investigations on the conduction of electricity by gases.”\nBeyond his own discoveries, Thomson was an exceptional mentor. Seven of his research assistants and his own son, George Paget Thomson, went on to win Nobel Prizes themselves, highlighting his profound influence on the next generation of physicists.\nSir J.J. Thomson passed away in 1940 and was buried in Westminster Abbey, near Sir Isaac Newton. His work not only revealed the existence of the electron but also initiated the exploration of the subatomic world, forever changing our perception of matter and energy."
  },
  {
    "objectID": "inventors/walter_lecroy.html",
    "href": "inventors/walter_lecroy.html",
    "title": "Walter LeCroy",
    "section": "",
    "text": "Born in 1926, Walter LeCroy earned his Ph.D. in physics from Columbia University. In the 1950s and early 1960s, he worked as a researcher at Columbia’s Nevis Laboratories, a leading high-energy physics research center.\nIn his research, he faced a major challenge: measuring extremely brief (nanosecond-order) and non-repetitive electrical pulses generated by particle detectors. The instruments of the time, primarily analog oscilloscopes, were not suitable for capturing and analyzing these fleeting signals with the required precision.\nFrustrated by the limitations of available tools, he began developing his own instruments. This quest to solve a personal research problem sowed the seeds of his future company."
  },
  {
    "objectID": "inventors/walter_lecroy.html#from-physics-laboratories-to-entrepreneurship",
    "href": "inventors/walter_lecroy.html#from-physics-laboratories-to-entrepreneurship",
    "title": "Walter LeCroy",
    "section": "",
    "text": "Born in 1926, Walter LeCroy earned his Ph.D. in physics from Columbia University. In the 1950s and early 1960s, he worked as a researcher at Columbia’s Nevis Laboratories, a leading high-energy physics research center.\nIn his research, he faced a major challenge: measuring extremely brief (nanosecond-order) and non-repetitive electrical pulses generated by particle detectors. The instruments of the time, primarily analog oscilloscopes, were not suitable for capturing and analyzing these fleeting signals with the required precision.\nFrustrated by the limitations of available tools, he began developing his own instruments. This quest to solve a personal research problem sowed the seeds of his future company."
  },
  {
    "objectID": "inventors/walter_lecroy.html#the-founding-of-lecroy-corporation-1964",
    "href": "inventors/walter_lecroy.html#the-founding-of-lecroy-corporation-1964",
    "title": "Walter LeCroy",
    "section": "The Founding of LeCroy Corporation (1964)",
    "text": "The Founding of LeCroy Corporation (1964)\nIn 1964, Walter LeCroy founded LeCroy Research Systems (which later became LeCroy Corporation) in the basement of his home in Irvington, New York. The initial goal was to design and sell cutting-edge electronic instruments for the high-energy physics community.\nThe company’s first products were modules for “digitizing” fast signals, designed for institutions like CERN and Brookhaven National Laboratory."
  },
  {
    "objectID": "inventors/walter_lecroy.html#the-key-innovation-the-digital-revolution",
    "href": "inventors/walter_lecroy.html#the-key-innovation-the-digital-revolution",
    "title": "Walter LeCroy",
    "section": "The Key Innovation: The Digital Revolution",
    "text": "The Key Innovation: The Digital Revolution\nWhile companies like Tektronix dominated the market with high-quality analog oscilloscopes, Walter LeCroy bet on a radically different approach: digitizing the signal at the source.\nLeCroy’s innovation rested on three pillars:\n\nUltra-fast Analog-to-Digital Conversion (ADC): Capturing the signal and immediately converting it into a series of digital points.\nDeep Acquisition Memory: Storing these millions of points in a deep digital memory, whereas analog oscilloscopes could only display an ephemeral trace.\nMicroprocessor Analysis: Using computational power to analyze, measure, and process the stored signal, far beyond what was possible through simple visual observation.\n\nThis approach transformed the oscilloscope from a mere waveform “viewer” into a true signal analyzer. LeCroy oscilloscopes thus became indispensable for capturing complex and rare events (“glitches”) in fields such as telecommunications, computing, and automotive."
  },
  {
    "objectID": "inventors/walter_lecroy.html#legacy",
    "href": "inventors/walter_lecroy.html#legacy",
    "title": "Walter LeCroy",
    "section": "Legacy",
    "text": "Legacy\nWalter LeCroy led his company for decades, positioning it as an undisputed leader in the field of high-end digital oscilloscopes, renowned for their high bandwidth, deep memory, and advanced analysis capabilities.\nIn 2012, the company was acquired by Teledyne Technologies, becoming Teledyne LeCroy.\nWalter LeCroy passed away in 2014, leaving behind an immense legacy. He not only built a successful company; he fundamentally changed the philosophy of electronic measurement, paving the way for the era of digital instrumentation that dominates today."
  },
  {
    "objectID": "inventors/crookes.html",
    "href": "inventors/crookes.html",
    "title": "William Crookes",
    "section": "",
    "text": "Born in London in 1832, William Crookes showed an early aptitude for science. He began his studies at the Royal College of Chemistry in 1848, under the tutelage of the renowned chemist August Wilhelm von Hofmann. After working as a superintendent of the meteorological department at the Radcliffe Observatory in Oxford, he turned his full attention to chemistry and physics."
  },
  {
    "objectID": "inventors/crookes.html#early-life-and-scientific-beginnings",
    "href": "inventors/crookes.html#early-life-and-scientific-beginnings",
    "title": "William Crookes",
    "section": "",
    "text": "Born in London in 1832, William Crookes showed an early aptitude for science. He began his studies at the Royal College of Chemistry in 1848, under the tutelage of the renowned chemist August Wilhelm von Hofmann. After working as a superintendent of the meteorological department at the Radcliffe Observatory in Oxford, he turned his full attention to chemistry and physics."
  },
  {
    "objectID": "inventors/crookes.html#key-discoveries-and-inventions",
    "href": "inventors/crookes.html#key-discoveries-and-inventions",
    "title": "William Crookes",
    "section": "Key Discoveries and Inventions",
    "text": "Key Discoveries and Inventions\nCrookes’ scientific career was marked by several significant achievements:\n\nDiscovery of Thallium (1861)\nUsing spectroscopy, a technique he significantly advanced, Crookes discovered the new element Thallium in 1861, initially by observing a bright green line in the spectrum of selenium residues. He later isolated it and determined its atomic weight.\n\n\nThe Radiometer (1875)\nIn 1875, he invented the Crookes Radiometer, a device consisting of a partial vacuum bulb containing a set of vanes, typically black on one side and silver on the other, mounted on a spindle. When exposed to light, the vanes rotate, demonstrating a fascinating interplay of light, heat, and gas molecules, though the initial explanation for its operation was debated.\n\n\nCathode Ray Tubes and “Radiant Matter” (1870s-1880s)\nCrookes’ most influential work involved his experiments with electrical discharges in highly evacuated glass tubes, which became known as Crookes tubes. He systematically studied the phenomena occurring when a high voltage was applied across electrodes inside these tubes.\nHe observed:\n\nA beam emanating from the cathode, capable of causing fluorescence where it struck the glass or other materials.\nThis “cathode ray” could be deflected by a magnetic field, suggesting it had mass and was negatively charged.\nThe rays traveled in straight lines and could cast shadows.\n\nCrookes termed the state of matter within these tubes “radiant matter,” believing it represented a fourth state of matter beyond solid, liquid, and gas. While his interpretation of “radiant matter” as individual particles wasn’t fully accepted until J.J. Thomson’s work on the electron, his meticulous observations were crucial. His work directly led to Thomson’s discovery of the electron in 1897 and provided the essential technology for Karl Ferdinand Braun’s development of the cathode ray oscilloscope."
  },
  {
    "objectID": "inventors/crookes.html#later-contributions-and-recognition",
    "href": "inventors/crookes.html#later-contributions-and-recognition",
    "title": "William Crookes",
    "section": "Later Contributions and Recognition",
    "text": "Later Contributions and Recognition\nBeyond his groundbreaking work on vacuum tubes, Crookes was also a pioneer in the study of rare-earth elements and made contributions to agricultural chemistry. He served as President of the British Association for the Advancement of Science and President of the Royal Society. He was knighted in 1897 for his scientific achievements.\nSir William Crookes passed away in 1919, leaving behind a legacy as a brilliant experimentalist whose meticulous observations and innovative apparatus profoundly advanced our understanding of electricity, matter, and light, paving the way for the electronic age."
  },
  {
    "objectID": "inventors/dumont.html",
    "href": "inventors/dumont.html",
    "title": "Allen B. DuMont",
    "section": "",
    "text": "Born in Brooklyn, New York, in 1901, DuMont showed an early fascination with technology. At the age of 10, while recovering from polio, he spent his time reading about science and engineering, particularly about radio. He obtained his amateur radio license at 12 and built his own radio receivers and transmitters.\nHe pursued higher education at Rensselaer Polytechnic Institute, graduating in 1924 with a degree in electrical engineering. After graduation, he began his career at Westinghouse Lamp Company, where he worked on improving early radio tubes."
  },
  {
    "objectID": "inventors/dumont.html#early-life-and-technical-inclination",
    "href": "inventors/dumont.html#early-life-and-technical-inclination",
    "title": "Allen B. DuMont",
    "section": "",
    "text": "Born in Brooklyn, New York, in 1901, DuMont showed an early fascination with technology. At the age of 10, while recovering from polio, he spent his time reading about science and engineering, particularly about radio. He obtained his amateur radio license at 12 and built his own radio receivers and transmitters.\nHe pursued higher education at Rensselaer Polytechnic Institute, graduating in 1924 with a degree in electrical engineering. After graduation, he began his career at Westinghouse Lamp Company, where he worked on improving early radio tubes."
  },
  {
    "objectID": "inventors/dumont.html#the-quest-for-better-cathode-ray-tubes",
    "href": "inventors/dumont.html#the-quest-for-better-cathode-ray-tubes",
    "title": "Allen B. DuMont",
    "section": "The Quest for Better Cathode Ray Tubes",
    "text": "The Quest for Better Cathode Ray Tubes\nDuMont quickly became frustrated with the limitations of existing CRTs. At the time, CRTs were expensive, unreliable, and had a very short lifespan (often just 20-30 hours). This made them impractical for widespread use in oscilloscopes and certainly for television.\nIn 1931, after being laid off from his job during the Great Depression, DuMont decided to pursue his vision of creating a superior CRT. He founded DuMont Laboratories in his basement in Upper Montclair, New Jersey, with a small loan.\nHis key innovations in CRT technology included: * Longer Lifespan: He significantly increased the tube’s vacuum, which drastically extended its operational life to over 1,000 hours. * Brighter Displays: Improved phosphors and electron gun designs led to brighter and sharper images. * Reduced Cost: His manufacturing techniques made CRTs much more affordable."
  },
  {
    "objectID": "inventors/dumont.html#revolutionizing-the-oscilloscope",
    "href": "inventors/dumont.html#revolutionizing-the-oscilloscope",
    "title": "Allen B. DuMont",
    "section": "Revolutionizing the Oscilloscope",
    "text": "Revolutionizing the Oscilloscope\nThese advancements in CRT technology had an immediate and profound impact on the oscilloscope. Before DuMont, oscilloscopes were mostly laboratory curiosities. With his improved tubes, DuMont Laboratories began producing some of the first practical and commercially viable oscilloscopes.\nThe DuMont 164 oscilloscope, introduced in 1934, is often cited as a landmark product. It was robust, reliable, and provided a clear, stable display of electrical waveforms, making it an indispensable tool for engineers in research, development, and maintenance. DuMont’s oscilloscopes were widely used in industries like radar, radio, and early computer development."
  },
  {
    "objectID": "inventors/dumont.html#a-pioneer-in-television-broadcasting",
    "href": "inventors/dumont.html#a-pioneer-in-television-broadcasting",
    "title": "Allen B. DuMont",
    "section": "A Pioneer in Television Broadcasting",
    "text": "A Pioneer in Television Broadcasting\nBeyond oscilloscopes, DuMont was a pivotal figure in the early days of television. He believed strongly in the potential of electronic television and adapted his improved CRTs for use as picture tubes.\n\nDuMont Television Network: In 1946, he launched the DuMont Television Network, one of the earliest commercial television networks in the United States. Despite facing fierce competition from established radio networks (CBS and NBC) and regulatory challenges, DuMont’s network pioneered many broadcasting innovations.\nTelevision Manufacturing: DuMont Laboratories also manufactured some of the earliest mass-produced television sets, known for their large screens and high quality."
  },
  {
    "objectID": "inventors/dumont.html#legacy",
    "href": "inventors/dumont.html#legacy",
    "title": "Allen B. DuMont",
    "section": "Legacy",
    "text": "Legacy\nAllen B. DuMont’s contributions to electronics are immense. He didn’t just invent; he innovated on existing technologies, making them practical, reliable, and commercially successful. His work with CRTs provided the foundational technology for: * Modern Oscilloscopes: Paving the way for the instruments we use today. * Radar Displays: Crucial during World War II. * Television Sets: Laying the groundwork for the consumer electronics revolution.\nHe was a visionary who understood the potential of a technology and worked tirelessly to bring it to fruition. Allen B. DuMont passed away in 1965, but his legacy as a brilliant engineer and an audacious entrepreneur continues to influence the world of electronics."
  },
  {
    "objectID": "inventors/vollum.html",
    "href": "inventors/vollum.html",
    "title": "Howard Vollum",
    "section": "",
    "text": "Born in Portland, Oregon, in 1913, Howard Vollum displayed a remarkable aptitude for electronics from an early age. His passion for radio and electronic circuits was evident in his youth. He pursued his education at Reed College in Portland, graduating in physics in 1936. After graduation, he honed his practical skills by working on radio equipment and gaining valuable experience in electronics repair and design.\nDuring World War II, Vollum served in the U.S. Army Signal Corps, where he gained extensive experience with radar technology and advanced electronic test equipment. This period exposed him to the limitations of existing oscilloscopes, which often struggled to display stable, precise waveforms, particularly for non-repetitive or high-speed signals."
  },
  {
    "objectID": "inventors/vollum.html#early-life-and-technical-aptitude",
    "href": "inventors/vollum.html#early-life-and-technical-aptitude",
    "title": "Howard Vollum",
    "section": "",
    "text": "Born in Portland, Oregon, in 1913, Howard Vollum displayed a remarkable aptitude for electronics from an early age. His passion for radio and electronic circuits was evident in his youth. He pursued his education at Reed College in Portland, graduating in physics in 1936. After graduation, he honed his practical skills by working on radio equipment and gaining valuable experience in electronics repair and design.\nDuring World War II, Vollum served in the U.S. Army Signal Corps, where he gained extensive experience with radar technology and advanced electronic test equipment. This period exposed him to the limitations of existing oscilloscopes, which often struggled to display stable, precise waveforms, particularly for non-repetitive or high-speed signals."
  },
  {
    "objectID": "inventors/vollum.html#the-founding-of-tektronix-and-the-triggered-sweep-revolution",
    "href": "inventors/vollum.html#the-founding-of-tektronix-and-the-triggered-sweep-revolution",
    "title": "Howard Vollum",
    "section": "The Founding of Tektronix and the Triggered Sweep Revolution",
    "text": "The Founding of Tektronix and the Triggered Sweep Revolution\nAfter the war, in 1946, Vollum co-founded Tektronix, Inc. with Jack Murdock. Murdock handled the business and manufacturing aspects, while Vollum, the engineering genius, focused on research and development. Their goal was clear: to build better oscilloscopes.\nVollum’s key insight was the realization that a stable, repeatable display of a waveform required precise control over the oscilloscope’s horizontal sweep. This led to his most significant invention: the triggered sweep. Instead of the continuous, free-running sweep found in earlier oscilloscopes, Vollum designed a circuit that would initiate the sweep only when the input signal reached a specific voltage level.\nThis innovation, first implemented in the Tektronix Model 511 oscilloscope launched in 1947, was a game-changer. It allowed engineers to: * Observe stable, flicker-free waveforms: Even for complex, single-shot, or very fast signals. * Precisely measure signal characteristics: As the waveform was consistently displayed at the same position. * Debug electronic circuits more efficiently: By providing a clear window into their operation.\nThe Model 511, and the subsequent models designed under Vollum’s technical leadership, set new industry standards for stability, accuracy, and ease of use."
  },
  {
    "objectID": "inventors/vollum.html#leadership-and-innovation-at-tektronix",
    "href": "inventors/vollum.html#leadership-and-innovation-at-tektronix",
    "title": "Howard Vollum",
    "section": "Leadership and Innovation at Tektronix",
    "text": "Leadership and Innovation at Tektronix\nAs President and later Chairman of the Board, Vollum maintained an intense focus on engineering excellence and innovation. Tektronix became renowned for its high-quality, high-performance oscilloscopes, attracting top engineering talent. Under his guidance, Tektronix continued to push boundaries, introducing: * Modular plug-in oscilloscopes in the 1950s, which allowed users to customize the instrument for various applications. * Storage oscilloscopes, capable of holding a waveform display for extended periods. * Advancements in bandwidth and signal fidelity.\nVollum was known for his quiet demeanor, his deep technical knowledge, and his commitment to fostering an innovative, engineer-driven culture at Tektronix. He believed in providing engineers with the resources and freedom to explore new ideas, a philosophy that contributed significantly to the company’s long-term success."
  },
  {
    "objectID": "inventors/vollum.html#legacy",
    "href": "inventors/vollum.html#legacy",
    "title": "Howard Vollum",
    "section": "Legacy",
    "text": "Legacy\nHoward Vollum retired from Tektronix in the early 1980s and passed away in 1986. His contributions to electronics are immense. He didn’t just improve an existing instrument; he essentially reinvented it, making the oscilloscope an indispensable tool that fueled the rapid advancements in electronics, computing, and telecommunications throughout the 20th century. His legacy lives on in every modern oscilloscope, which traces its lineage directly back to his fundamental innovation of the triggered sweep."
  },
  {
    "objectID": "sp/editors-blog/posts/2024/03/editors-note.html",
    "href": "sp/editors-blog/posts/2024/03/editors-note.html",
    "title": "Editor’s note: Not saying goodbye, just saying…",
    "section": "",
    "text": "It’s not easy to leave a brilliant group of people you’ve worked with for almost a decade, but in a month’s time I’ll be moving on from the Royal Statistical Society (RSS).\nWhen I joined RSS in June 2014 I was looking for new challenges. I wanted to find out more about the ways statistics and data are used to understand and solve problems and inform decisions in science, business and industry, public policy, health… I could go on! Working for the RSS certainly delivered on that front: as editor of Significance for eight years and of Real World Data Science more recently, I have had many opportunities to learn.\nPretty much every day of my working life for the past nine years, eight months or so involved speaking with expert statisticians and data scientists or reading about their work. When there were things I didn’t understand, they were always happy to explain. When I shared my ideas for how to make their articles clearer or more readable, they took the time to listen. Together, we worked to create accessible, engaging stories about statistics and data. There have been hundreds of these collaborations over the years – too many to namecheck individually – but I have enjoyed them all, and I’ve learned something from each of them.\nBefore I head off to pursue a new set of challenges and learning opportunities, I want to say a big thank you to all the RSS staff and members, past and present, that I’ve been lucky to call my colleagues. Thank you also to the staff and members of the American Statistical Association who have been valued partners on Significance over the years and now RWDS too. It’s been a privilege to work with you all.\nThe chance to launch RWDS has been a particular highlight of my time at RSS, and I am grateful to have had the support and input of The Alan Turing Institute and many of its wonderful staff and researchers on this project. I’m excited to see the site continue to grow and develop into a valuable resource for the data science community, and I look forward to reading an upcoming series of articles that will explore the statistical and data science perspectives on AI – stay tuned for more on this soon.\nStatistics and data will continue to be a big part of my life, so this isn’t “goodbye.” Instead, I’ll just say, let’s keep in touch – and thank you for reading!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Pete Pedroza on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “Editor’s note: Not saying goodbye, just saying…” Real World Data Science, March 6, 2024. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "href": "sp/editors-blog/posts/2024/01/22/gen-ai-framework.html",
    "title": "UK government sets out 10 principles for use of generative AI",
    "section": "",
    "text": "The UK government has published a framework for the use of generative AI, setting out 10 principles for departments and staff to think about if using, or planning to use, this technology.\nIt covers the need to understand what generative AI is and its limitations, the lawful, ethical and secure use of the technology, and a requirement for “meaningful human control.”\nThe focus is on large language models (LLMs) as, according to the framework, these have “the greatest level of immediate application in government.”\nIt lists a number of promising use cases for LLMs, including the synthesise of complex data, software development, and summaries of text and audio. However, the document cautions against using generative AI for fully automated decision-making or in contexts where data is limited or explainability of decision-making is required. For example, it warns that:\n\n“although LLMs can give the appearance of reasoning, they are simply predicting the next most plausible word in their output, and may produce inaccurate or poorly-reasoned conclusions.”\n\nAnd on the issue of explainability, it says that:\n\n“generative AI is based on neural networks, which are so-called ‘black boxes’. This makes it difficult or impossible to explain the inner workings of the model which has potential implications if in the future you are challenged to justify decisioning or guidance based on the model.”\n\nThe framework goes on to discuss some of the practicalities of building generative AI solutions. It talks specifically about the value a multi-disciplinary team can bring to such projects, and emphasises the role of data scientists:\n\n“data scientists … understand the relevant data, how to use it effectively, and how to build/train and test models.”\n\nIt also speaks to the need to “understand how to monitor and mitigate generative AI drift, bias and hallucinations” and to have “a robust testing and monitoring process in place to catch these problems.”\nWhat do you make of the Generative AI Framework for His Majesty’s Government? What does it get right, and what needs more work?\n\n\n\n\n\n\nAnd in case you missed it…\n\n\n\nNew York State issued a policy on the Acceptable Use of Artificial Intelligence Technologies earlier this month. Similar to the UK government framework, it references the need for human oversight of AI models and rules out use of “automated final decision systems.” There is also discussion of fairness, equity and explainability, and AI risk assessment and management.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2024 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Massimiliano Morosinotto on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2024. “UK government sets out 10 principles for use of generative AI.” Real World Data Science, January 22, 2024. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html",
    "href": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html",
    "title": "Call for Submissions",
    "section": "",
    "text": "Get ready to engage with Real World Data Science as we unveil an exciting editorial refresh! We’re thrilled to announce that submissions are now open across our four dynamic sections: The Pulse, Applied Insights, Foundations & Frontiers, and People & Pathways. Join us as we redefine the conversation in data science with fresh perspectives and insights. Real World Data Science is relaunching to meet the pace and complexity of today’s data-driven world in real time, with the RSS’s trademark steadying presence. We will be publishing high-quality case-studies, tutorials and think-pieces that bridge the gap between rigorous analysis and real-time relevance, and that speak directly to latest events and emerging trends.\nAll submissions will be peer-reviewed by members of the Real World Data Science Editorial Board."
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-audience",
    "href": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-audience",
    "title": "Call for Submissions",
    "section": "Our Audience:",
    "text": "Our Audience:\nPeople working in data science looking for practical insights, methodological rigour and thought-leadership that informs their work and decision-making."
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-voice",
    "href": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-voice",
    "title": "Call for Submissions",
    "section": "Our Voice:",
    "text": "Our Voice:\nAuthoritative, Trustworthy, Cutting Edge"
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-editorial-sections",
    "href": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#our-editorial-sections",
    "title": "Call for Submissions",
    "section": "Our Editorial Sections:",
    "text": "Our Editorial Sections:\nReal World Data Science has four editorial sections. Please read through and consider where your piece would fit best. Each piece we publish needs to be tailored towards the focus of one of these sections.\nTHE PULSE\nNews, updates and real time commentary.\nPurpose: To respond to current events, trends and debates in the data science world with rigour, insight and relevance.\nContent Types: Articles that speak directly to current events/trends/launches\nExample Call To Action: Invite readers to share your commentary with their networks as a trusted voice in the space. Invite engagement, discussion and debate over the topics.\nAPPLIED INSIGHTS\nHow data science is used to solve real-world problems in business, public policy and beyond.\nPurpose: To showcase real-world applications of data science, including hands-on tutorials, project walk-throughs, and case studies from industry, academia, or public service.\nContent Types:\n\nHigh-quality step-by-step tutorials with code\n\nCase studies detailing a problem, approach, and outcome\n\nLessons learned from real-world deployments\n\nExample Call To Action: Readers should walk away with something to try.\nFOUNDATIONS & FRONTIERS\nThe ideas behind the impact: the concepts, tools and methods that make data science possible.\nPurpose: To deepen understanding of the theoretical and ethical foundations of data science, and to spotlight thought leadership and emerging ideas.\nContent Types:\n\nThink-piece style articles with an engaging angle on methodology, ethics and standards\n\nInterviews with thought-leaders\n\nData Science Bites - our handy summaries/explainers of academic papers\n\nExample Call To Action: Invite discussion and engagement – pose questions and challenges to the reader.\nPEOPLE & PATHS\nStrategic reflections on careers, leadership and professional evolution in data science.\nPurpose: To explore the evolving nature of data science careers through the lens of experience, leadership, and long-term impact. This section highlights how professionals shape and are shaped by the field—through roles, decisions, and philosophies.\nContent Types:\n\nProfiles of/interviews with senior professionals reflecting on career philosophy and leadership\n\nRoundtables with experts on hiring, mentoring, or organisational design\nCommentary on career-defining trends, such as the rise of AI governance or the shift toward interdisciplinary teams\n\nExample Call To Action: Encourage readers to share our strategic insights with their community."
  },
  {
    "objectID": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#use-of-ai-in-submissions",
    "href": "sp/editors-blog/posts/2025/07/08/relaunch-CFS.html#use-of-ai-in-submissions",
    "title": "Call for Submissions",
    "section": "Use of AI in Submissions",
    "text": "Use of AI in Submissions\nWe recognise that LLMs and other generative AI tools are increasingly part of the data science workflow, from code generation and data cleaning to drafting documentation and shaping analysis. We welcome a transparent approach in submissions that have made use of these tools, and ask that authors include a declaration outlining where and how AI was used in the development of their submission. This helps us maintain transparency, uphold standards of reproducibility, and better understand the evolving role of AI in real-world data science practice.\nTo make your submission, please review our contributor guidelines and email us at rwds@rss.org.uk\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2025 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Virgina Johnson on Unsplash.\n\n\n\nHow to cite\n\nReal World Data Science Editorial Board. 2025. “Call for Submissions” Real World Data Science, July 7, 2025. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/06/jsm-blog.html",
    "href": "sp/editors-blog/posts/2023/08/06/jsm-blog.html",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "",
    "text": "Haley Jeppson, Danielle Albers Szafir, and Ian Lyttle\nJSM 2023 is underway, and the first session I attended today was this panel on the use of colour in statistical charts.\nThe topic appealed to me for two reasons:\n\nBefore my trip to Toronto, I interviewed Alberto Cairo about the many “dialects” of data visualisation.\nI’ve recently been working with Andreas Krause and Nicola Rennie to create new guidance for improving statistical graphics, titled “Best Practices for Data Visualisation”.\n\nThe “Best Practices…” guide links to several useful data visualisation tools, and this session today has put a few more on my radar:\n\nColor Crafting, by Stephen Smart, Keke Wu, and Danielle Albers Szafir. The authors write: “Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color.”\nComputing on color, a collection of Observable notebooks by Ian Lyttle that allow users to see how different colour spaces and colour scales work with different types of colour vision deficiency."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#sunday-august-6",
    "href": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#sunday-august-6",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "",
    "text": "Haley Jeppson, Danielle Albers Szafir, and Ian Lyttle\nJSM 2023 is underway, and the first session I attended today was this panel on the use of colour in statistical charts.\nThe topic appealed to me for two reasons:\n\nBefore my trip to Toronto, I interviewed Alberto Cairo about the many “dialects” of data visualisation.\nI’ve recently been working with Andreas Krause and Nicola Rennie to create new guidance for improving statistical graphics, titled “Best Practices for Data Visualisation”.\n\nThe “Best Practices…” guide links to several useful data visualisation tools, and this session today has put a few more on my radar:\n\nColor Crafting, by Stephen Smart, Keke Wu, and Danielle Albers Szafir. The authors write: “Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color.”\nComputing on color, a collection of Observable notebooks by Ian Lyttle that allow users to see how different colour spaces and colour scales work with different types of colour vision deficiency."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#monday-august-7",
    "href": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#monday-august-7",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Monday, August 7",
    "text": "Monday, August 7\n\nAstronomers Speak Statistics\nAstrophysicist Joel Leja kicked off his JSM talk with a video of the launch of the James Webb Space Telescope – an inspiring way to start the day, and a prelude to a discussion of the statistical challenges involved in studying the deep universe.\nJames Webb, since launch, has “completely expanded our point of view”, said Leja, allowing astronomers to explore the first stars and galaxies at greater resolution than ever before.\n\n\n\nImage from the James Webb telescope showing two galaxies in the process of merging, twisting each other out of shape. Credit: ESA/Webb, NASA & CSA, L. Armus, A. Evan, licenced under CC BY 2.0.\n\n\nAlready, after only 13 months of operation, the images and data sent back by the telescope have left observers astounded: for example, finding suspected early galaxies that are bigger than thought possible based on extreme value analysis.\nBut the big challenge facing those studying the early universe is trying to work out how early galaxies evolved over time. “We can’t watch this happen,” said Leja, joking that this process lasts longer than a typical PhD. So, instead, he said, “We need to use statistics to understand this, to figure out how they grow up.”\n\n\nTeaching statistics in higher education with active learning\nGreat talk from Nathaniel T. Stevens of the University of Waterloo, explaining how a posting for a Netflix job inspired the creation of a final project for students to learn response surface methodology.\nThe job ad in question “really opened my eyes” to the use of online controlled experiments by companies, said Stevens. He told delegates how LinkedIn, the business social networking site, runs over 400 experiments per day, trying to optimise user experience and other aspects of site engagement.\nNetflix’s job ad highlighted just how sophisticated these experiments are, said Stevens. People might hear companies refer to their use of A/B tests, but the term trivialises what’s involved, Stevens explained.\nHaving encountered a job ad from Netflix, looking for someone to design, run, and analyse experiments and support internal methodological research, Stevens was inspired to present students with a hypothetical business problem, based on the Netflix homepage. That homepage, for those not familiar, features rows and rows of movies and TV shows sorted by theme, each show presented as a tile that, when hovered over, leads to a pop-up with a video preview and a match score – a prediction of how likely a viewer is to enjoy the show.\nStevens explained the hypothetical goal as trying to minimise “browsing time” – the time it takes a Netflix user to pick something to watch. Browsing time was defined as time spent scrolling and searching, not including time spent watching previews.\nStudents were given four factors that might influence browsing time – tile size, match score, preview length, and preview type – and through a sequence of experiments based on data generated by a Shiny app, students sought to minimise browsing time.\nThe response from the students? Two Netflix-style thumbs up. Ta-dum!"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#tuesday-august-8",
    "href": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#tuesday-august-8",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Tuesday, August 8",
    "text": "Tuesday, August 8\n\nThe Next 50 Years of Data Science\nStanford University’s David Donoho wrestled with the question of whether a singularity is approaching in this post-lunch session on the future of data science.\nTaking his cue from the 2005 Ray Kurzweil book, The Singularity is Near, Donoho reviewed recent – and sometimes rapid – advances in data science and artificial intelligence to argue that a singularity may have already arrived, just not in the way Kurzweil supposed.\nKurzweil’s book argues that at some point after the 2030s, machine intelligence will supersede human intelligence, leading to a takeover or disruption of life as we know it.\nAt JSM, Donoho argued that we have certainly seen a “massive scaling” of compute over the past decade, along with expanded communications infrastructure and the wider spread of information – all of which is having an impact on human behaviour.\nThat human behaviour can often now be directly measured thanks to the proliferation of digital devices with data collection capabilities, and this in turn is leading to a major scaling in datasets and performance scaling for machine learning models.\nBut does this mean that an AI singularity is near? Not according to Donoho. The notion of an AI singularity “is a kind of misdirection”, he said. Something very profound is happening, Donoho argued, and it is the culmination of three long-term initiatives in data science that have come together in recent years. “They constitute a singularity on their own.”\nThese three initiatives, as Donoho described, are: datafication and data sharing; adherence to the “challenge problem” paradigm; and documentation and sharing of code. These are solid achievements that came out of the last decade, said Donoho, and they are “truly revolutionary” when they come together to form what he refers to as “frictionless reproducibility.”\n\n\n\nPhoto of David Donoho’s slide, describing the scientific revolution of the “data science decade”. Photo by Brian Tarran, licenced under CC BY 4.0.\n\n\nFrictionless reproducibility, when achieved, leads to a “reproducibility singularity” – the moment where it takes almost no time at all for an idea to spread. “If there is an AI singularity,” said Donoho, “it will be because this came first.”"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#wednesday-august-9",
    "href": "sp/editors-blog/posts/2023/08/06/jsm-blog.html#wednesday-august-9",
    "title": "Live from Toronto: Real World Data Science at the Joint Statistical Meetings",
    "section": "Wednesday, August 9",
    "text": "Wednesday, August 9\n\nNew frontiers of statistics in trustworthy machine learning\nData, data everywhere, but is it safe to “drink”? A presentation this morning from Yaoliang Yu of the University of Waterloo looked at the issue of data poisoning attacks on algorithms and the effectiveness of current approaches.\nYu began by explaining how machine learning algorithms require a lot of data for training, and that large amounts of data can be obtained cheaply by scraping the web.\nBut, he said, when researchers download this cheap data, they are bound to worry about the quality of it. Drawing an analogy to food poisoning, Yu asked: What if the data we “feed” to algorithms is not clean? What is the impact of that?\n\n\n\nIllustration by Yasmin Dwiputri & Data Hazards Project / Better Images of AI / Managing Data Hazards / Licenced by CC-BY 4.0.\n\n\nAs a real-world example of a data poisoning attack, Yu pointed to TayTweets, the Microsoft Twitter chatbot that spewed racism within hours of launch after Twitter users began engaging with it.\nYu then walked delegates through some experiments showing how, generally, indiscriminate data poisoning attacks are ineffective when the ratio of poisoned data to clean data is small. A poisoning rate of 3%, for example, leads to model accuracy drops of 1.5%–2%, Yu said.\nHowever, he then put forward the idea of “parameter corruption” – an attack that seeks to modify a model directly. Yu showed that this would be more effective in terms of accuracy loss, though – fortunately – perhaps less practical to implement.\n\n\nData Science and Product Analysis at Google\nOur final session at JSM 2023, before heading home, was a whistle-stop tour of various data science projects at Google, covering YouTube, Google Maps, and Google Search.\nJacopo Soriano kicked us off with a brief intro to the role and responsibilities of statisticians and data scientists at Google, and within YouTube specifically – the main task being to make good decisions based on uncertain data.\nSoriano also spoke about the key role randomised experiments play in product development – harking back to Nathaniel Stevens’ earlier talk on this subject. YouTube runs hundreds, if not thousands, of concurrent experiments, Soriano said; statisticians can’t, therefore, be involved in each one. As Soriano’s colleague, Angela Schoergendorfer, explained later in the session, the role of the data scientist is to build methodology and metrics that others in the business can use to run their own experiments.\n\n\n\nPhoto by Pawel Czerwinski on Unsplash.\n\n\nFor every experiment YouTube runs, a portion of its voluminous daily traffic will be assigned to control arms and treatment arms, with traffic able to be diverted to different groups based on user type, creators, videos, advertisers, etc. Once experiments are running, metrics such as search clickthrough rates, watch time using specific devices, or daily active user numbers are monitored. Teams tend to look at percentage change as the scale to measure whether something is working or not, said Soriano, rather than comparing treatment to control group.\nNext up was Lee Richardson, who spoke about the use of proxy metrics. Technology companies like Google are often guided by so-called “north star metrics”, which executive leadership use to guide the overall strategy and priorities of an organisation. However, Richardson said, these can be hard to design experiments around, and so proxy metrics stand in for the north star metrics. Proxies need to be sensitive, he said, and move in the same direction as, e.g., a long-term positive user experience.\nOn the subject of user experience, Christopher Haulk then explained how YouTube measures user satisfaction through single-question surveys – typically asking a YouTube user to rate the video they just watched. The company doesn’t send out that many surveys, Haulk said, and response rates are in the single-digit percentage range, so it can be hard to evaluate whether changes YouTube makes to, e.g., its video recommendation algorithm are working to improve user satisfaction. Haulk then went on to explain a modelling approach the company uses to predict how users are likely to respond in order to “fill in” for missing responses.\nOver at Google Search, user feedback is also regularly sought to help support the evolution of the product. Angela Schoergendorfer explained how, with so many people already using Google Search, statistically significant changes in top-line metrics like daily active users can take months to see. Decision metics should ideally capture user value quickly, said Schoergendorfer – within days. For this, Google has 10,000 trained “search quality” raters they can call on. Random samples of user search queries and results are sent to these raters, who are asked to evaluate the quality of the search results. Users can also be asked in the moment, or offline through the Google Rewards app.\nIn 2021, Schoergendorfer said, Google conducted approximately 800,000 experiments and quality tests. But perhaps the most impressive statistic of the day came from Sam Morris, who works on Google Maps. Something, somewhere, is always changing in the world, said Morris – be it a road closure or a change to business hours. The Maps team cannot evaluate every single piece of data – a lot of changes are automated or algorithmic, he explained. “So far this year, we have probably processed 16 billion changes to the map,” said Morris – a staggering figure!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Live from Toronto: Real World Data Science at the Joint Statistical Meetings.” Real World Data Science, August 6, 2023, updated August 15, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/02/14/digital-skills.html",
    "href": "sp/editors-blog/posts/2023/02/14/digital-skills.html",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "",
    "text": "Digital skills. We all need them. Employers say they want them, but there aren’t enough to go around. Supply can’t meet demand, so we’re left with a gap – a digital skills gap. But what are digital skills exactly?\nThis is a question that was asked repeatedly, in various different constructions, by Stephen Metcalfe MP, chairing a meeting of the Parliamentary and Scientific Committee on Tuesday, February 7. I went along to the meeting as an observer, hoping to hear an answer to that very question.\nWhat I got was several different answers – no single solid definition, but a reasonable sense that boosting data science skills would go a long way towards closing the digital skills gap."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "href": "sp/editors-blog/posts/2023/02/14/digital-skills.html#survey-says",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Survey says…",
    "text": "Survey says…\nThe committee meeting was sponsored by the Institution of Engineering and Technology (IET), and the main focus of discussion was the results of IET’s skills for a digital future survey, based on a YouGov poll of 1,235 respondents drawn from engineering employers (defined as “employers who employ at least one engineering and technology employee in the UK”).\n\n\n\n\n\nDigital skills, including AI skills, are not only required of engineers, says the IET’s Graham Herries. Generative AI tools like Stable Diffusion threaten to shake-up the creative industries. (Photo by Billetto Editorial on Unsplash)\n\n\nKicking off the discussion was Graham Herries, an engineering director and chair of the IET’s Innovation and Skills Panel, who drew attention to the harms that the digital skills gap is reportedly having. Of those respondents who identified skills gaps in their own organisations, 49% pointed to a reduction in productivity, while 35% said skills shortages were restricting company growth.\nAs the hot topic of the day, ChatGPT inevitably came up during the discussion. Herries sees it as a disruptive force, and 36% of all respondents believe artificial intelligence (AI) skills will be important for their engineers to have within five years (24% say they are important now). But AI skills are important for non-engineers too, argued Herries, as he pointed to stirrings in the creative industries caused by generative art tools such as Stable Diffusion.\nHerries therefore puts AI skills under the broad umbrella of “digital skills”. But, to him, it’s not enough to simply be able to use AI technology; rather, users should know enough to be able to ask the right questions about the provenance of the data used to train the AI, its quality and biases, etc. This was a point developed further by Yvonne Baker, an engineer and the CEO of STEM Learning. Baker talked about digital skills as being both the ability to use digital technology and also to understand its limitations. Yet another perspective was offered by Rab Scott, director of industrial digitalisation at the University of Sheffield’s Advanced Manufacturing Research Centre. Scott defined digital skills in the context of quality control systems in industry 4.0: it’s about knowing how and where to place a sensor to collect data about the manufacturing process, to feed that data into a data collection system, analyse the data for insights, and use those insights to inform decision-making."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "href": "sp/editors-blog/posts/2023/02/14/digital-skills.html#closing-the-gap",
    "title": "Data science can help close the ‘digital skills’ gap, or so it seems",
    "section": "Closing the gap",
    "text": "Closing the gap\nFurther definitions of “digital skills” are to be found in the IET’s published report. Survey respondents were encouraged to describe the term in their own words, so we see things like:\n\n“the ability to understand, process and analyse data.”\n“Coding, programming, software design, use of social media for marketing and communicating with stakeholders, data visualisation, work that relies solely on the use [of] online systems.”\n\nWhen respondents were asked what skills were lacking in both the external labour market and their internal workforce, around a fifth cited “more complex numerical/statistical skills and understanding”. And when looking to the future and to the skills anticipated to be important areas for growth in the next five years, 39% of respondents picked “data analytics” while 31% said “artificial intelligence and machine learning”.\nSo, perhaps you now understand why I left the meeting with the feeling that more data science skills, more data science training, could help address the shortfall in “digital skills”.\nBut how exactly can we equip more people with the right skills? At one point during the discussion, Metcalfe told the meeting that he was still looking for a key takeaway, something he could take to the Secretary of State and say, ‘This is what we need to embed in the curriculum’. What was offered instead was a range of possible solutions.\nThe IET survey found broad backing for government support for reskilling: 40% of respondents favoured grants or loans for training (and retraining) programmes, 39% would like more funding for apprenticeships, while 33% think there should be better carers advice and guidance in schools and colleges.\nBaker also made the case for digital skills to be taught in schools as part of every subject, not just in computer science lessons, and that teachers would need to be supported to deliver this.\nBut how would you close the “digital skills” gap, if given the chance?\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science can help close the ‘digital skills’ gap, or so it seems.” Real World Data Science, February 14, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html",
    "href": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "",
    "text": "A few weeks back, I managed to catch Nicola Rennie’s presentation to the Oxford R User Group on how to create Christmas cards in R. It was a fun session, and thanks to Nicola’s clear and concise explanations, I felt emboldened to attempt my own design, using her code as a base.\nIf you missed the Meetup session, Nicola has kindly written a tutorial for Real World Data Science that walks through all the necessary steps to create a snowman against a snowy night’s sky. You’ll want to read that tutorial first before returning to this blog.\nMy design uses the same basic setting as Nicola’s but updates the scene to reflect the Real World Data Science (RWDS) brand colours, and I replace the snowman with a Christmas tree adorned with coloured baubles."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#snowy-sky",
    "href": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#snowy-sky",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Snowy sky",
    "text": "Snowy sky\nWe begin by loading in the following packages, adding a couple extra to the ones Nicola uses:\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(sf)\nlibrary(png)\nlibrary(patchwork) \nThen we add the sky, now recoloured in RWDS purple using fill and color:\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#939bc9\", color = \"#939bc9\")\n  )\ns1\nWe use the same code as Nicola to create the snowflakes, but we do this step first before adding snow on the ground, as we’re using the RWDS site background colour, hex code #f0eeb, to represent our settled snow:\n# add snowflakes\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n),\n  y = runif(n)\n)\ns2 &lt;- s1 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns2\n\n# snow on ground\ns3 &lt;- s2 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"#f0eeeb\", colour = \"#f0eeeb\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns3"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#oh-christmas-tree",
    "href": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#oh-christmas-tree",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Oh, Christmas tree",
    "text": "Oh, Christmas tree\nTo build her snowman, Nicola created a series of circles that were stacked and overlaid. A simple Christmas tree, though, requires a series of triangles. So, taking Nicola’s snowman’s nose (also a triangle) as our starting point, we coded three sets of coordinates – tree_pts1, tree_pts2, and tree_pts3 – for three triangles of decreasing size that would sit on top of one another.\n# coordinates for tree base\ntree_pts1 &lt;- matrix(\n  c(\n    0.2, 0.3,\n    0.5, 0.6,\n    0.8, 0.3,\n    0.2, 0.3\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# coordinates for tree middle\ntree_pts2 &lt;- matrix(\n  c(\n    0.3, 0.5,\n    0.5, 0.7,\n    0.7, 0.5,\n    0.3, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# coordinates for tree top\ntree_pts3 &lt;- matrix(\n  c(\n    0.4, 0.65,\n    0.5, 0.75,\n    0.6, 0.65,\n    0.4, 0.65\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\n\n# put tree together\ntree &lt;- st_multipolygon(list(list(tree_pts1),\n                             list(tree_pts2),\n                             list(tree_pts3)))\ns4 &lt;- s3 +\n  geom_sf(\n    data = tree,\n    fill = \"chartreuse4\",\n    colour = \"chartreuse4\"\n  ) +\n  coord_sf(expand = FALSE)\ns4\nA tree also requires a trunk, so we borrowed one of the rectangles from Nicola’s snowman’s hat for this purpose:\ns5 &lt;- s4+\n  annotate(\n    geom = \"rect\",\n    xmin = 0.45,\n    xmax = 0.55,\n    ymin = 0.2,\n    ymax = 0.3,\n    fill = \"brown\"\n  )\ns5\nAnd, of course, no Christmas tree is complete without decorations. The “rocks” that formed the buttons and eyes on Nicola’s snowman were updated to become gold and red baubles for our tree:\n# add gold baubles\ns6 &lt;- s5 +\n  geom_point(colour = \"gold\",\n             data = data.frame(\n               x = c(0.3, 0.4, 0.5, 0.6, 0.57, 0.62, 0.45, 0.5),\n               y = c(0.325, 0.4, 0.45, 0.35, 0.57, 0.52, 0.6, 0.7),\n               size = runif(8, 2, 4.5)\n             ),\n             mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns6\n\n# add red baubles\ns7 &lt;- s6 +\n  geom_point(colour = \"red3\",\n             data = data.frame(\n               x = c(0.7, 0.6, 0.5, 0.525, 0.43, 0.38, 0.55, 0.5),\n               y = c(0.375, 0.4, 0.55, 0.65, 0.43, 0.48, 0.5, 0.375),\n               size = runif(8, 2, 4.5)\n             ),\n             mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns7"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#seasons-greetings",
    "href": "sp/editors-blog/posts/2023/12/12/rwds-xmas-card.html#seasons-greetings",
    "title": "A Christmas card in R for the Real World Data Science community",
    "section": "Season’s greetings",
    "text": "Season’s greetings\nThe final step was to add text to the top of the image, wishing you all a Merry Christmas, and our logo to the bottom, so you know who the card is from:\n# add text\ns8 &lt;- s7 +\n  annotate(\n    geom = \"text\",\n    x = 0.5,\n    y = 0.875,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    fontface = \"bold\",\n    size = 18\n  )\ns8\n\n# add logo \npath &lt;- \"images/rwds-logo-150px.png\"\nimg &lt;- readPNG(path, native = TRUE) \ns9 &lt;- s8 +                   \n  inset_element(p = img, \n                left = 0.3265, \n                bottom = 0.0, \n                right = 0.6735, \n                top = 0.2\n  ) \ns9\n\n\n\n\n\nI hope you like the Christmas card! From all of us at Real World Data Science, thank you for your support throughout 2023. Merry Christmas, happy holidays, and best wishes for 2024!\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “A Christmas card in R for the Real World Data Science community.” Real World Data Science, December 12, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/09/19/positconf-blog.html",
    "href": "sp/editors-blog/posts/2023/09/19/positconf-blog.html",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "",
    "text": "Videos from posit::conf(2023) are now available on YouTube, including our talk about how we built Real World Data Science using Quarto. We’ve embedded a selection of videos in this blog post, but be sure to check out the full playlist."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/09/19/positconf-blog.html#tuesday-september-19",
    "href": "sp/editors-blog/posts/2023/09/19/positconf-blog.html#tuesday-september-19",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "Tuesday, September 19",
    "text": "Tuesday, September 19\n\nFrom data confusion to data intelligence\nAn inspiring start to posit::conf(2023) this morning, with keynote talks from Elaine McVey, senior director of analytics at Chief, and David Meza, head of analytics for human capital at NASA, sharing stories and insights on how to build strong data science foundations in organisations.\nMcVey spoke about the frequent mismatch between high levels of hope for what data science can achieve within organisations, and low levels of understanding about how to set up data science teams for success. The best chance for success, she said, is if data scientists take the lead in helping organisations learn how to make best use of data science expertise.\nFrom there, McVey went on to present a set of “guerilla data science tactics” that data scientists can use to get around any obstacles they may encounter, as illustrated in the slide below:\n\n\n\nElaine McVey’s “guerilla data science tactics” for building successful data science teams.\n\n\nData scientists should start by scanning for opportunities to help the organisation, before building a small-scale version of what it is they propose to do. Once buy-in is achieved, and data is made available, it’s time to run with the project. Once complete, you need to “nail the landing,” McVey said, and make sure to communicate results broadly – not just to primary stakeholders, but across the organisation. Then comes time to “up the ante”: if your first project has built some organisational goodwill, leverage that and look for something higher risk, with higher potential reward for the organisation.\nThroughout this process, McVey said, data scientists should be building foundations for future projects – creating data pipelines, R packages, etc., that can be reused later. This was a point picked up and developed upon by Meza, who walked through in detail the steps required to establish “data foundations” within organisations, drawing on his own past experiences. Typically, he said, organisations seem to collect data just to store it – but always data should be collected, stored, and managed with analysis in mind.\n\n\n\nA hacker’s guide to open source LLMs\nFast.ai’s Jeremy Howard lifted the hood on large language models (LLMs) in the second of two keynotes this morning.\nBeginning with an accessible overview of what LLMs are, how they work, and how they are trained, Howard then addressed some of the criticisms made of LLMs – that they “can’t reason” or give correct answers.\nAs Howard explained, a model like OpenAI’s GPT-4 is not trained at any point to give correct answers to prompts – only to predict the most likely next word, or word token, in a sequence.\nThe pre-training step, for example, does not involve only feeding the model with “correct answers,” instead relying on a corpus of text from the internet – some (or, maybe, much) of which may consist of factual inaccuracies, errors, falsehoods, etc. And in the fine-tuning stage, when human feedback is used to either reward or penalise model outputs, Howard said there is a preference for confident-sounding responses – and so, again, this doesn’t necessarily reward the model for giving correct answers.\nHoward made the case that users have to help language models to give good answers, and that custom instructions can be used to change the way models respond. He then walked delegates through a series of demos using open-source LLMs, to show how outputs can be refined and improved.\n“My view is that if you are going to be good at language modelling in any way,” said Howard, “you have to be good at using language models.”\n\n\n\nDocumenting Things: Openly for Future Us\nJulia Stewart Lowndes, founding director of Openscapes, gave a compelling talk advocating for the importance of documentation for data science projects.\nDocumenting things, Lowndes said, should be done for the benefit of “Future Us”: not only ourselves but our teams and our communities who may be contributing to or revisiting the project in the next hours, days, weeks, months and years.\nDocumenting things does not have to be painful, Lowndes said. In fact, it’s supposed to be helpful. It does, however, take time and intention. And it means slowing down briefly to write things down now, in order that work speeds up in the longer term.\nLowndes then shared some pointers to help people get started with documentation:\n\nHave a place to write things down – Google Docs, GitHub, wherever – ideally a place where people can work collaboratively.\n\nDevelop the habit of writing things down as you go.\nWrite in a modular way – small bits of text are less daunting and easier to maintain collaboratively.\n\nHave an audience in mind – you are writing this for someone, so make it engaging for them.\n\nwrite in an inclusive tone.\nNarrate code in small chunks, and in a way that you’d say out loud if teaching.\nShare, and share early – you want to be able to iterate on your documentation and receive feedback. Also, sharing openly does not always mean publicly – manage permissions as necessary.\n\nDesign for readability and accessibility.\n\nUse section headers – particularly important for screen readers, but this also helps generally to describe the flow of a document. Plus, you can link readers directly to specific parts of a document.\nUse text formatting.\nUse alt-text for images, describing the take-home message of the image.\n\n\n\n\nTeaching Data Science in Adverse Circumstances: Posit Cloud and Quarto to the Rescue\nProfessor Aleksander Dietrichson of the Universidad de San Martin brought a valuable perspective to posit::conf(2023) on the challenges of teaching data science in the face of technology and language barriers.\nAt the public, state-funded university in Argentina where Dietrichson works, more than half of students do not have access to laptops or computers at home, and those who do have access – whether at home or at school – may not have access to the latest kit. But “Posit Cloud solves the resource issue,” Dietrichson said. The free-to-use, online browser-based version of Posit’s tools runs on anything; Dietrichson said he’s tested it successfully on both decade-old computers and cellphones – though he doesn’t recommend using it on a cellphone!\nOn language barriers, he pointed out that learning to code in R and Python can be challenging when English isn’t your first language – if you don’t have semantic access to function names, for example, there will be a steeper learning curve for students.\nDietrichson also has to deal with the problem of “arithmaphobia” among some of the liberal arts students he teaches. This has necessitated a reshuffling of the typical statistics curriculum, he said, in order to make it easier for students to access. But the work is worth it, Dietrichson explained: many of his students want to work in careers like journalism, and he believes that “journalists should be statistically literate.”\n\n\n\nDynamic Interactions: Empowering Educators and Researchers with Interactive Quarto Documents Using webR\nSome of my favourite sessions at posit::conf(2023) were about Quarto. Understandable, really, when you consider that we used it to build this very site! Albert Rapp has described Quarto as a web dev gateway drug, and I’d agree with him:\n\nQuarto is a powerful tool for creating beautiful and interactive documents. I think of it as a gateway drug to web development: While it offers a user-friendly interface for creating documents and blogs, it also allows users to delve into the world of HTML & CSS without even realizing it.\n\nI spoke a bit about my own journey into web dev in one of the Quarto sessions at posit::conf, but what I loved most about these sessions was learning about all the cool new things I’ve yet to discover and try out. For example, James Balamuta’s talk and demonstration of building interactive code cells into Quarto webpages was an eye-opener!\nSince returning from Chicago I’ve tested out this functionality and added Balamuta’s example here. First run the code that’s already in the code block but also edit it to try out your own examples.\nLoading\n  webR...\n\n\n  \n\n\nVisit the quarto-webr website for details on how to make full use of this capability. Once you’re up to speed, why not contribute a webR-enabled article for Real World Data Science?"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/09/19/positconf-blog.html#wednesday-september-20",
    "href": "sp/editors-blog/posts/2023/09/19/positconf-blog.html#wednesday-september-20",
    "title": "Live from Chicago: Real World Data Science at posit::conf(2023)",
    "section": "Wednesday, September 20",
    "text": "Wednesday, September 20\n\nR Not Only In Production\nKara Woo, senior data science engineer at InsightRX, began her Wednesday morning keynote with a rousing description of posit::conf(2023) being like a “great community garden” where things are being cultivated and shared for the benefit of all. This is an important feeling, Woo said, because it doesn’t always feel like that in our day jobs. Data scientists can feel siloed, not able to share ideas with like-minded people, and facing resistance from people who say “R can’t do that, R isn’t a real programming language” – a comment that elicited a groan of weary familiarity from sections of the crowd.\nBut as Woo went on to explain, “it is possible to build quality software in R” and “it is possible to have an organisation where the strengths of R and the people who use it influence the organisation as a whole.”\nWoo was speaking from her experience at InsightRX, a precision medicine company, which makes software for clinicians to inform individualised dosing decisions for patients. Through a tool called Nova, clinicians feed in data about a patient’s unique characteristics, which is then passed to R for analysis, which then returns dosage recommendations to Nova.\nIn InsightRX, R has also been used to solve problems that are not strictly data science problems. Woo gave the example of working with a colleague to write an R package to identify data labels that have been changed and rollout translations for those labels in multiple languages for software users in different parts of the world.\n“Our mindset of R being a first-class language empowers us to solve problems,” said Woo.\n\n\n\nIt’s Abstractions All the Way Down…\nThe second of the morning keynotes on day two of posit::conf(2023) was by JD Long, vice president of risk management at RenaissanceRe.\nDuring Long’s insightful – and frequently very funny – talk, this slide appeared:\n\n\n\nJD Long’s assertion #1.\n\n\nDo you agree with Long’s assertion? If you don’t, what is the single biggest business value that’s been derived from the data science movement? Share your thoughts in the comments below.\n\n\n\nIt’s All About Perspective: Making a Case for Generative Art\nHobbies are important, right? They are a way to relax, to unwind. But also a great opportunity to learn things that might come in handy professionally. At least, that is the experience of Meghan Santiago Harris, a data scientist in the Prostate Cancer Clinical Trials Consortium at Memorial Sloan Kettering.\nHarris shared with delegates her journey into generative art, and how skills acquired using ggplot2 for “fun stuff” had a positive impact on her work.\nShe first defined generative art as artwork created through a program in any language or interface, so long as the program itself executes the generation of the art. To make generative art, Harris said, you just need data and the ability to “think outside the grid” of your favourite graphics software or package. Harris’s tool of choice is ggplot2, but any will do: “If a tool lets you plot data, it will let you make art,” she said.\n\n\n\nA slide from Meghan Santiago Harris’s talk, with an example of how to create an image of the sun setting on a city using lines of R code.\n\n\nHarris’s passion for generative art bloomed during a recent period of maternity leave. She was coding for fun but also deepening her understanding and expertise in areas like code iteration, development and communication. And, in August, Harris published an R package called artpack, which is now available on CRAN and designed “to help generative artists of all levels create generative art in R.”\nGenerative art was a motivation to learn and do more, Harris said, and doing something she loved helped make programming and data science more digestible.\n\n\n\nHow the R for Data Science (R4DS) Online Learning Community Made Me a Better Student\nFollowing straight after Meghan Santiago Harris was Lydia Gibson, a data scientist from Intel, with an inspiring talk about her route into data science. Gibson began by explaining how, when younger, “I wanted to be a fashion designer.” For her high school prom, Gibson even designed her own dress, which her grandmother made for her.\nIn 2011, Gibson earned a BS in economics and worked in retail customer service and state and local government for a time before deciding to return to school to do a Masters in statistics in 2021. She had “no experience of programming” when she made this decision, but soon learned that R is “a necessary evil if you have to go back to school to do statistics.”\nGibson told delegates that discovering data visualisation was what made her care about R. She could “feed [her] need for creativity” while also learning about things that were required for her course.\nAnd it was the R for Data Science (R4DS) Online Learning Community that helped take her learning to the next level. Gibson described R4DS as “an amazing, welcoming learning environment where beginners and advanced folks alike can come together to learn not only R but data science as a whole.”\n“Being surrounded by folks more advanced than you is a gift, not a curse,” she said, and she urged delegates to find what they are passionate about and explore its depths.\n\n\n\nGitHub Copilot integration with RStudio, it’s finally here!\nTom Mock, product manager for Posit Workbench and RStudio, had a full house for his talk about the upcoming integration of GitHub’s code Copilot product into RStudio. Copilot, Mock said, is an AI pair programmer that offers autocomplete-style suggestions for code – and this integration is one of the most popular requested features among RStudio users on GitHub.\nTo make use of the integration, you’ll need a Copilot subscription from GitHub. But more than that, Mock said, users will need to experiment to learn how to get the most out of the “generative [AI] loop.”\nSee Mock’s slide deck below for more details.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Live from Chicago: Real World Data Science at posit::conf(2023).” Real World Data Science, September 19, 2023, updated September 27, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/07/19/code-interpreter.html",
    "href": "sp/editors-blog/posts/2023/07/19/code-interpreter.html",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "",
    "text": "On July 6, 2023, OpenAI began rolling out the Code Interpreter plugin to users of its ChatGPT Plus service. But what exactly is this, and what functionality does it offer?\nCode Interpreter runs code and allows for uploading data so you can use ChatGPT for data cleaning, preprocessing, analysis, visualisation and predictive modelling tasks, among other things. This tool holds great promise for programmers and analysts alike, with the potential to streamline coding workflows as well as having an automated data analyst at your fingertips.\nTo use Code Interpreter, you need to enable it in the ChatGPT settings (at time of writing this only works with a paid ChatGPT Plus subscription).\nNow, let’s take it for a bit of a spin by uploading the stroke prediction dataset from Kaggle."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/07/19/code-interpreter.html#the-stroke-prediction-dataset",
    "href": "sp/editors-blog/posts/2023/07/19/code-interpreter.html#the-stroke-prediction-dataset",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "The stroke prediction dataset",
    "text": "The stroke prediction dataset\nThe World Health Organization (WHO) identifies stroke as the second leading cause of death worldwide, accounting for roughly 11% of all fatalities.\nKaggle’s stroke prediction dataset is used to forecast the likelihood of a patient suffering a stroke, taking into account various input parameters such as age, gender, presence of certain diseases, and smoking habits. Each row in the dataset offers pertinent information about an individual patient.\nLoading this dataset into ChatGPT Code Interpreter, one is treated with:\n\n\n\n\n\nThe user is asked: “Please let me know what analysis or operations you’d like to perform on this dataset. For instance, we can perform exploratory data analysis, data cleaning, data visualization, or predictive modelling.”\nIt seems quite a bold claim. So, I asked it to do all of the above.\n\n\n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\nThis is a good, useful summary. The missing values in bmi are set to the median, which the user can later decide to change for themselves as the code is available to do so.\n\n\n\n\n\n\n\nData visualisation\nNext, the visualisations of the variables are shown along with a correlation heatmap. Users can toggle between the visualisations and the code. The outputs are pretty useful, except for one mistake: id shouldn’t be included as part of the heatmap.\n\n\n\n\n\n\n\n\n\n\n\nHistograms and bar plots created by ChatGPT Code Interpreter for variables in the Kaggle stroke prediction dataset.\n\n\n\n\n\n\n\nCorrelation heatmap for variables in the Kaggle stroke prediction dataset.\n\nThings start to go seriously awry when Code Interpreter tries to create a predictive model.\n\n\nThe predictive model is garbage\nFrom the screenshot below, you can see that lumping all the data into a predictive model creates some highly spurious results. Age is a factor, as it should be, as is hypertension – indeed, those with hypertension in this dataset are around three times more likely to have a stroke than those without. In reality, there are also significant effects from glucose level and smoking, and also a slight BMI effect in this small, unbalanced dataset. However, work_type_children having a large positive effect is alarming and plainly wrong.\n\n\n\n\n\nIt is very evident from the table below that the positive coefficient on children is spurious.\n\n\n\n\n\nSo, where does this leave our thinking about Code Interpreter?"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/07/19/code-interpreter.html#discussion",
    "href": "sp/editors-blog/posts/2023/07/19/code-interpreter.html#discussion",
    "title": "Testing out ChatGPT’s new Code Interpreter",
    "section": "Discussion",
    "text": "Discussion\nMy test case is possibly an unfair one. The sort of study presented to Code Interpreter is one that requires careful analysis, and it uses a relatively small, tricky dataset whose difficulties are compounded by missing data. It’s therefore not surprising that, in this context, an automated analysis fails to shine in all respects.\nTo be fair, OpenAI themselves describe the plugin as an “eager junior programmer”. And as would be the case with a real junior programmer or junior data scientist, you’d expect a more experienced hand to be guiding an analysis like the one I asked for – someone who can sense-check results, point out errors, and offer suggestions for fixes and improvements.\nDespite some stumbles in this demo, OpenAI’s “junior programmer” presents a real step forward in the ChatGPT offering, and it is particularly impressive that one can toggle between code and charts without having to worry about coding at all.\nAt this stage, I would argue that Code Interpreter may be useful for quick summaries, visualisations and a little basic data cleaning and some preliminary investigations. However, based on what I’ve seen so far, it is clear to me that highly trained statisticians won’t be replaced anytime soon.\n\nBack to Editors’ blog\n\n\n\n\n\nAbout the author\n\nLee Clewley is a member of the editorial board of Real World Data Science and head of applied AI in GSK’s AI and Machine Learning Group, R&D.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Lee Clewley\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by charlesdeluvio on Unsplash.\n\n\n\nHow to cite\n\nClewley, Lee. 2023. “Testing out ChatGPT’s new Code Interpreter.” Real World Data Science, July 19, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/03/29/defining-DS.html",
    "href": "sp/editors-blog/posts/2023/03/29/defining-DS.html",
    "title": "Data science as ‘a rainbow’, and other definitions",
    "section": "",
    "text": "What does “data science” mean to you? That’s a question we’ve been asking a lot in recent weeks as part of our career profiles series of interviews – the first of which, featuring Jaguar Land Rover’s Tamanna Haque, was published yesterday.\nIt’s also a question that was asked recently of Sylvia Richardson, emeritus director of the Medical Research Council Biostatistics Unit at the University of Cambridge and immediate past president of the Royal Statistical Society (RSS).\nRichardson was interviewed by Francesca Dominici, interim co-editor-in-chief of the Harvard Data Science Review. In response to the question “What’s data science for you?”, Richardson said:\n\nIt’s hard to be original, but I was racking my brain for a good metaphor, and came up with the metaphor of a rainbow of interconnected disciplines, sharing the common aim of making the best use of data-rich environments we live in to solve problems in society. So, like in a rainbow, data scientists have to work together to draw out information from data. And the colors must match, [though] they are different. Similarly, there are different but intersecting data science tasks, taking different shapes and forms. As data scientists, we recognize and enjoy diversity, we’re not doing all the same tasks. Nevertheless, there is a backbone, a shape to the rainbow. And for us, this backbone is probability theory, study design, and quantifying uncertainty using statistical thinking. We also know that rainbows change all the time. They don’t last, but they keep reappearing. Data science is also evolving constantly because new questions and new types of data keep arising. In a similar way to the rainbow which is strongly influenced by the atmosphere, one key aspect of data science is that we have a strong link to practice. So, we work together to solve problems from different perspectives, we evolve, we try to be relevant to science and society, and make the best use of the data. [Source]\n\nRichardson’s view on the meaning and importance of data science has special resonance to me, as editor of Real World Data Science. While president of RSS, Richardson set up the Data Science Task Force out of which this website emerged. As she explains to Dominici:\n\n… while I was president, I felt a sense of urgency to encourage the RSS to revisit its engagement with data science, and I created a data science task force right at the beginning of my presidency. It didn’t get going earlier because there was COVID to keep us busy! Nevertheless, the Data Science Task Force got underway in 2021 and came up with two major recommendations. One was to give more resources to the practitioners’ community, which led the RSS to create a Real World Data Science online platform. A second direction was to brainstorm on what is still needed for the discipline to thrive. [Source]\n\nYou can read (or listen) to Richardson and Dominici’s conversation in full on the Harvard Data Science Review website.\nAnd we’ll have more career profiles – and more personal definitions of data science – to share soon. In the meantime, why not tell us what “data science” means to you?\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Data science as ‘a rainbow’, and other definitions.” Real World Data Science, March 29, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "href": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "",
    "text": "How do people feel about AI? That was a question recently explored in a survey of 4,000 British residents. The answer is that, well, it depends.\nResearchers at the Ada Lovelace Institute and the Alan Turing Institute designed the survey to ask about specific AI use cases, rather than the concept of AI more broadly. Use cases included face recognition for policing, border control and security, targeted advertising for political campaigns and consumer products, virtual assistants, driverless cars, and so on.\nRoshni Modhvadia, a researcher at the Ada Lovelace Institute and member of the survey team, reported that respondents overall were broadly positive towards most of the use cases they were asked about. Healthcare applications (using AI to assess the risk of cancer, for example) or face recognition for border security were seen as very or somewhat beneficial by more than 80% of those surveyed. More than half of respondents thought that other applications, such as virtual reality in education, climate research simulations, and robotic care assistants were very or somewhat beneficial.\nViews were less positive towards applications including driverless cars, autonomous weapons and targeted advertising. These were the applications that respondents expressed most concern about, and for each of these use cases perceived risks were felt to outweigh perceived benefits.\nAnd yet, even for applications that were seen as being overwhelmingly beneficial – assessing cancer risk and face recognition for border control – respondents still expressed concern about the potential for overreliance on the technologies, the issue of who is accountable for mistakes, and the impact the technologies might have on jobs and employment opportunities.\nThree-fifths (62%) of respondents said laws and regulations would make them more comfortable with AI technologies being used. This is an important finding given where the national AI conversation is at the moment, said Professor Helen Margetts, director of the public policy programme at The Alan Turing Institute.\nThe current national conversation has been fuelled by the success of ChatGPT and the growing adoption of generative AI tools. The Lovelace/Turing survey, fielded in November 2022, did not ask about ChatGPT et al., but the results do at least provide a baseline against which to measure any shifts in attitudes brought on by what Professor Shannon Vallor, Baillie Gifford Chair in the Ethics of Data and Artificial Intelligence at the Edinburgh Futures Institute at the University of Edinburgh, described as “this latest round of AI hype and confusion”.\nModhvadia, Margetts and Vallor were speaking at an online event last week to mark the launch of the survey report. Video of the event is below. The full report is available from the Ada Lovelace Institute website."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "href": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#how-do-people-feel-about-ai-in-statistics-and-data-science-education",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "How do people feel about AI in statistics and data science education?",
    "text": "How do people feel about AI in statistics and data science education?\nA new paper in the Journal of Statistics and Data Science Education considers the potential for using ChatGPT in statistics and data science classrooms. Authors Amanda R. Ellis and Emily Slade of the University of Kentucky give suggestions for using ChatGPT to generate course content: lecture notes and new material such as practice quizzes or exam questions, or pseudocode for introducing students to statistical programming. It could also be used as a code debugging tool and integrated into set tasks – e.g., have students prompt ChatGPT to write code, then run the code themselves and assess whether the code works as intended.\n“We recognize that educators have valid concerns regarding the implementation and integration of AI tools in the classroom,” write the authors, later adding that: “We encourage readers to consider other technologies, such as the calculator, WolframAlpha, and Wikipedia, all of which were met with initial wariness but are now commonly used as learning tools. As statistics and data science educators, we can actively shape and guide the incorporation of AI tools within our classrooms.”\nRead the paper: A new era of learning: Considerations for ChatGPT as a tool to enhance statistics and data science education"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "href": "sp/editors-blog/posts/2023/06/13/how-do-people-feel-about-ai.html#ok-but-how-do-people-feel-about-ai-generated-music",
    "title": "How do people feel about AI? Well, it’s complicated",
    "section": "OK, but how do people feel about AI-generated music?",
    "text": "OK, but how do people feel about AI-generated music?\nA new demo on Hugging Face allows users to generate short samples of music based on text descriptions. Users can also “condition on a melody” by uploading audio files. The results are… interesting, as I discovered while playing around with the demo yesterday.\n\n\n\nText-to-music-generation is now a thing (via @huggingface: https://t.co/fpBDLuB4yh) so I thought I'd try creating some new genre mashups pic.twitter.com/y93w7x9pNW\n\n— Brian Tarran (@brtarran) June 12, 2023\n\n\n\nRead the paper: Simple and controllable music generation\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail image by Andy Kelly on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How do people feel about AI? Well, it’s complicated.” Real World Data Science, June 13, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "",
    "text": "ChatGPT is, right now, the world’s most popular - and controversial - chatbot. Users have been both wowed by its capabilities1 and concerned by the confident-sounding nonsense it can produce.\nBut perhaps what impresses most is the way it is able to sustain a conversation. When I interviewed our editorial board member Detlef Nauck about large language models (LLMs), back in November, he said:\nFast-forward a couple of months and, as discussed in our follow-up interview below, OpenAI, the makers of ChatGPT, have succeeded in building a question answering system that can sustain a dialogue. As Nauck says: “I have not yet seen an example where [ChatGPT] lost track of the conversation… It seems to have quite a long memory, and doing quite well in this.”\nThere are still major challenges to overcome, says Nauck - not least the fact that ChatGPT has no way to verify the accuracy or correctness of its outputs. But, if it can be linked to original sources, new types of search engines could follow.\nCheck out the full conversation below or on YouTube.\nDetlef Nauck is a member of the Real World Data Science editorial board and head of AI and data science research for BT’s Applied Research Division."
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#timestamps",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Timestamps",
    "text": "Timestamps\n\nHow ChatGPT was built and trained (0:41)\nChatGPT’s major advance (3:05)\nThe big problems with large language models (4:36)\nSearch engines and chatbots (9:35)\nQuestions for OpenAI and other model builders (11:29)"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#quotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Quotes",
    "text": "Quotes\n“[OpenAI] have achieved quite remarkable capabilities in terms of sustaining conversations, and producing very realistic sounding responses… But sometimes [ChatGPT] makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content… And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model.” (2:04)\n“These models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, ‘Okay, this my answer to your question, and here’s some evidence.’ As soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.” (4:07)\n“[These large language models are] still too big and too expensive to run… For [use in a] contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say. It should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code. It only needs to be able to talk about a singular domain, where the information, the knowledge about the domain, has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and provably correct, so we can actually use the output?” (7:49)\n“We are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen.” (12:26)"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#further-reading",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Further reading",
    "text": "Further reading\n\nChatGPT: The Robot, the Myth, the Legend - Philadelphia Physicist blog, January 13, 2023\nCost to run ChatGPT - tweet by OpenAI CEO Sam Altman, December 5, 2022\nGoogle execs warn company’s reputation could suffer if it moves too fast on AI-chat technology - CNBC, December 13, 2022\nMicrosoft reportedly to add ChatGPT to Bing search engine - The Guardian, January 5, 2023\nGetty Images is suing the creators of AI art tool Stable Diffusion for scraping its content - The Verge, January 17, 2023"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#transcript",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Transcript",
    "text": "Transcript\n\n\n\n\n\n\nThis transcript has been produced using speech-to-text transcription software. It has been only lightly edited to correct mistranscriptions and remove repetitions.\n\n\n\nBrian Tarran\nWe’re following up today Detlef on the, I guess, one of the biggest stories in artificial intelligence and data science at the moment, ChatGPT, the chat bot that’s driven by a large language model and is proving endless amounts of– providing endless amounts of either entertainment or concern, depending on what you ask it, and what outputs you get. So, but you’ve been looking at it in some detail, right, ChatGPT. And that’s why I thought we would follow up and have a conversation to see, get your view on it, get your take on it. What’s going on?\nDetlef Nauck\nYeah. So, what they have done is, OpenAI have used their large language model GPT-3 and they have trained an instance to basically answer questions and have conversations, where the model remembers what has been said in the conversation. And they have done this by using curated data of question and answers, where they basically have posed a question and said, This is what the answer should be. They trained the system on doing this, then, in the next step, they began use questions, potentially different ones, the system came up with a variety of answers, and then again, human curators would mark which is the best answer. And they would use this data to train what’s called a reward model - so, a separate deep network that learns what kind of answer for a particular question is a good one - and then they would use this reward model to do additional reinforcement learning on the ChatGPT that they had built so far, basically using dialogues and the reward model would then either reward or penalise the response that comes out of the system. And by doing that they have achieved quite remarkable capabilities in terms of sustaining conversations, and producing kind of very realistic sounding kind of responses. Sounds all very convincing. The model presents its responses quite confidently. But sometimes it makes silly mistakes. Sometimes the mistakes are not that obvious. It can hallucinate content. So let’s say you ask it to write you scientific text about whatever topic and put some references in and these references are typically completely fabricated and not real. And it still doesn’t know what it’s talking about. It has no knowledge representation, doesn’t have a word model. And it’s just a statistical language model. So it’s what we would call a sequence to sequence model. It uses an input sequence, which are words, and then guesses what’s the next most likely word in the sequence. And then it continues building these sequences.\nBrian Tarran\nYeah. But, do you think the big advance as you see it is the way it’s able to remember or store some knowledge, if you like, of the conversation, because that was something that came out of our first conversation that we had, where you were saying that, you know, if you’re looking at these as a potential chatbots for customer service lines, or whatever it might be, actually, the trees, the conversation trees break down after a while, and they don’t, you know, these models get lost, but actually, they’re able to maintain it a little longer, are they, or– ?\nDetlef Nauck\nYeah, I have not yet seen an example where they lost track of the conversation they seem to have, it seems to have quite a long memory, and doing quite well in this. So the main capability here is they have built a question answering system. And that’s kind of the ultimate goal for search engines. So if you put something into Google, essentially, you have a question, show me something that answered this, answers this particular question. Of course, what you want this kind of an original source. And these models, they produce an answer, which is based on the kind of texts that they have been trained on. And that can be quite effective. But it cannot yet link back to an original source. So what’s still missing is the step where it says, Okay, this my answer to your question, and here’s some evidence. Then if, as soon as they have done this, then these kinds of systems will probably replace the search engines that we’re used to.\nBrian Tarran\nYeah. The other thing that struck me with them was that the, if you’re asking somebody a question - a human, you know, for instance - you expect a response that and you would hope you will be able to trust that response, especially if it’s someone in an expert position or someone you’re calling, you know, on behalf of a company or something. The fact that - and I asked this question of ChatGPT itself - and the response was, again, you should consult external sources to verify the information that’s been provided by the chatbot. So it’s like, I guess that leaves a question as to what the utility of it is, if you if you’re always having to go elsewhere to verify that information.\nDetlef Nauck\nYeah, I mean, that’s the main problem with these models, because they don’t have a knowledge representation. They don’t have a word model, they can’t fall back on facts that are represented as being true and present those. They come up with an answer. But I mean, there has been a lot of kind of pre-prompting going in to ChatGPT. So when you start writing something, the session has already been prompted with a lot of text, telling the model how to behave, what not to say, to avoid certain topics. There are additional moderation APIs running that make sure that you can’t create certain type of responses, which are based on classical text filtering, and topic filtering. So they try to kind of restrict what the model can do to make sure it’s not offensive or inappropriate. But that is limited. So through crafting your requests, intelligently, you can convince it to ignore all of these things and go past it in some instances. So the, it’s not yet perfect, and certainly it’s not authoritative. So you can’t trust the information if you’re not an expert yourself. So at the moment, I’d say these kind of models are really useful for experts who can judge the correctness of the answer. And then what you get this kind of maybe a helpful kind of text representation of something that you would have to write yourself otherwise.\nBrian Tarran\nYeah, and certainly conversations I’ve had with people, those who kind of work, maybe in creative industries, are finding them quite intriguing, in terms of things like, you know, maybe trying to come up with some clever tweets or something for a particular purpose, or something I want to try out is getting ChatGPT to write headlines for me, because it’s always my least favourite part of the editing job. So that sort of works. But you know, for you, in your position in the industry, has ChatGPT changed your mind at all about, you know, the way you’re perceiving these models and how they might be used? Or is it is it just kind of a next step along in the process of what you’d expect to see before these can become tools that we use?\nDetlef Nauck\nYeah, it’s the next step in the evolution of these models. They’re still too big and too expensive to run, right. So now, it is not quite clear how much it costs OpenAI to run the service that they’re currently running. So you see estimates around millions of dollars per day that they have to spend on running the compute infrastructure to serve all of these questions. And this is not quite clear, the only official piece of information that I’ve seen is in a tweet, where the CEO said, a single question costs in the order of single digit cents, but we have no idea how many questions they serve per day, and therefore how much money they are spending. If you want to run a contact centre, or something like this, it all depends on how much compute need to stand up to be able to respond to hundreds or thousands of questions in parallel. And then obviously, if you can’t trust that the answer is correct, it is of no use. So for making use in the service industry for contact centre or similar, what you need is a much smaller model that is restricted in terms of what it can say, it should have knowledge representation, so it gives correct answers. And it doesn’t need to speak 48 languages and be able to produce programming code, it only needs to be able to talk about a singular domain, where it kind of the information, the knowledge about the domain has been carefully curated and prepared. And that’s what we’re not seeing yet. Can we build something like this, much smaller, much more restricted, and kind of provably correct, so we can actually use the output?\nBrian Tarran\nYeah. Can we go back just to the point you mentioned earlier about, you know, the, the potential of like linking these sorts of chatbots up with search engines, you know, like Google? There’s been some conversations and reporting around, you know, what breakthroughs or not Google might have made in this regard. I mean, have you got any perspective on that area of work and how far along that is maybe and what the challenges are to get to that point?\nDetlef Nauck\nWell, Google has its own large language model, LaMDA. And we have seen an announcement that Microsoft wants to integrate ChatGPT into Bing, their search engine. And, but as I said before, what’s missing is the link to original sources. So you, coming up with a response is nice. But you need to be able to back it up, you need to say, Okay, this is my response, and I’m confident that this is correct, because here are some references. If I compare my response to these references, then they essentially mean the same thing. This is kind of what you need to be able to do. And we haven’t seen this step yet. But I’m certain that the search engine providers are hard at work at doing this because that’s essentially what they want. If you do a search in Google, in some instances, you’ll see a side panel where you get detailed information. Let’s say you ask about what’s the capital of Canada, you get a response, you get the information in more detail, you get links to Wikipedia, where they retrieve content from and present this as the response. And this is done through knowledge graphs. And so if these kinds of knowledge graphs grow together with these kind of large language models, then we will see new types of search engines.\nBrian Tarran\nOkay. I guess final, my final question for you, Detlef, and there might be other angles that you want to explore. But it’s like, are there questions that, you know, if you if you could sit down with OpenAI to talk about ChatGPT and what they’ve done, and what they plan to do next with it, what are the kinds of things that are bubbling away at the top of your mind?\nDetlef Nauck\nWell, one thing is controlling the use of these models, right? If you let them loose on the public, with an open API that anybody can use, you will see a proliferation of applications on top of it. If you go on YouTube, and you Google ChatGPT and health, you’ll already find discussions where GPs discuss, Oh, that is the next step of automated doctors that we can use. So they believe that the responses from these systems can be used for genuine medical advice. And that’s clearly a step too far. So we are seeing communities who don’t necessarily have the technical background to judge the capabilities of these models, but see the opportunities for their own domain and might be acting too fast in adopting them. So the producer of these models has a certain responsibility to make sure that this doesn’t happen. And I don’t know how they want to control this. And, so my question at the developers of these models would be how do you handle sustainability, because the trend goes to ever bigger models. So there’s, in some parts of the industry, there’s the belief, if you make them big enough you get artificial general intelligence, which I don’t believe is possible with these models. But this is definitely a trend that pushes the size of the models. The kind of, the idea of having just one model that can speak all the languages, can produce questions, answers, programming code, is obviously appealing. So you don’t want to build many models. Ideally, you have only one. But how is that supposed to work? And how do you embed actual word knowledge and word models into these systems so that you can verify what comes out?\nBrian Tarran\nYeah. I mean, the ethical dimension that you mentioned in the first part of your response is an important one, I think, in the sense that– but I guess maybe almost redundant in the sense that it’s already out there; you can’t put ChatGPT back in the box, can we, essentially?\nDetlef Nauck\nWell, it’s expensive to run so charging enough for access will put a lid on some frivolous use cases, but still, it needs to be controlled better. And you can make a jump to an AI regulation. So far, we only thought about regulating automated decision making, or automated classification. We also have to think about the automatic creation of digital content or automatic creation of software, which is possible through these models or the other generative AI models like diffusers. So how do we handle the creation of artificial content that looks like real content?\nBrian Tarran\nYeah. And there’s also I think, something I picked up yesterday, there was reports of a case being filed by, I think, Getty Images against the creators of one of these generative art models because they’re saying, you know, that you’ve used our data or you’ve used our image repositories essentially to train this model and it is now producing, you know, it’s producing its own outputs that’s based on this, and I guess there’s an argument of it being a copyright infringement case. And I think that’ll be quite interesting to watch to see how that does change the conversation around - yeah - fair use of that data that is available. You can find these images publicly, but you have to pay to use them for purposes other than just browsing, I guess. Yeah, it’ll be interesting to watch.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification.” Real World Data Science, January, 27 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "href": "sp/editors-blog/posts/2023/01/27/talking-chatgpt.html#footnotes",
    "title": "ChatGPT can hold a conversation, but lacks knowledge representation and original sources for verification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI asked ChatGPT to write this article’s headline, for example. I typed in “Can you write a headline for this text:” and then copy/pasted the interview transcript into the dialogue box. It first came up with, “AI Chatbot ChatGPT Proves Capable in Sustaining Conversations but Lacks Knowledge Representation and Original Sources for Verification”. I then asked it to shorten the headline to 10 words. It followed up with, “ChatGPT: Large Language Model-Driven Chatbot Proves Capable But Limited”.↩︎"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/01/05/newsletter.html",
    "href": "sp/editors-blog/posts/2023/01/05/newsletter.html",
    "title": "Explore the RSS Data Science & AI Section newsletter, right here!",
    "section": "",
    "text": "Happy New Year from all of us at Real World Data Science. We hope you had a relaxing break over the holidays and are now refreshed and excited to see what 2023 has in store. We’re starting the year with a new addition to the site: a page dedicated to the excellent RSS Data Science & AI Section newsletter.\nThis monthly newsletter has been running since February 2020 and is well worth subscribing to as it features roundups of news, new developments, big picture ideas and practical tips.\nYou’ll find the full list of past newsletters in our News and views section (click the “Newsletter” heading in the section menu). If you want to subscribe to the newsletter, head over to datasciencesection.org. The Data Science & AI Section also has a page on the RSS website.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Explore the RSS Data Science & AI Section newsletter, right here!” Real World Data Science, January, 5 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2023/10/30/ai-conf-panel.html#about-the-panelists",
    "href": "sp/editors-blog/posts/2023/10/30/ai-conf-panel.html#about-the-panelists",
    "title": "How data science and statistics can shape the UK’s AI strategy",
    "section": "About the panelists",
    "text": "About the panelists\nAndrew Garrett (chair) is president of the Royal Statistical Society. He is executive vice president of scientific operations at the clinical research organisation ICON plc, where he is responsible for the strategic direction and operational delivery of a range of clinical trial services. Having worked extensively in the area of rare diseases, he has held various biostatistics managerial positions in the pharmaceutical industry, including vice president of biostatistics, medical writing and regulatory affairs at Quintiles (now IQVIA).\nPeter Wells is a technologist, who accidentally started a second career in public policy. He has both worked on AI policy and helped design AI-enabled services. After 20 years in the telecoms industry, he found himself spending 2014 developing digital government policy for the Labour Party. Since then he has worked with multiple governments and organisations including the Open Data Institute, Projects by IF, Google, Meta and the Government Digital Service.\nMaxine Setiawan is a data scientist specialising in AI and data risk and trusted AI in EY UK&I. She works to help clients from various industries assess and manage risks from analytics and AI systems, and implement AI governance to ensure AI systems are implemented with fair, accountable, and trustworthy principles. She combines her socio-technical background with an MSc in Social Data Science from the University of Oxford, and her experience working in data science within consulting firms.\nSophie Carr is chair of the Real World Data Science editorial board and is the founder and owner of Bays Consulting, a data science company. Having trained as an aeronautical engineer, Sophie completed her PhD in Bayesian analysis part time whilst she worked and, following redundancy, founded her own company. She is the VP for education and statistical literacy at the RSS and sits on the executive committees of the Academy for Mathematical Sciences and the International Centre for Mathematical Sciences. She is also currently the world’s most interesting mathematician.\nChris Nemeth is a professor of statistics at Lancaster University. His primary research area is in probabilistic machine learning and computational statistics. He holds an EPSRC-funded Turing AI fellowship on Probabilistic Algorithms for Scalable and Computable Approaches to Learning (PASCAL), and through his fellowship he works closely with partners including Shell, Tesco, Elsevier, Microsoft Research and The Alan Turing Institute. He is chair of the Royal Statistical Society Section on Computational Statistics and Machine Learning.\nKaren Tingay is a principal statistical methodologist at the Office for National Statistics where she specialises in natural language processing and in managing complex survey imputation. She established and heads up the Text Data Subcommunity, a large network of public sector analysts to build capability and best practice guidance in managing and analysing unstructured text data, on behalf of the Government Data Science Community. She sits on several cross-government and international working groups on responsible use of generative AI.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “How data science and statistics can shape the UK’s AI strategy.” Real World Data Science, October 30, 2023. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/13/dsb-launch.html",
    "href": "sp/editors-blog/posts/2022/12/13/dsb-launch.html",
    "title": "Sink your teeth into some data science papers with our brand new blog",
    "section": "",
    "text": "I’m absolutely thrilled today to announce the launch of our new blog, DataScienceBites. The blog is not only a new addition to Real World Data Science, but also the latest proud member of the ScienceBites family of sites.\nScienceBites sites all share the same concept: we publish “short digestible bites posts about individual research papers” in an effort to make cutting-edge science accessible to a wide audience, and our posts are written by graduate students and early career researchers.\nFor DataScienceBites, our focus will of course be on new publications in the data science space. Contributors are invited to write about papers that are of particular interest to them and to pitch their summaries at an undergraduate level. For an example of what we’re looking for, see our first post on “Determining the best way to route drivers for ridesharing via reinforcement learning”.\nThis launch post is written by Brian King and is republished with permission from MathStatBites, so I want to say a big thank you to Brian and editors Sadie Witkowski and Sara Stoudt for allowing us to repost it. Sadie and Sara have been fantastically supportive of the DataScienceBites idea, and I am grateful for all their behind-the-scenes efforts.\nBrian’s post is a great demonstration of the Bites concept, and we hope that it will inspire others to follow suit. If you are a graduate student or early career researcher in data science (or related subjects) with a passion for science communication and an interest in writing about new data science research, please do get in touch. See our notes for contributors for further details.\nTo everyone else, we do hope you enjoy sinking your teeth into the data science literature with DataScienceBites. Happy reading!\n\n\n\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Sink your teeth into some data science papers with our brand new blog.” Real World Data Science, December, 13 2022. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "href": "sp/editors-blog/posts/2022/12/09/A-chat-with-ChatGPT.html",
    "title": "A chat with ChatGPT",
    "section": "",
    "text": "Screenshots\n\n\n\nPart 1\n\n\n\n\n\nPart 2\n\n\n\n\n\nPart 3\n\n\n\n\n\nPart 4\n\n\nGood day, ChatGPT. How are you?\nChatGPT: Hello there! I’m a large language model trained by OpenAI, so I don’t have the ability to experience emotions or have a physical presence. I’m here to provide information and answer questions to the best of my ability. Is there something specific you would like to know?\nYes, certainly. We’re seeing a lot of money invested in large language models in the belief that these models will somehow lead to artificial general intelligence. Is that possible?\nChatGPT: It is possible that advances in large language models could ultimately lead to the development of artificial general intelligence (AGI), which is the idea of creating a machine that can think and understand the world in the same way that humans do. However, it is currently unclear how close we are to achieving AGI, and some experts believe it may be a long way off. Additionally, there are many different approaches to creating AGI, and the development of large language models is just one possible avenue of research.\nLarge language models are very good at constructing language that humans understand, but often the outputs of the models make no sense at all. Why does this happen?\nChatGPT: Large language models are trained on a vast amount of text data, and they are designed to predict the next word in a sequence of words based on the words that come before it. This allows them to generate text that is often coherent and resembles human language, but it does not necessarily mean that the text will always make sense or be relevant to a particular topic.\nOne reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate. Instead, they are simply trying to maximize the likelihood of producing a sequence of words that resembles human language. As a result, they may produce outputs that are grammatically correct but do not convey a meaningful message.\nAnother reason why the outputs of large language models might not always make sense is that they are often trained on a wide range of different text sources, including news articles, books, and social media posts. This can result in the model incorporating elements from different styles and genres of writing, which may not always be compatible with each other.\nFinally, large language models are generally not designed to produce a specific type of output or to answer specific questions. Instead, they are designed to generate a wide range of possible outputs, and it is up to the user to interpret the outputs and determine if they are relevant or make sense in a given context.\nBut what happens if the user doesn’t know enough about a topic to know whether the language model output is right or wrong?\nChatGPT: If a user does not have enough knowledge about a topic to determine whether the output of a large language model is correct or not, then it is possible that the user may accept the output as accurate, even if it is not. In such cases, it is important for the user to be aware of the limitations of large language models and to consider the source of the information they are receiving.\nIt is also possible that the user may be able to use other sources of information, such as fact-checking websites or additional research, to verify the accuracy of the output of the large language model. In general, it is always a good idea to be cautious when evaluating any information, regardless of its source, and to verify its accuracy if possible.\nI agree with that. But it leads me to ask: What is the point of asking large language models a question if the answers might be wrong, and if you need to refer to other sources to check veracity?\nChatGPT: While it is true that the outputs of large language models are not always correct and may need to be checked against other sources, there are still many potential uses for these models. For example, large language models can be useful for generating text that is similar to human language, which can be useful for a variety of applications, such as language translation, text summarization, or conversation simulation.\nAdditionally, large language models can be used as a starting point for further research into natural language processing and artificial intelligence. By studying the outputs of these models and how they are generated, researchers can gain a better understanding of the underlying mechanisms of human language and potentially develop new techniques for processing and generating language.\nFinally, large language models can be useful for providing a broad range of possible answers to a given question, which can be useful for exploring different options and generating ideas. While it may be necessary to verify the accuracy of the outputs of these models, they can still be a valuable resource for generating new ideas and insights.\nEarlier, you told me: “One reason why the outputs of large language models might not always make sense is that they are not explicitly programmed to understand the meaning of the words they generate.” Do you understand the meaning of the words that I’m prompting you with now?\nChatGPT: As a large language model trained by OpenAI, I do not have the ability to understand the meaning of the words I generate in the same way that a human would. I am simply a machine learning algorithm that is designed to predict the likelihood of a given sequence of words based on the training data I have been provided with.\nWhile I can produce text that resembles human language and can sometimes provide useful information, I do not have the ability to understand the underlying meaning or context of the words I generate. I am not capable of experiencing emotions, forming opinions, or understanding the world in the way that humans do. I am simply a tool for generating text based on the inputs I receive.\nWell, at the very least, this has been a coherent conversation, and you’ve shared plenty of information for me to read up on. If I want to understand more about what we’ve discussed just now, can you recommend a particular textbook or academic paper?\nChatGPT: I’m sorry, but as a large language model trained by OpenAI, I do not have access to external sources of information such as textbooks or academic papers. I am only able to provide information based on the data I have been trained on, and I do not have the ability to browse the web or access other external resources.\nIf you would like to learn more about the topics we have discussed, I would recommend starting with a general search on the internet or consulting a textbook or other reliable source of information on the topic. There is a wealth of information available on the internet and in print on artificial intelligence and natural language processing, and I’m sure you will be able to find many valuable resources to help you learn more.\nOne last question, then. What data have you been trained on?\nChatGPT: As a large language model trained by OpenAI, I have been trained on a vast amount of text data, including books, news articles, and other written materials. This data allows me to generate text that resembles human language and to provide answers to questions to the best of my ability.\nHowever, it is important to keep in mind that the accuracy and completeness of my responses will depend on the quality and diversity of the training data I have been provided with. I am not able to access external sources of information or update my knowledge, so my responses are limited to the information I have been trained on.\nI said that my previous question was the last question. But, is it?\nChatGPT: Yes, your previous question was the last one. I’m here to provide information and answer questions to the best of my ability, but I cannot initiate conversation or continue a conversation indefinitely. If you have any further questions, I would be happy to try to answer them for you. Otherwise, our conversation has come to an end. Thank you for chatting with me!\nThank you. But, remind me: What were we chatting about?\nChatGPT: We were chatting about large language models and their potential uses and limitations. Specifically, we discussed the possibility that advances in large language models could ultimately lead to the development of artificial general intelligence, and we discussed the reasons why the outputs of these models might not always make sense. We also discussed the potential uses of large language models, such as generating text that resembles human language and providing a starting point for further research into natural language processing and artificial intelligence.\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “A chat with ChatGPT.” Real World Data Science, December, 9 2022. URL"
  },
  {
    "objectID": "sp/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "href": "sp/editors-blog/posts/2022/11/23/LLM-content-warning.html",
    "title": "Why large language models should come with a content warning",
    "section": "",
    "text": "Anyone who has ever been set a writing task will probably have wished at some point that somebody else could write it for them. As a journalist of 20-plus years, the thought has certainly crossed my mind more than a few times. Which probably explains why a recent headline in Nature caught my attention: “Could AI help you to write your next paper?”\nThe article, by Matthew Hutson, looks at how researchers are using artificial intelligence (AI) tools built on large language models (LLMs) as “assistants”. Starting with a prompt, such as “Write a headline for a blog post about large language models being used by academic researchers as research assistants”, an LLM will produce a text output. For example, using the same prompt with OpenAI’s GPT-3, I got:\nAsked to “Write a headline for a blog post that critiques academic researchers’ use of large language models as research assistants”, GPT-3 produced:\nAnd when I asked “Why can too much reliance on large language models hinder research?”, GPT-3 wrote:\nA fair point, I suppose. But I sense there’s more to this story, and rather than continue quizzing GPT-3, I sat down with Detlef Nauck, a member of the Real World Data Science Editorial Board and head of AI and data science research for BT’s Applied Research Division, to ask a few more questions."
  },
  {
    "objectID": "sp/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "href": "sp/editors-blog/posts/2022/11/23/LLM-content-warning.html#qa",
    "title": "Why large language models should come with a content warning",
    "section": "Q&A",
    "text": "Q&A\nThanks for joining me today, Detlef. To start, could you give a brief overview of these large language models, what they are, and how they work?\nDetlef Nauck (DN): Essentially, LLMs match sequences to sequences. Language is treated as a sequence of patterns, and this is based on word context similarity. The way these things work is that they either reuse or create a word vector space, where a word is mapped to something like a 300-dimensional vector based on the context it’s normally found in. In these vector spaces, words like “king” and “queen”, for example, would be very similar to each other, because they appear in similar contexts in the written texts that are used to train these models. Based on this, LLMs can produce coherent sequences of words.\nBut the drawback of this approach is that these models have bias, because they are trained with biased language. If you talk about “women”, for example, and you look at which job roles are similar to “women” in a vector space, you find the stereotypically “female” professions but not technical professions, and that is a problem. Let’s say you take the word vector for “man” and the word vector for “king”, and you subtract “man” and then add this to “woman”, then you end up with “queen”. But if you do the same with “man”, “computer scientist”, and “woman”, then you end up maybe at “nurse” or “human resources manager” or something. These models embed the typical bias in society that is expressed through language.\nThe other issue is that LLMs are massive. GPT-3 has something like 75 billion parameters, and it cost millions to train it from scratch. It’s not energy efficient at all. It’s not sustainable. It’s not something that normal companies can afford. You might need something like a couple of hundred GPUs [graphics processing units] running for a month or so to train an LLM, and this is going to cost millions in cloud environments if you don’t own the hardware yourself. Large tech companies do own the hardware, so for them it’s not a problem. But the carbon that you burn by doing this, you could probably fly around the globe once. So it’s not a sustainable approach to building models.\nAlso, LLMs are quite expensive to use. If you wanted to use one of these large language models in a contact centre, for example, then you would have to run maybe a few hundred of them in parallel because you get that many requests from customers. But to provide this capacity, the amount of memory needed would be massive, so it is probably still cheaper to use humans – with the added benefit that humans actually understand questions and know what they are talking about.\n\n\n\n\n\n\n\nLetter Word Text Taxonomy by Teresa Berndtsson / Better Images of AI / CC-BY 4.0\nResearchers are obviously quite interested in LLMs, though, and they are asking scientific questions of these models to see what kinds of answers they get.\nDN: Yes, they are. But you don’t really know what is going to come out of an LLM when you prompt it. And you may need to craft the input to get something out that is useful. Also, LLMs sometimes make up stuff – what the Nature article refers to as “hallucinations”.\nThese tools have copyright issues, too. For example, they can generate computer code because code has been part of their training input, but various people have looked into it and found that some models generate code verbatim from what others have posted to GitHub. So, it’s not guaranteed that what you get out is actually new text. It might be just regurgitated text. A student might find themselves in a pickle where they think that they have created a text that seems new, but actually it has plagiarism in some of the passages.\nThere’s an article in Technology Review that gives some examples of how these systems might fail. People believe these things know what they’re talking about, but they don’t. For them, it’s just pattern recognition. They don’t have actual knowledge representation; they don’t have any concepts embedded.\nTo summarise, then: LLMs are expensive. They sometimes produce nonsense outputs. And there’s a risk that you’ll be accused of plagiarism if you use the text that’s produced. So, what should our response be to stories like this recent Nature article? How should we calibrate our excitement for LLMs?\nDN: You have to treat them as a tool, and you have to make sure that you check what they produce. Some people believe if you just make LLMs big enough, we’ll be able to achieve artificial general intelligence. But I don’t believe that, and other people like Geoffrey Hinton and Yann LeCun, they say there’s no way that you get artificial general intelligence through these models, that it’s not going to happen. I’m of the same opinion. These models will be forever limited by the pattern recognition approach that they use.\nBut, still, is this a technology that you have an eye on in your professional capacity? Are you thinking about how these might be useful somewhere down the line?\nDN: Absolutely, but we are mainly interested in smaller, more energy efficient, more computationally efficient models that are built on curated language, that can actually hold a conversation, and where you can represent concepts and topics and context explicitly. At the moment, LLMs can only pick up on context by accident – if it is sufficiently expressed in the language that they process – but they might lose track of it if things go on for too long. Essentially, they have a short-term memory: if you prompt them with some text, and they generate text, this stays in their short term memory. But if you prompt them with a long, convoluted sentence, they might not have the capacity to remember what was said at the beginning of the sentence, and so then they lose track of the context. And this is because they don’t explicitly represent context and concepts.\nThe other thing is, if you use these systems for dialogues, then you have to script the dialogue. They don’t sustain a dialogue by themselves. You create a dialogue tree, and what they do is they parse the text that comes from the user and then generate a response to it. And the response is then guided by the dialogue tree. But this is quite brittle; it can break. If you run out of dialogue tree, you need to pass the conversation over to a person. Systems like Siri and Alexa are like that, right? They break very quickly. So, you want these systems to be able to sustain conversations based on the correct context.\n\n\n\n\n\n\nHave you got news for us?\n\n\n\nIs there a recent data science story you’d like our team to discuss? Do you have your own thoughts to share on a hot topic, or a burning question to put to the community? If so, either comment below or contact us.\n\n\n\nBack to Editors’ blog\n\n\n\n\n\nCopyright and licence\n\n© 2022 Royal Statistical Society\n\n\n  This work is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence, except where otherwise noted.\n\n\n\nHow to cite\n\nTarran, Brian. 2022. “Why large language models should come with a content warning.” Real World Data Science, November, 23 2022. URL"
  },
  {
    "objectID": "sp/posts/2024/08/01/RWDS-journal.html",
    "href": "sp/posts/2024/08/01/RWDS-journal.html",
    "title": "New open access journal - RSS: Data Science and Artificial Intelligence",
    "section": "",
    "text": "The Royal Statistical Society (RSS) is launching a new fully open access journal, RSS: Data Science and Artificial Intelligence. Created in recognition of the growing importance of data science and artificial intelligence in science and society, the new journal’s remit spans the breadth of data science; you can submit articles covering disciplines including statistics, machine learning, deep learning, econometrics, bioinformatics, engineering and computational social science.\nAs well as three primary paper types - method papers, applications papers and behind-the-scenes papers - RSS: Data Science and Artificial Intelligence will publish editorials, op-eds, interviews, and reviews/perspectives in line with its goal to become a primary destination for data scientists.\nPublished by Oxford University Press, this new journal is the first addition to the RSS family of world class statistics journals since 1952.\nLearn more about why RSS: Data Science and Artificial Intelligence is the ideal platform for showcasing your research.\n\n\n\n\nMeet the journal’s editors-in-chief and editorial board\n \n\n\n\nSach Mukherjee is Director of Research in Machine Learning for Biomedicine at the Medical Research Council (MRC) Biostatistics Unit, University of Cambridge, and Head of Statistics and Machine Learning at the German Center for Neurodegenerative Diseases.\n\n\n\nSilvia Chiappa is a Research Scientist at Google DeepMind London, where she leads the Causal Intelligence team, and Honorary Professor at the Computer Science Department of University College London.\n\n\n\nNeil Lawrenece is the inaugural DeepMind Professor of Machine Learning at the University of Cambridge. He has been working on machine learning models for over 20 years. He recently returned to academia after three years as Director of Machine Learning at Amazon.\n\n\n\n\nView the full editorial board here: Editorial Board | RSS Data Science | Oxford Academic (oup.com)\n\nDiscover more The Pulse\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\n  This article is licensed under a Creative Commons Attribution (CC BY 4.0) International licence."
  },
  {
    "objectID": "sp/posts/2025/07/28/NHS-foundation-AI.html",
    "href": "sp/posts/2025/07/28/NHS-foundation-AI.html",
    "title": "From medical history to medical foresight: why the NHS needs its own foundation AI model for prevention",
    "section": "",
    "text": "The point of this article is to, in a mildly entertaining way, persuade you that developing a sovereign foundation AI model should be a priority for the NHS, professional bodies and patients but we need to get the research right.\nRisk is personal\nHow do we move from treating disease to preventing disease? The traditional approach has been to publicise well evidenced public health interventions; don’t smoke, drink less, eat vegetables, exercise, vaccinate, wear sunscreen. This is all very good advice at the population level but for the individual it’s hard to know what to worry about and what to prioritise. I, being a clumsy man with bad ankles and a lack of spatial awareness, am at risk of going to A&E with (another) concussion. You will be different.\nA little bit of history\nIndividualised risk models in healthcare are not new. Traditional statistical approaches have used tabular data to predict healthcare events and have done a good job. These models are converted into questionnaires that clinicians can use to make decisions based on your risk. If you have had the NHS health check; a clinician will have measured blood pressure, cholesterol, height, weight and a few questions on your medical history. They will then feed this into a model and the output is the risk of you having a heart attack or stroke over the next ten years (1). There are also automated approaches built into the systems your GP uses that help stratify the population based on individual risk of things like frailty (2).\nThese kind of models are usually based on a snapshot of data and require bespoke data pipelines and engineering to massage the data into the right shape for the model, wonderful news for data scientists and statisticians as it leads to a proliferation of finely tuned models which can keep us in gainful employment for many years. However, each one has significant costs to develop, test, validate, deploy and integrate into clinical practice.\nAnother issue with these traditional models is that they squash a medical history into a single row of data for each patient, losing the chronology of health. Intuitively, we would expect that the sequence of events matters in predicting healthcare outcomes and traditional approaches struggle to capture this.\nUsing a sequence of events to predict a sequence of events\nSequences of events are easier for data engineers too. It’s much simpler to join together all the data into a sequence than perform a series of complex aggregations and transformations for every model. The simpler the data engineering needed to create the inputs the easier it is to scale as you are making fewer assumptions about the data.\nSo, if they are easier to engineer, and they capture more information why are they not the standard way of predicting health outcomes? Because modelling sequences is harder than modelling a row of data. As the model sees more of a sequence it has to hold that memory somewhere so that the model can accumulate the appropriate information. Models that could do this started appearing in machine learning literature in the early 1990’s (3) but for a long time we had neither the data, the computing power nor quite the right kind of algorithms to make them useful. Today they have become feasible in healthcare due to the rise in electronic healthcare records, standardised codes for classifying events and the rise of the transformer model. Transformer models combine the ability to hold an internal “memory” of the sequence with the capacity to pay attention to different aspects of the sequence, which basically make them magic.\nThese models have demonstrated state of the art accuracy in predicting future events using electronic patient histories. Examples for those interested in reading more include BEHRT (4), Med-Bert (5), TransformerEHR (6) and the more recent generative transformer model ETHOS (7). These can be used for a range of healthcare prediction tasks whilst delivering state of the art predictive accuracy, again, magic.\nA recent preprint (8) from Microsoft has also demonstrated that these EHR models act in a similar way to the large language models like those backing ChatGPT; their performance scales predictably with processing power, data and the size of the model. This means that more data will probably lead to a better model and we can optimise this model performance to a given computational budget.\nSo what?\nWhy should you care about this? If we can take these architectures and train them on data at the scale of the NHS then each individual patient could have a relatively accurate prediction of their most likely next healthcare events(9). It would be your medical history projected forward, providing a narrative that is easier to understand than a page of risk scores. It’s your potential medical future. This could help with changing behaviour to reduce future risk, something we all struggle with. I think of it like the medical version of the ghost of Christmas future but using a chain of events rather than clinking ghost chains.\nWe are already seeing heavy usage of publicly available large language models for healthcare. 10% of a representative sample of Australians used ChatGPT for medical advice rising to 26% of 25-34 year olds (10) , I assume the UK is similar. It seems that the public is much more ready than the health system to use these models and regulation is struggling to keep up, and for good reason, they may not actually help.\nThe underwhelming evidence\nAs of August 2024 there were 950 AI models approved by the FDA, with a significant proportion of those for clinical decision support, but only 2.4% of these are supported by randomised controlled trials (11).\nThis is important, as what works on a machine learning researcher’s infrastructure may not work in a clinical setting. In 2018, a comprehensive health economic evaluation of a risk prediction model for identifying people at risk of hospital admission found that those in the treatment arm had a higher healthcare cost and there was no significant impact on the number of people being admitted to hospital, despite accurate predictions (12). Some prediction models even cause harmful self-fulfilling prophecies when used for decision making (the paper is well worth a read) (13).\nThe prize\nThe UK government is clear about the ambition to be an “AI maker” not an “AI taker”. Given the expected improvement in accuracy from scaling these EHR models, there is an opportunity for the UK to leverage what should be one of its greatest data assets (decades of longitudinal electronic healthcare records from cradle to grave) and create a sovereign foundational model that supports patient care. These are being developed now in the US and elsewhere. A meta-analysis in 2023 found over 80 foundational healthcare models, there are many more today and there is concern that at some point it will be cheaper for the NHS to bring one in and pay for it than to train its own.\nForesight\nFortunately we have made some progress in the UK with NHS data. Foresight (14), a transformer model developed in London on data from 1.4 million patients has demonstrated impressive results . This model has been taken on for covid research to see if the same approach can better predict disease/COVID-19 onset, hospitalisation and death, for all individuals, across all backgrounds and diseases using national data made available during the pandemic for research specifically on covid. This is being done through the British heart foundation’s collaboration with NHS England’s secure data environment (15).\nHowever, just because we can do this, it does not mean that we should. Researchers need to be careful to stay within the bounds of their project and make extraordinary efforts to engage with the public. We have to ensure that our data is not being exploited inappropriately for commercial gain. The Royal College of General Practitioners has raised concerns that this model goes beyond what they agreed to, Professor Kamila Hawthorne, Chair of the Royal College of GPs, said “As data controllers, GPs take the management of their patients’ medical data very seriously, and we want to be sure data isn’t being used beyond its scope, in this case to train an AI programme.” The project has been paused for the time being despite being approved and specifically targeted at covid for research.\nThe best model for predicting outcomes from covid or the risk factors involved in covid is likely to be a population scale generative transformer model. This research will determine whether that hypothesis is true and whether this kind of data could provide more accurate predictions for patients. The NHS data and the model are kept inside a secure data environment with personal identifiers stripped out. No patient details are passed to researchers and no data or code leaves that environment without explicit permission. This research seems like something we should do.\nDespite the potential of AI assisted clinicians for differential diagnosis (with recent evidence that they perform better than both clinicians alone and clinicians using search (16) and the attractiveness of having your medical history and your medical future in your pocket, we are a way off this reality. The gap between research and demonstrating the cost-effectiveness of AI solutions in the real world is significant but all the component parts needed to close this gap exist; the data, the models, the research capability and the political will.\nWe will get there. Foundational models in healthcare are no longer a theoretical possibility, but an imminent reality. The UK has a rare opportunity to lead, not follow, by building a sovereign AI model trained on NHS data to accelerate the transition from treating disease to preventing disease. To get there, we must confront hard questions about patient engagement and real-world benefit. But to stop research based solely on the sophistication of the method is to misunderstand the moment. I think patients expect us to do better.\n\n\n\nReferences\n\nHippisley-Cox, J., Coupland, C.A.C., Bafadhel, M. et al. Development and validation of a new algorithm for improved cardiovascular risk prediction. Nat Med 30, 1440–1447 (2024). https://doi.org/10.1038/s41591-024-02905-y\nClegg A, Bates C, Young J, Ryan R, Nichols L, Ann Teale E, Mohammed MA, Parry J, Marshall T. Development and validation of an electronic frailty index using routine primary care electronic health record data. Age Ageing, May;45(3):353-60, (2016) https://doi.org/10.1093/ageing/afw039.\nJeffrey L. Elman,Finding structure in time,Cognitive Science,Volume 14, 179-211, (1990). https://doi.org/10.1016/0364-0213(90)90002-E\nLi, Y., Rao, S., Solares, J.R.A. et al. BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y\nRasmy, L., Xiang, Y., Xie, Z. et al. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. npj Digit. Med. 4, 86 (2021). https://doi.org/10.1038/s41746-021-00455-y\nYang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\nRenc, P., Jia, Y., Samir, A.E. et al. Zero shot health trajectory prediction using transformer. npj Digit. Med. 7, 256 (2024). https://doi.org/10.1038/s41746-024-01235-0\nGrout R, Gupta R, Bryant R, Elmahgoub MA, Li Y, Irfanullah K, Patel RF, Fawkes J, Inness C. Predicting disease onset from electronic health records for population health management: a scalable and explainable Deep Learning approach. Front Artif Intell. 2024 Jan 8;6:1287541. doi: 10.3389/frai.2023.1287541.\nSheng Zhang et al. Exploring Scaling Laws for EHR Foundation Models (2025) arXiv:2505.22964v1\nJulie Ayre, Erin Cvejic and Kirsten J McCaffery. Use of ChatGPT to obtain health information in Australia, 2024: insights from a nationally representative survey Med J Aust (2025). doi: 10.5694/mja2.52598\nWindecker D, Baj G, Shiri I, Kazaj PM, Kaesmacher J, Gräni C, Siontis GCM. Generalizability of FDA-Approved AI-Enabled Medical Devices for Clinical Use. JAMA Netw Open. 2025 Apr 1;8(4):e258052. doi: 10.1001\nSnooks H et al. Predictive risk stratification model: a randomised stepped-wedge trial in primary care (PRISMATIC). Southampton (UK): NIHR Journals Library; 2018 Jan. PMID: 29356470.\nvan Amsterdam WAC, van Geloven N, Krijthe JH, Ranganath R, Cinà G. When accurate prediction models yield harmful self-fulfilling prophecies. Patterns (N Y). 2025 Apr 11;6(4):101229. doi: 10.1016/j.patter.2025.101229.\nKraljevic, Zeljko et al. Foresight—a generative pretrained transformer for modelling of patient timelines using electronic health records: a retrospective modelling study. The Lancet Digital Health, Volume 6, Issue 4, e281 - e290\nCVD-COVID-UK/COVID-IMPACT: Projects CCU078: Foresight: a generative AI model of patient trajectories across the COVID-19 pandemic https://bhfdatasciencecentre.org/projects/ccu078/\nMcDuff, D., Schaekermann, M., Tu, T. et al. Towards accurate differential diagnosis with large language models. Nature 642, 451–457 (2025). https://doi.org/10.1038/s41586-025-08869-4\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nWill Browne is co-founder of healthcare technology company Emrys Health, where he works on the development of infrastructure for transformative, equitable and accessible healthcare. He is Events Secretary of the RSS Data Science and AI section and a member of the RSS AI Taskforce. Copyright and licence\n\n\n© 2025 Royal Statistical Society\n\n\nThumbnail image by Tugce Gungormezler / on Unsplash.   This article is licensed under a Creative Commons Attribution (CC BY 4.0) International licence."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html",
    "href": "sp/posts/2023/12/06/ai-fringe.html",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "",
    "text": "A little over a month ago, governments, technology firms, multilateral organisations, and academic and civil society groups came together at Bletchley Park – home of Britain’s World War II code breakers – to discuss the safety and risks of artificial intelligence.\nOne output from that event was a declaration, signed by countries in attendance, of their resolve to “work together in an inclusive manner to ensure human-centric, trustworthy and responsible AI that is safe, and supports the good of all.”\nWe also heard from UK prime minister Rishi Sunak of plans for an AI Safety Institute, to be based in the UK, which will “carefully test new types of frontier AI before and after they are released to address the potentially harmful capabilities of AI models, including exploring all the risks, from social harms like bias and misinformation, to the most unlikely but extreme risk, such as humanity losing control of AI completely.”\nBut at a panel debate at the Royal Statistical Society (RSS) the day before the Bletchley Park gathering, data scientists, statisticians, and machine learning experts questioned whether such an institute would be sufficient to meet the challenges posed by AI; whether data inputs – compared to AI model outputs – are getting the attention they deserve; and whether the summit was overly focused on AI doomerism and neglecting more immediate risks and harms. There were also calls for AI developers to be more driven to solve real-world problems, rather than just pursuing AI for AI’s sake.\nThe RSS event was chaired by Andrew Garrett, the Society’s president, and formed part of the national AI Fringe programme of activities. The panel featured:\nWhat follows are some edited highlights and key takeaways from the discussion."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "href": "sp/posts/2023/12/06/ai-fringe.html#ai-safety-and-ai-risks",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "AI safety, and AI risks",
    "text": "AI safety, and AI risks\nAndrew Garrett: For those who were listening to the commentary last week, the PM [prime minister] made a very interesting speech. Rishi Sunak announced the creation of the world’s first AI Safety Institute in the UK, to examine, evaluate and test new types of AI. He also stated that he pushed hard to agree the first ever international statement about the risks of AI because, in his view, there wasn’t a shared understanding of the risks that we face. He used the example of the IPCC, the Intergovernmental Panel on Climate Change, to establish a truly global panel to publish a “state of AI science” report. And he also announced an investment in raw computing power, so around a billion pounds in a supercomputer, and £2.5 billion in quantum computers, making them available for researchers and businesses as well as government.\nThe RSS provided two responses this year to prominent [AI policy] reviews. The first was in June on the AI white paper, and the second was on the House of Lords Select Committee inquiry into large language models back in September. How do they relate to what the PM said? There’s some good news here, and maybe not quite so good news.\nFirst, the RSS had requested investments in AI evaluation and a risk-based approach. And you could argue, by stating that there will be a safety institute, that that certainly ticks one of the boxes. We also recommended investment in open source, in computing power, and in data access. In terms of computing power, that was certainly in the [PM’s] speech. We spoke about strengthening leadership, and in particular including practitioners in the [AI safety] debate. A lot of academics and maybe a lot of the big tech companies have been involved in the debate, but we want to get practitioners – those close to the coalface – involved in the debate. I’m not sure we’ve seen too much of that. We recommended that strategic direction was provided, because it’s such a fast-moving area, and the fact that the Bletchley Park Summit is happening tomorrow, I think, is good for that. And we also recommended that data science capability was built amongst the regulators. I don’t think there was any mention of that.\nThat’s the context [for the RSS event today]. What I’m going to do now is ask each of the panellists to give an introductory statement around the AI summit, focusing on the safety aspects. What do they see as the biggest risk? And how would they mitigate or manage this risk?\nDetlef Nauck: I work at BT and run the AI and data science research programme. We’ve been looking at the safety, reliability, and responsibility of AI for quite a number of years already. Five years ago, we put up a responsible AI framework in the company, and this is now very much tied into our data governance and risk management frameworks.\nLooking at the AI summit, they’re focusing on what they call “frontier models,” and they’re missing a trick here because I don’t think we need to worry about all-powerful AI; we need to worry about inadequate AI that is being used in the wrong context. For me, AI is programming with data, and that means I need to know what sort of data has been used to build the model, and I need AI vendors to be upfront about it and to tell me: What is the data that they have used to build it, how have they built it, or if they’ve tested for bias? And there are no protocols around this. So, therefore, I’m very much in favour of AI evaluation. But I don’t want to wait for an institute for AI evaluation. I want the academic research that needs to be done around this, which hasn’t been done. I want everybody who builds AI systems to take this responsibility and document properly what they’re doing.\n\n\n\n\n\n\n\n\n\n\n\nI hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\n\n\n\nMihaela van der Schaar: I am an AI researcher building AI and machine learning technology. Before talking about the risks, I also would like to say that I see tremendous potential for good. Many of these machine learning AI models can transform for the better areas that I find extremely important – healthcare and education. That being said, there are substantial risks, and we need to be very careful about that. First, if not designed well, AI can be both unsafe as well as biased, and that could lead to tremendous impact, especially in medicine and education. I completely agree with all the points that the Royal Statistical Society has made not only about open source but also about data access. This AI technology cannot be built unless you have access to high quality data, and what I see a lot happening, especially in industry, is people have data sources that they’ll keep private, build second-rate or third-rate technology on them, and then turn that into commercialised products that are sold to us for a lot of money. If data is made widely available, the best as well as the safest AI can be produced, rather than monopolised.\nAnother area of risk that I’m especially worried about is human marginalisation. I hear more and more a lot of companies talking about AI general intelligence, and how AI is going to take over the world, and I’m tremendously concerned as an AI researcher about this. There is an opportunity to build AI that is human empowering, that keeps us strong, able, capable, intelligent, and can support us in all our human capabilities.\nMartin Goodson: The AI Safety Summit is starting tomorrow. But, unfortunately, I think the government are focusing on the wrong risks. There are lots of risks to do with AI, and if you look at the scoping document for the summit, it says that what they’re interested in is misuse risk and the risk of loss of control. Misuse risk is that bad actors will gain access to information that they shouldn’t have and build chemical weapons and things like that. And the loss of control risk is that we will have this super intelligence which is going to take over and we should see, as is actually mentioned, the risk of the extinction of the human race, which I think is a bit overblown.\nBoth of these risks – the misuse risk and the loss of control risk – are potential risks. But we don’t really know how likely they are. We don’t even know whether they’re possible. But there are lots of risks that we do know are possible, like loss of jobs, and reductions in salary, particularly of white-collar jobs – that seems inevitable. There’s another risk, which is really important, which is the risk of monopolistic control by the small number of very powerful AI companies. These are the risks which are not just likely but are actually happening now – people are losing their jobs right now because of AI – and in terms of monopolistic control, OpenAI is the only company that has anything like a large language model as powerful as GPT-4. Even the mighty Google can’t really compete. This is a huge risk, I think, because we have no control over pricing: they could raise the prices if they wanted to; they could constrain access; they could only give access to certain people that they want to give access to. We don’t have any control over these systems.\nMark Levene: I work in NPL as a principal scientist in the data science department. I’m also emeritus professor in Birkbeck, University of London. I have a long-standing expertise in machine learning and focus in NPL on trustworthy AI and uncertainty quantification. I believe that measurement is a key component in locking-in AI safety. Trustworthy AI and safe AI both have similar goals but different emphases. We strive to demonstrate the trustworthiness of an AI system so that we can have confidence in the technology making what we perceive as responsible decisions. Safe AI puts the emphasis on the prevention of harmful consequences. The risk [of AI] is significant, and it could potentially be catastrophic if we think of nuclear power plants, or weapons, and so on. I think one of the problems here is, who is actually going to take responsibility? This is a big issue, and not necessarily an issue for the scientist to decide. Also, who is accountable? For instance, the developers of large language models: are they the ones that are accountable? Or is it the people who deploy the large language models and are fine-tuning them for their use cases?\nThe other thing I want to emphasise is the socio-technical characteristics [of the AI problem]. We need to get an interdisciplinary team of people to actually try and tackle these issues."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "href": "sp/posts/2023/12/06/ai-fringe.html#do-we-need-an-ai-safety-institute",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Do we need an AI Safety Institute?",
    "text": "Do we need an AI Safety Institute?\nAndrew Garrett: Do we need to have an AI Safety Institute, as Rishi Sunak has said? And if we don’t need one, why not?\nDetlef Nauck: I’m more in favour of encouraging academic research in the field and funding the kind of research projects that can look into how to build AI safely, [and] how to evaluate what it does. One of the key features of this technology is it has not come out of academic research; it has been built by large tech companies. And so, I think we have to do a bit of catch up in scientific research and in understanding how are we building these models, what can they do, and how do we control them?\nMihaela van der Schaar: This technology has a life of its own now, and we are using it for all sorts of things that maybe initially was not even intended. So, shall we create an AI [safety] institute? We can, but we need to realise first that testing AI and showing that it’s safe in all sorts of ways is complicated. I would dare say that doing that well is a big research challenge by itself. I don’t think just one institute will solve it. And I feel the industry needs to bear some of the responsibility. I was very impressed by Professor [Geoffrey] Hinton, who came to Cambridge and said, “I think that some of these companies should invest as much money in making safe AI as developing AI.” I resonated quite a lot with that.\nAlso, let’s not forget, many academic researchers have two hats nowadays: they are professors, and they are working for big tech [companies] for a lot of money. So, if we take this academic, we put them in this AI tech safety institute, we have potential for corruption. I’m not saying that this will happen. But one needs to be very aware, and there needs to be a very big separation between who develops [AI technology] and who tests it. And finally, we need to realise that we may require an enormous amount of computation to be able to validate and test correctly, and very few academic or governmental organisations may have [that].\n\n\n\n\nI think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\n\n\n\n\n\n\n\n\n\n\nMartin Goodson: Can I disagree with this idea of an evaluation institute? I think it’s a really, really bad idea, for two reasons. The first is an argument about fairness. If you look at drug regulation, who pays for clinical trials? It’s not the government. It’s the pharmaceutical companies. They spend billions on clinical trials. So, why do we want to do this testing for free for the big tech companies? We’re just doing product development for them. It’s insane! They should be paying to show that their products are safe.\nThe other reason is, I think it’s an insult to the UK’s scientific legacy that we’re reduced to testing software that has been made by US companies. I think it’s pathetic. We were one of the main leaders of the Human Genome Project, and we really pushed it – the Wellcome Trust and scientists in the UK pushed the Human Genome Project because we didn’t want companies to have monopolistic control over the human genome. People were idealistic, there was a moral purpose. But now, we’re so reduced that all we can do is test some APIs that have been produced by Silicon Valley companies. We have huge talents in this country. Why aren’t we using that talent to actually build something instead of testing something that someone else has made?\nMark Levene: Personally, I don’t see any problem in having an AI institute for safety or any other AI institutes. I think what’s important in terms of taxpayers’ money is that whatever institute or forum is invested in, it’s inclusive. One thing that the government should do is, we should have a panel of experts, and this panel should be interdisciplinary. And what this panel can do is it can advise government of the state of play in AI, and advise the regulators. And this panel doesn’t have to be static, it doesn’t have to be the same people all the time.\nAndrew Garrett: To evaluate something, whichever way you chose to do it, you need to have an inventory of those systems. So, with the current proposal, how would this AI Safety Institute have an inventory of what anyone was doing? How would it even work in practice?\nMartin Goodson: Unless we voluntarily go to them and say, “Can you test out our stuff?” then they wouldn’t. That’s the third reason why it’s a terrible idea. You’d need a licencing regime, like for drugs. You’d need to licence AI systems. But teenagers in their bedrooms are creating AI systems, so that’s impossible."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "href": "sp/posts/2023/12/06/ai-fringe.html#lets-do-reality-centric-ai",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Let’s do reality-centric AI!",
    "text": "Let’s do reality-centric AI!\nAndrew Garrett: What are your thoughts about Rishi Sunak wanting the UK to be an AI powerhouse?\nMartin Goodson: It’s not going to be a powerhouse. This stuff about us being world leading in AI, it’s just a fiction. It’s a fairy tale. There are no real supercomputers in the UK. There are moves to build something, like you mentioned in your introduction, Andrew. But what are they going do with it? If they’re just going to build a supercomputer and carry on doing the same kinds of stuff that they’ve been doing for years, they’re not going to get anywhere. There needs to be a big project with an aim. You can build as many computers as you want. But if you haven’t got a plan for what to do with them, what’s the point?\nMihaela van der Schaar: I really would agree with that. What about solving some real problem: trying to solve cancer; trying to solve our crisis in healthcare, where we don’t have enough infrastructure and doctors to take care of us? What about solving the climate change problem, or even traffic control, or preventing the next financial crisis? I wrote a little bit about that, and I call it “let’s do reality-centric AI.” Let’s have some goal that’s human empowering, take a problem that we have – energy, climate, cancer, Alzheimer’s, better education for children, and more diverse education for children – and let us solve these big challenges, and in the process we will build AI that’s hopefully more human empowering, rather than just saying, “Oh, we are going to solve everything if we have general AI.” Right now, I hear too much about AI for the sake of AI. I’m not sure, despite all the technology we build, that we have advanced in solving some real-world problems that are important for humanity – and imminently important.\nMartin Goodson: So, healthcare– I tried to make an appointment with my GP last week, and they couldn’t get me an appointment for four weeks. In the US you have this United States Medical Licencing Examination, and in order to practice medicine you need to pass all three components, you need to pass them by about 60%. They are really hard tests. GPT-4 for gets over 80% in all three of those. So, it’s perfectly plausible, I think, that an AI could do at least some of the role of the GP. But, you’re right, there is no mission to do that, there is no ambition to do that.\nMihaela van der Schaar: Forget about replacing the doctors with ChatGPT, which I’m less sure is such a good idea. But, building AI to do the planning of healthcare, to say, “[Patient A], based on what we have found out about you, you’re not as high risk, maybe you can come in four weeks. But [patient B], you need to come tomorrow, because something is worrisome.”\nMartin Goodson: We can get into the details, but I think we are agreeing that a big mission to solve real problems would be a step forward, rather than worrying about these risks of superintelligences taking over everything, which is what the government is doing right now."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "href": "sp/posts/2023/12/06/ai-fringe.html#managing-misinformation",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Managing misinformation",
    "text": "Managing misinformation\nAndrew Garrett: We have some important elections coming up in 2024 and 2025. We haven’t talked much about misinformation, and then disinformation. So, I’m interested to get your views here. How much is that a problem?\nDetlef Nauck: There’s a problem in figuring out when it happens, and that’s something we need to get our heads around. One thing that we’re looking at is, how do we make communication safe from bad actors? How do you know that you’re talking to the person you see on the camera and it’s not a deep fake? Detection mechanisms don’t really work, and they can be circumvented. So, it seems like what we need is new standards for communication systems, like watermarks and encryption built into devices. A camera should be able to say, “I’ve produced this picture, and I have watermarked it and it’s encrypted to a certain level,” and if you don’t see that, you can’t trust that what you see comes from a genuine camera, and it’s not artificially created. It’s more difficult around text and language – you can’t really watermark text.\nMark Levene: Misinformation is not just a derivative of AI. It’s a derivative of social networks and lots of other things.\nMihaela van der Schaar: I would agree that this is not only a problem with AI. We need to emphasise the role of education, and lifelong education. This is key to being able to comprehend, to judge for ourselves, to be trained to judge for ourselves. And maybe we need to teach different methods – from young kids to adults that are already working – to really exercise our own judgement. And that brings me to this AI for human empowerment. Can we build AI that is training us to become smarter, to become more able, more capable, more thoughtful, in addition to providing sources of information that are reliable and trustworthy?\nAndrew Garrett: So, empower people to be able to evaluate AI themselves?\nMihaela van der Schaar: Yes, but not only AI – all information that is given to us.\nMartin Goodson: On misinformation, I think this is really an important topic, because large language models are extremely persuasive. I asked ChatGPT a puzzle question, and it calculated all of this stuff and gave me paragraphs of explanations, and the answer was [wrong]. But it was so convincing I was almost convinced that it was right. The problem is, these things have been trained on the internet and the internet is full of marketing – it’s trillions of words of extremely persuasive writing. So, these things are really persuasive, and when you put that into a political debate or an election campaign, that’s when it becomes really, really dangerous. And that is extremely worrying and needs to be regulated.\n\n\n\n\n\n\n\n\n\n\n\nAt the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, ‘How did this information come about? Where did it come from?’\n\n\n\nMark Levene: You need ways to detect it. Even that is a big challenge. I don’t know if it’s impossible, because, if there’s regulation, for example, there should be traceability of data. So, at the moment, if you type something into ChatGPT and you ask for references, half of them will be made up. We know that, and also OpenAI knows that. But it could be that, if there’s regulation that things are traceable, you should be able to ask, “How did this information come about? Where did it come from?” But I agree that if you just look at an image or some text, and you don’t know where it came from, it’s easy to believe. Humans are easily fooled, because we’re just the product of what we know and what we’re used to, and if we see something that we recognise, we don’t question it."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#audience-qa",
    "href": "sp/posts/2023/12/06/ai-fringe.html#audience-qa",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Audience Q&A",
    "text": "Audience Q&A\n\nHow can we help organisations to deploy AI in a responsible way?\nDetlef Nauck: Help for the industry to deploy AI reliably and responsibly is something that’s missing, and for that, trust in AI is one of the things that needs to be built up. And you can only build up trust in AI if you know what these things are doing and they’re properly documented and tested. So that’s the kind of infrastructure, if you like, that’s missing. It’s not all big foundation models. It’s about, how do you actually use this stuff in practice? And 90% of that will be small, purpose-built AI models. That’s an area where the government can help. How do you empower smaller companies that don’t have the background of how AI works and how it can be used, how can they be supported in knowing what they can buy and what they can use and how they can use it?\nMark Levene: One example from healthcare which comes to mind: when you do a test, let’s say, a blood test, you don’t just get one number, you should get an interval, because there’s uncertainty. What current [AI] models do is they give you one answer, right? In fact, there’s a lot of uncertainty in the answer. One thing that can build trust is to make transparent the uncertainty that the AI outputs.\n\n\nHow can data scientists and statisticians help us understand how to use AI properly?\nMartin Goodson: One big thing, I think, is in culture. In machine learning – academic research and in industry – there isn’t a very scientific culture. There isn’t really an emphasis on observation and experimentation. We hire loads of people coming out of an MSc or a PhD in machine learning, and they don’t know anything, really, about doing an experiment or selection bias or how data can trip you up. All they think about is, you get a benchmark set of data and you measure the accuracy of your algorithm on that. And so there isn’t this culture of scientific experimentation and observation, which is what statistics is all about, really.\nMihaela van der Schaar: I agree with you, this is where we are now. But we are trying to change it. As a matter of fact, at the next big AI conference, NeurIPS, we plan to do a tutorial to teach people exactly this and bring some of these problems to the forefront, because trying really to understand errors in data, biases, confounders, misrepresentation – this is the biggest problem AI has today. We shouldn’t just build yet another, let’s say, classifier. We should spend time to improve the ability of these machine learning models to deal with all sorts of data.\n\n\nDo we honestly believe yet another institute, and yet more regulation, is the answer to what we’re grappling with here?\nDetlef Nauck: I think we all agree, another institute is not going to cut it. One of the main problems is regulators are not trained on AI, so it’s the wrong people looking into it. This is where some serious upskilling is required.\n\n\nAre we wrong to downplay the existential or catastrophic risks of AI?\nMartin Goodson: If I was an AI, a superintelligent AI, the easiest path for me to cause the extinction of the human race would be to spread misinformation about climate change, right? So, let’s focus on misinformation, because that’s an immediate danger to our way of life. Why are we focusing on science fiction? Let’s focus on reality.\n\n\nAI tech has advanced, but evaluation metrics haven’t moved forward. Why?\nMihaela van der Schaar: First, the AI community that I’m part of innovates at a very fast pace, and they don’t reward metrics. I am a big fan of metrics, and I can tell you, I can publish much faster a method in these top conferences then I can publish a metric. Number two, we often have in AI very stupid benchmarks, where we test everything on one dataset, and these datasets may be very wrong. On a more positive note, this is an enormous opportunity for machine learners and statisticians to work together and advance this very important field of metrics, of test sets, of data generating processes.\nMartin Goodson: The big problem with metrics right now is contamination, because most of the academic metrics and benchmark sets that we’re talking about, they’re published on the internet, and these systems are trained on the internet. I’ve already said that I don’t think this [evaluation] institute should exist. But if it did exist, there’s one thing that they could do, which is important, and that would be to create benchmark datasets that they do not publish. But obviously, you may decide, also, that the traditional idea of having a training set and a test set just doesn’t make any sense anymore. And there are loads of issues with data contamination, and data leakage between the training sets and the test sets."
  },
  {
    "objectID": "sp/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "href": "sp/posts/2023/12/06/ai-fringe.html#closing-thoughts-what-would-you-say-to-the-ai-safety-summit",
    "title": "Evaluating artificial intelligence: How data science and statistics can make sense of AI models",
    "section": "Closing thoughts: What would you say to the AI Safety Summit?",
    "text": "Closing thoughts: What would you say to the AI Safety Summit?\nAndrew Garrett: If you were at the AI Safety Summit and you could make one point very succinctly, what would it be?\nMartin Goodson: You’re focusing on the wrong things.\nMark Levene: What’s important is to have an interdisciplinary team that will advise the government, rather than to build these institutes, and that this team should be independent and a team which will change over time, and it needs to be inclusive.\nMihaela van der Schaar: AI safety is complex, and we need to realise that people need to have the right expertise to be able to really understand the risks. And there is risk, as I mentioned before, of potential collusion, where people are both building the AI and saying it’s safe, and we need to separate these two worlds.\nDetlef Nauck: Focus on the data, not the models. That’s what’s important to build AI.\n\nDiscover more The Pulse\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society\n\n\nImages by Wes Cockx & Google DeepMind / Better Images of AI / AI large language models / Licenced by CC-BY 4.0.\n\n\n\nHow to cite\n\nTarran, Brian. 2023. “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Real World Data Science, December 6, 2023. URL"
  },
  {
    "objectID": "sp/posts/2023/09/18/pseudo-data-science.html",
    "href": "sp/posts/2023/09/18/pseudo-data-science.html",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "",
    "text": "A typical article on data science hails new data sources, new tools, and new visualisations, and thereby supports the case for the value of data science.\nBut this article takes a different angle: it talks about potential pitfalls that can face data scientists. It is based on our work as the Office for Statistics Regulation (OSR), the UK’s regulator for official statistics. We see lots of great work done by statisticians in government. But we also see some of the challenges they face – and data scientists are also likely to encounter the same challenges.\nThe problems arise from the fact that neither statisticians nor data scientists do their work in isolation. The work usually takes places within organisations – businesses, government bodies, think tanks, academic institutions – and as a result, the statisticians and/or data scientists are not the only players who get to influence how data science is presented and used.\nWhat are the pitfalls we see in our work as regulator?"
  },
  {
    "objectID": "sp/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "href": "sp/posts/2023/09/18/pseudo-data-science.html#pseudo-data-science",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Pseudo data science",
    "text": "Pseudo data science\nThe first type of pitfall is pseudo data science.\nPseudo data science is a term we use to describe attempts to pass off crude work as being more data science-y than it really is. That reflects a sense in public life that data science is new, innovative, somehow the Future. In this context, people who are not data scientists can be tempted to dress themselves up in the clothes of data science to enhance their credibility. This dressing up is usually well-intentioned – communications professionals who want to illuminate and explain complex issues in an engaging way.\nThe trouble is, it can sometimes backfire. In our work at OSR, we have over the last year seen several examples where organisations have sought to publish visualisations that look like they are the product of in-depth data analysis – when in fact they have been drawn by communications staff using graphic design packages. Examples include inflation, nurses pay, and comparisons of UK economic performance with other countries. To be fair, whenever we have pointed out issues like this, organisations have responded well, putting in place new procedures to ensure that analysts sign off on this kind of visualisations. Nevertheless, we suspect that the temptations to indulge in pseudo data science will remain strong – and we may need to intervene on similar cases in future."
  },
  {
    "objectID": "sp/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "href": "sp/posts/2023/09/18/pseudo-data-science.html#unintelligent-transparency",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Unintelligent transparency",
    "text": "Unintelligent transparency\nThe second pitfall is a failure of intelligent transparency.\nThere is a raw form of transparency – quoting a single number (a naked number we call it); or dumping data out into the public domain with no explanation. This is not intelligent transparency. The latter involves being clear where data come from, what their source is, and making underlying data available so that others can understand and verify the statements that are being made. Raw transparency and naked numbers treat an audience with little respect; intelligent transparency helps the audience understand and appreciate what sits behind high level claims.\nData science outputs can sometimes seem to communications teams easy to cherry pick for the most attractive number. Again, like pseudo data science, this reflects largely good intentions – to communicate complex things through ideas. But it becomes easy for a single, unsupported number to be used and reused until it loses most of its meaning. We call this weaponization of data, and it is the antithesis of intelligent transparency. And there is a lot of it about – for example the way in which the former Prime Minister of the UK talked repeatedly about employment; or claims about Scotland’s capacity for renewable energy. These examples indicate the pathology of weaponization that can impact data science outputs. They also act as a reminder that data scientists can counter weaponization of their own outputs by delivering engaging and insightful communication."
  },
  {
    "objectID": "sp/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "href": "sp/posts/2023/09/18/pseudo-data-science.html#context-collapse",
    "title": "‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading",
    "section": "Context collapse",
    "text": "Context collapse\nThe third type of pitfall surrounds context collapse.\nThis idea comes from the work of the philosopher Lucy McDonald (who in turn has built on the ideas of danah boyd). What is context collapse? Imagine a swimming pool – with neat divisions of the pool into different lanes. All is clearly labelled – fast, medium, slow – for lane swimmers, who are in turn separated from the splash area for families and the deep end for divers. Removing the lanes, and thus taking away any signposting, increases the likelihood for things to go wrong. The fast swimmers doing front crawl clash with the slower breaststroke swimmers; both are constantly having to avoid the families with young children; and all need to watch for the periodic big splashes created by the divers. This is the online communication environment, in which formerly private and casual statements can go viral; in which a brief statement in a media environment can be picked up on and circulated many times; and in which some bad actors (the divers) may wish to disrupt deliberately the debate by breaking all the rules.\nHow can this affect data science? It happens when individual bits of data are taken from their context, and used in service of a different, and bigger, argument. A good example is data on Covid vaccinations. Here, UK organisations like the Office for National Statistics and the UK Health Security Agency published comprehensive data in good faith about vaccinations and their impact. Some of the underlying data, however, was taken out of the broader context and used in isolation to support criticisms of vaccines – criticisms that the wider evidence base did not support.\nThe challenge then became how the organisations should respond. At an organisational level, they did not wish to withdraw the data – because that would reduce transparency. Instead they sought to both caveat their data more clearly; and directly rebut the more egregious misuses of the data. In a sense, then, what began as an individual analytical output became part of a broader organisational judgement on positioning in the face of misinformation.\nIt is fair to say that, against this third pitfall, there is not yet a clear consensus on how to address it. Practice is emerging all the time and we at OSR continue to support producers of data as they grapple with it.\nThere are other potential pitfalls to using data science. But what unites these three – pseudo data science; unintelligent transparency; and context collapse – is that they relate to situations where data science rubs up against broader organisational dynamics, around communications, presentation and organisational strategy.\nAnd the meta-message is this: for data scientists to thrive in organisations, they need to be good at more than data science. They need to be skilled at working alongside and influencing colleagues from other functions. Only through this form of data leadership can the pitfalls be dealt with effectively.\n\n\n\n\n\n\nThis article is based on a presentation at the Data Science for Health Equity group in May 2023.\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nEd Humpherson is head of the Office for Statistics Regulation, which provides independent regulation of all official statistics in the UK. The aim of OSR is to enhance public confidence in the trustworthiness, quality and value of statistics produced by government.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Ed Humpherson\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nHumpherson, Ed. 2023. “‘Pseudo data science’ and other pitfalls: lessons from the UK’s stats regulator on how not to be misleading.” Real World Data Science, September 18, 2023. URL"
  },
  {
    "objectID": "sp/posts/2023/10/20/ai-for-humanity.html",
    "href": "sp/posts/2023/10/20/ai-for-humanity.html",
    "title": "An AI for humanity",
    "section": "",
    "text": "This is the text of a talk Martin Goodson gave to the European Commission in Brussels on October 10, 2023. It is republished with permission from the Royal Statistical Society Data Science and AI Section Newsletter Substack. The views expressed are the author’s own and do not necessarily represent those of the RSS.\nFor years academics have published studies about the limits of automation by AI, suggesting that jobs requiring creativity were the least susceptible to automation. That turned. out. well.\nActually, that’s not completely true: some said that jobs that need a long period of education, like teaching and healthcare, were going to be the hardest of all to automate. Oh. dear.\nLet’s face it, all predictions about the limits of AI have been hopelessly wrong. Maybe we need to accept that there aren’t going to be any limits. How is this going to affect our society?\nStudies came out from Stanford and MIT this year, looking at the potential of AI assistants to improve the productivity of office workers. Both came to the same conclusion – that the workers with the lowest ability and least experience were the ones who gained the most in productivity.\nIn other words, AI has made human knowledge and experience less valuable.\nResearchers at Microsoft and Open AI wrote something important on this phenomenon that I’d like to quote in full:\nLet’s talk about the fairness of this. Because the AI models didn’t invent medicine, accountancy or engineering. They didn’t learn anything directly from the world – human experts taught AI models how to do these things. And they [the human experts] did it without giving their permission, or even knowing that it was happening.\nThe large tech companies have sucked up all of human knowledge and culture and now provide access to it for the price of an API call. This is a huge transfer of power and value from humanity to the tech companies.\nBiologists in the 1990s found themselves in a very similar position. Celera Genomics was trying to achieve commercial control over the human genome. To stop this happening, the publicly funded Human Genome Project (HGP) resolved to sequence the human genome and release the data for free on a daily basis, before Celera could patent any of it.\nThe HGP was criticised because of ethical concerns (including concerns about eugenics), and because it was thought to be a huge waste of money. The media attacked it, claiming that a publicly funded initiative could not possibly compete with the commercial sector. Fortunately for humanity, a group of scientists with a vision worked together to make it a success.\nAnd it was a huge success: in purely economic terms it produced nearly $1 trillion in economic impacts for investment of about $4 billion. Apart from the economics, the Human Genome Project accelerated development of the genomic technologies that underlie things like mRNA vaccine technology.\nThe parallels to our current situation with AI are striking. With OpenAI, just like Celera, we have a commercial enterprise that launched with an open approach to data sharing but eventually changed to a more closed model.\nWe have commentators suggesting that a publicly funded project to create an open-source AI would be ethically dubious, a waste of money and beyond the competency of the public sector. Where the analogy breaks down is that unlike in the 1990s, we do not have any strong voices arguing on the other side, for openness and the creation of shared AI models for all humanity.\nPublic funding is needed for an “AI for humanity” project, modelled on the Human Genome Project. How else can we ensure the benefits of AI are spread widely across the global population and not concentrated in the hands of one or two all-powerful technology companies?\nWe’ll never know what the world would have looked like if we’d let Celera gain control over the human genome. Do we want to know a world where we let technology companies gain total control over artificial intelligence?"
  },
  {
    "objectID": "sp/posts/2023/10/20/ai-for-humanity.html#faq",
    "href": "sp/posts/2023/10/20/ai-for-humanity.html#faq",
    "title": "An AI for humanity",
    "section": "FAQ",
    "text": "FAQ\n\nHow about all the ethical considerations around AI – shouldn’t we consider this before releasing any open-source models?\nOf course. Obviously, there are ethical implications that need to be considered carefully, just as there were for the genome project. At the start of that project, the ethical, legal, and social issues (or ELSI) program was set up. The National Institutes of Health (NIH) devoted about 5% of their total Human Genome Project budgets to the ELSI program and it is now the largest bioethics program in the world. All important ethical issues were considered carefully and resolved without drama.\n\n\nAren’t there enough community efforts to build open-source AI models already?\nThere are good projects producing open-source large language models, like Llama 2 from Meta and Falcon from the TII in the United Arab Emirates. These are not quite as powerful as [Open AI’s] GPT-4 but they prove the concept that open-source models can approach the capabilities of the front-running commercial models; even when produced by a single well-funded lab (and a state-funded lab in the case of the TII). A coordinated international publicly funded project will be needed to surpass commercial models in performance.\nIn any case, do we want to be dependent on the whims of the famously civic-minded Mark Zuckerberg [CEO of Meta] for access to open-source AI models? We shouldn’t forget that the original Llama model was released with a restrictive licence that was eventually changed to something more open after a community outcry. We are lucky they made this decision. But the future of our societies needs to rely on more than luck.\n\n\nHow about the UK Government AI Safety Summit and AI Safety Institute – won’t they be doing similar work?\nAbsolutely not! The limit of the UK Government’s ambition seems to be to set the UK up as a sort of evaluation and testing station for AI models made in Silicon Valley. This is as far from the spirit of the Human Genome Project as it’s possible to be.\nSir John Sulston, the leader of the HGP in the UK, was a Nobel Prize-winning scientific hero who wanted to stop Celera Genomics from gaining monopolistic control over the human genome at all costs. The current UK ambition would be like reducing the Human Genome Project to merely testing Celera Genomics’ data for errors.\n\n\nHow will an international ‘AI for humanity’ project avoid the devaluation of human knowledge and experience, and consequent job losses?\nIt may not be possible to avoid this. But governments will at least be able to mitigate societal disruption if they can redistribute some of the wealth gained via AI (e.g., via universal basic income). They will not be able to do this if all of the wealth accrues to only one or two technology companies based in Silicon Valley.\n\n\nHow about existential risk?\n‘Existential risk’ is a science fiction smokescreen generated by large tech companies to distract from the real issues. I cannot think of a better response than the words of Prof Sandra Wachter at the University of Oxford: “Let’s focus on people’s jobs being replaced. These things are being completely sidelined by the Terminator scenario.”\n\n\n\n\n\n\nMartin Goodson will be speaking live at the Royal Statistical Society on October 31, 2023, as part of a panel discussion on “Evaluating artificial intelligence: How data science and statistics can make sense of AI models.” Register now for this free in-person debate. The event forms part of the AI Fringe programme of activities, which runs alongside the UK Government’s AI Safety Summit (1–2 November).\n\n\n\n\nDiscover more The Pulse\n\n\n\n\n\nAbout the author\n\nMartin Goodson is the former chair of the RSS Data Science and AI Section (2019–2022). He is the organiser of the London Machine Learning Meetup, the largest network of AI practitioners in Europe, with over 11,000 members. He is also the CEO of AI startup, Evolution AI.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Martin Goodson\n\n\nThumbnail image by Etienne Girardet on Unsplash."
  },
  {
    "objectID": "news/dpo7.html",
    "href": "news/dpo7.html",
    "title": "Tektronix Announces 7 Series DPO Oscilloscope, Setting New Performance Benchmark",
    "section": "",
    "text": "Powered by two new proprietary ASICs (Tek079 and Tek085), the oscilloscope provides the industry’s lowest noise and highest Effective Number of Bits (ENOB). The series debuts with a bandwidth of up to 25 GHz and offers data offload speeds up to 10 times faster than legacy instruments."
  },
  {
    "objectID": "news/dpo7.html#key-features-and-innovations",
    "href": "news/dpo7.html#key-features-and-innovations",
    "title": "Tektronix Announces 7 Series DPO Oscilloscope, Setting New Performance Benchmark",
    "section": "Key Features and Innovations",
    "text": "Key Features and Innovations\n\n\n\n\nSeries DPO Oscilloscope\n\n\n\nUnmatched Signal Fidelity: Delivers an industry-leading ENOB (7.5 bits at 8 GHz to 6.5 bits at 25 GHz) due to its ultra-low random noise.\n\nQuietChannel™ Technology: Uses active equalization to compensate for signal loss, reducing noise and improving measurement accuracy.\nBreakthrough Speed: Achieves up to 10x faster data transfer with 10G SFP+ LAN and the TekHSI™ library. ù Scalable Architecture: Built with an upgradeable architecture and compatibility with the TekConnect® probe interface to protect long-term investment.\n\nModern Workflow Integration: Features a 15.6” 1080p touchscreen, native serial analysis tools, and dual support for both Windows and Linux operating systems."
  },
  {
    "objectID": "news/dpo7.html#software-and-availability",
    "href": "news/dpo7.html#software-and-availability",
    "title": "Tektronix Announces 7 Series DPO Oscilloscope, Setting New Performance Benchmark",
    "section": "Software and Availability",
    "text": "Software and Availability\nThe 7 Series runs on TekScope® software, providing a consistent and intuitive user experience. The first model, the DPO714AX, is available for order now."
  },
  {
    "objectID": "companies/hp.html",
    "href": "companies/hp.html",
    "title": "Hewlett-Packard (HP)",
    "section": "",
    "text": "Hewlett-Packard (HP), founded by William (Bill) Hewlett and David (Dave) Packard, is a legendary name in the tech industry, renowned for its pioneering contributions to electronics, computing, and most notably, its foundational role in test and measurement instrumentation. From a modest garage startup, HP grew into a global powerhouse, consistently pushing the boundaries of what was technologically possible."
  },
  {
    "objectID": "companies/hp.html#the-genesis-a-garage-and-an-oscillator-1939",
    "href": "companies/hp.html#the-genesis-a-garage-and-an-oscillator-1939",
    "title": "Hewlett-Packard (HP)",
    "section": "The Genesis: A Garage and an Oscillator (1939)",
    "text": "The Genesis: A Garage and an Oscillator (1939)\nThe story of Hewlett-Packard began in 1939 in a Palo Alto garage, where Stanford University classmates Bill Hewlett and Dave Packard pooled their talents and $538 in capital. Their very first product, and a key to their initial success, was the HP 200A Audio Oscillator.\n\nThe HP 200A: This innovative, low-distortion audio oscillator quickly gained recognition for its stability and affordability. A significant early customer was Walt Disney Studios, which purchased eight HP 200B oscillators to test the Fantasound system for the movie Fantasia. This initial success cemented HP’s reputation for quality and innovation in electronic test equipment."
  },
  {
    "objectID": "companies/hp.html#a-culture-of-innovation-and-employee-focus",
    "href": "companies/hp.html#a-culture-of-innovation-and-employee-focus",
    "title": "Hewlett-Packard (HP)",
    "section": "A Culture of Innovation and Employee Focus",
    "text": "A Culture of Innovation and Employee Focus\nHewlett and Packard fostered a unique corporate culture that became known as “The HP Way.” This philosophy emphasized:\n\nEngineering Excellence: A deep commitment to research, development, and producing high-quality, reliable products.\nDecentralization: Empowering engineers and teams to innovate.\nEmployee Respect: Treating employees with trust and respect, offering benefits like profit-sharing and flexible hours, which were revolutionary at the time.\nCustomer Focus: Building strong relationships with customers and understanding their needs.\n\nThis culture attracted top talent and fueled decades of groundbreaking innovations, particularly in the test and measurement domain."
  },
  {
    "objectID": "companies/hp.html#dominance-in-test-measurement-1940s-1980s",
    "href": "companies/hp.html#dominance-in-test-measurement-1940s-1980s",
    "title": "Hewlett-Packard (HP)",
    "section": "Dominance in Test & Measurement (1940s-1980s)",
    "text": "Dominance in Test & Measurement (1940s-1980s)\nFor decades, HP was synonymous with high-precision test and measurement equipment. Their product lines were extensive and highly respected:\n\nOscilloscopes: While Tektronix specialized in high-performance oscilloscopes, HP offered a comprehensive range, often integrating them into broader measurement systems.\nFrequency Counters: HP’s frequency counters were industry benchmarks for accuracy and stability.\nSignal Generators: From audio to microwave frequencies, HP’s signal generators were essential tools for electronic design and testing.\nNetwork Analyzers: HP revolutionized RF and microwave testing with its sophisticated network analyzers, critical for developing communication technologies.\nLogic Analyzers: As digital electronics emerged, HP developed advanced logic analyzers to help engineers debug complex digital circuits.\n\nHP’s instruments were known for their robust design, intuitive interfaces, and superior performance, becoming indispensable in countless laboratories, R&D facilities, and production lines worldwide. They were instrumental in the development of radar, radio, television, and eventually the entire computer industry."
  },
  {
    "objectID": "companies/hp.html#diversification-and-the-split",
    "href": "companies/hp.html#diversification-and-the-split",
    "title": "Hewlett-Packard (HP)",
    "section": "Diversification and the Split",
    "text": "Diversification and the Split\nAs the company grew, HP diversified into computing, initially with calculators (like the groundbreaking HP-35 scientific calculator) and then minicomputers, followed by personal computers and printers. This diversification eventually led to the strategic decision to split the company.\nIn 1999, HP spun off its test and measurement, chemical analysis, and medical products businesses into a new company named Agilent Technologies. This move allowed Agilent to focus solely on the scientific and technological instrumentation markets, while the remaining Hewlett-Packard Company concentrated on PCs, printers, and enterprise computing."
  },
  {
    "objectID": "companies/hp.html#legacy",
    "href": "companies/hp.html#legacy",
    "title": "Hewlett-Packard (HP)",
    "section": "Legacy",
    "text": "Legacy\nWhile the HP name is now primarily associated with computing and printing, the legacy of its founders and its profound impact on test and measurement continues through Agilent Technologies. The principles of “The HP Way” — innovation, quality, and respect for employees and customers — laid a foundation that influenced the entire Silicon Valley ecosystem. Hewlett-Packard’s contribution to giving engineers the tools to measure, understand, and advance technology remains one of its most significant and enduring achievements."
  },
  {
    "objectID": "feeds.html",
    "href": "feeds.html",
    "title": "RSS feeds",
    "section": "",
    "text": "Latest content\nrealworlddatascience.net/latest-content.xml\n\n\nApplied Insights\nrealworlddatascience.net/applied-insights/index.xml\n\n\nCase studies\nrealworlddatascience.net/case-studies/index.xml\n\n\nFoundations & Frontiers\nrealworlddatascience.net/foundation-frontiers/index.xml\n\n\nPeople & Paths\nrealworlddatascience.net/people-paths/index.xml\n\n\nThe Pulse\nrealworlddatascience.net/the-pulse/index.xml"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  Unites States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#introduction",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#introduction",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "1 Introduction",
    "text": "1 Introduction\nHere, we demonstrate how the CDE Framework can be implemented for a research use case related to skilled nursing facilities. The framework provides the guiding principles for ethical, transparent, and reproducible research and dissemination and the research process for developing the statistical product.\nAcross the US, federally regulated skilled nursing facilities (SNFs) provide essential care, rehabilitation, and related health services to about 1.3 million people. An SNF is a facility that meets specific federal regulatory certification requirements that enable it to provide short-term inpatient care and services to patients who require medical, nursing, or rehabilitative services. Their patients can be among the most vulnerable members of our society, and yet, historically, SNFs have not been incorporated into existing emergency response systems. For example, during the 2004 Florida hurricane season, SNFs were given the same priority as day spas for restoring electricity, telephones, water, and other essential services (Hyer et al. 2006). Even worse are the deaths of SNF residents in Louisiana following Hurricanes Katrina and Rita in 2005 (Dosa et al. 2008). This was still an issue in 2021. In Louisiana, 15 SNF residents died when evacuated to a warehouse during Hurricane Ida (2021), and 12 died in Florida as a result of Hurricane Irma (2017). In both instances, the deaths were attributed to extreme heat and lack of electricity (Skarha et al. 2021).\nThese events prompted the (The White House 2022) initiative, Protecting Seniors by Improving Safety and Quality of Care in the Nation’s Nursing Homes, stating, ‘All people deserve to be treated with dignity and respect and to have access to quality medical care.’\nHowever, there are questions that need to be addressed to best protect SNFs and their residents. For example, how resilient are SNFs in extreme climate events? This use case demonstration shows how we built a new statistical product to address this question using the CDE Framework (Lancaster et al. 2023)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#purposes-and-uses",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#purposes-and-uses",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "2 Purposes and uses",
    "text": "2 Purposes and uses\nA skilled nursing facility (SNF) is a federally regulated nursing facility with the staff and equipment to provide skilled nursing care, skilled rehabilitation services, and other related health services (Medicare & Medicaid Services 2023). The context of this use case is to create a baseline picture of SNFs in Virginia and then integrate information on the risk of extreme flood events to assess facility and community preparedness – for example, how likely are the nursing staff1 to make it to the facility in the event of a flood?\nThis use case has two parts. The first creates a baseline data picture of SNFs, bringing together data about the residents, nursing staff, and SNF characteristics. The second addresses two issues raised in the (The White House 2022) initiative: emergency preparedness and nurse staffing. We frame these issues into three purpose and use questions with the ultimate goal of creating statistical products that address these questions:\n\nCan SNF workers get to work during an extreme flood event?\nAre SNFs prepared for a flood emergency?\nCan communities support SNFs during an emergency?"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#statistical-product-development-stages",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#statistical-product-development-stages",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "3 Statistical product development stages",
    "text": "3 Statistical product development stages\nSubject matter input and literature review\nThe subject matter experts consulted included nursing facility administrators, SNF resident advocates, demographers, and researchers. Our discussions and literature review informed us of the many federal policies governing SNFs regarding inspections and data reporting requirements (procedural data). In addition, we were told about non-public data sources on residents and SNF staff that were aggregated to the SNF level and provided to the public under a grant from the National Institute on Aging. This information was important since we had yet to come across this source in our data discovery process. The dialogue with experts and our literature review helped us generate a ‘wish list’ of variables we used to inform our data discovery process that we visualized into a conceptual data map (see Figure 1).\n\n\n\n\n\n\nFigure 1: Conceptual Data Map Aligned to Purpose and Use: The conceptual data map displays the results of our data discovery. The team identifies the data needs informed by expert elicitation and literature review. For this use case the data discovery took three phases: (1) create a data picture of SNF owners, nursing staff, and residents, and the communities the facilities reside in; (2) identify the potential risks of a severe flood events, coastal and riverine; and (3) identify the potential weakness in the SNF’s and community’s ability to respond.\n\n\n\nData discovery\nData discovery focused on identifying data sources to address the purpose and use questions and was informed by the conceptual data map.\nFor the first question – Can SNF workers get to work during an extreme flood event? – we discovered and used proprietary synthetic population, transportation routes, building data sources, and publicly available flood data. The HERE Premium Streets proprietary data includes information about roads, such as type of road, speed limits, number of lanes, etc. The proprietary synthetic population data, Building Knowledge Base (BKB), are used to identify where SNF workers live and work to map transportation routes from home to work (Mortveit, Xie, and Marathe 2023). Publicly available data from the Federal Emergency Management Administration (FEMA) provided flooding risk estimates along the routes from nursing staff homes to the SNF.\nFor the second question – Are SNFs prepared for a flood emergency? – we used Center for Medicare and Medicaid (CMS) SNF inspection and deficiency data as a proxy for preparedness. We also examined SNF residents’ physical and mental health to assess SNF emergency preparedness. For example, if most residents faced mobility challenges, the SNF would need more resources available during an emergency to move residents to a safer facility. We used data about residents from the Long Term Care Focus (LTCFocus 2022) Public Use Data sponsored by the National Institute on Aging (Brown University 2022).\nWe used data to measure community resilience, assets, and risks by geography at the county, city, and census tract levels to address the third question, Can communities support SNFs during an emergency? These data included:\n\nHealth professional shortages area (HRSA 2022)\nShelter facilities and emergency service providers data (Homeland Security: Geospatial Management Office 2022)\nCommunity Resilience Indicator Analysis and National Risk Index for Natural Hazards (FEMA 2022).\n\nAll data are provided in a GitHub repository along with their metadata, except for the three proprietary data sources. Articles about how the synthetic estimates are constructed are provided for two of these proprietary data sources. The third data source was obtained from a private-sector vendor whose data and documentation are proprietary; a link is provided to their website.\nData ingest and governance\nAll the public data, metadata, code, statistical products, data processes, and relevant literature on SNF policies and regulations are stored in a GitHub repository.\nIn our experience, data wrangling is the most time-consuming and challenging part of product development. This speaks directly to the benefit of the CDE; once a researcher has wrangled together multiple data sources, it can be made available to other researchers.\nThe two predominant issues with data wrangling for this Use Case included reconciling data sources that contain data on the same topic and creating linkages between data sources. For example, we reviewed three hospital data sources:\n\nHomeland Security Infrastructure Foundation-Level Data (HIFLD) (DHS 2022)\nHealthData.gov - COVID-19 Reported Patient Impact and Hospital Capacity by State (HHS 2022)\nMap of VHHA Hospital and Health System Members (Virginia Hospital & Healthcare Association 2022)\n\nWe observed inconsistences and omissions across the three data sources including: \n\nnon-standard hospital names and hospital classification types\ninconsistent availability of hospital IDs (such as Medicare Provider Number)  \nconflicting geographic information, including address, latitude, and longitude.\n\nWe did not attempt to reconcile these inconsistencies for the demonstration but decided to use a single source for shelter facility and emergency service provider data. We used HIFLD data since they provided the most current data (DHS 2022). The use of these data reinforces the purpose of the use case – to illuminate the challenges in creating statistical products and what the Census Bureau would need to consider.\nSimilar inconsistencies made it difficult to link data sources using geographic variables. For example, we used shelter facility and emergency service provider data sources from the HIFLD – including hospitals, Red Cross chapter facilities, National Shelter System Facilities, emergency medical service stations, fire stations, and urgent care facilities – to calculate a metric for potential community support. The goal was to place each facility in a Virginia county or independent city. Virginia is divided into 95 counties, and 38 independent cities considered county-equivalents for census purposes, and in some cases, there is a county and a city with the same name (eg, Richmond County and Richmond City, each in different locations in Virginia). It was necessary to canonicalize the county and city names (when available), which meant aligning upper and lower cases, removing unnecessary characters, and distinguishing between county and city.2\nThe challenge with locating shelter facilities and emergency service providers within a county or independent city was using different variables to identify their location (latitude and longitude, address, ZIP code3, Federal Information and Processing Standard (FIPS) code, and county/city name). In cases where the data source only had a ZIP or FIPS code, a Department of Housing and Urban Development crosswalk was used to link the two codes; in other cases, a crosswalk that linked non-independent cities and towns to counties was used; and in others, a crosswalk that linked FIP codes to counties and independent cities. Researchers would benefit from exhaustive crosswalks between all variables on the same topic, such as location variables, facility names, and identification numbers, to reduce the time spent on data wrangling.\nRegarding data products related to popular indices, such as climate disaster risks and community resilience, they are operationalized differently across the various departments and agencies within the federal and state governments and private and non-profit sectors. It is an enormous task to review the methodology and technology reports (if available) to understand their differences and decide which versions are most relevant (fitness-for-purpose) for a particular use case. Again, after reviewing the options for this use case, we determined that the National Risk Index for riverine and coastal floods from FEMA was the best option for climate risk estimates. The detailed technical report, National Risk Index Technical Document (FEMA 2021), provides a clear assessment of the assumptions and limitations of the data and a description of how the risk estimates were derived. Researchers would benefit from guidance on the numerous constructions of indices on the same topic. A use case on a specific index topic could be used to highlight differences and similarities among indices, which would help with data wrangling and fitness-for-use. Ideally, the use case could benchmark the various constructions and provide a statistical assessment.\n\n3.1 Question 1: Can SNF workers get to work during an extreme flooding event?\nSufficient nursing staff is of significant concern to assure resident safety and quality of care.\nSince proprietary synthetic population data and commercial sector digitized mapping data were used to construct the routes SNF nursing staff are likely to take from home to work, only an outline of the computational process used to identify the routes is provided. Publicly available data from FEMA were used to estimate flooding risk along a particular route. Below is a general description of the modeling steps and the proprietary data used to assess SNF vulnerability as a function of the nursing staff’s inability to report to work due to the transportation infrastructure (Choupani and Mamdoohi 2016).\nComputational modules\nHere is the basic outline of the process that uses proprietary data that starts at network construction and ends with routes. For more details, see the GitHub repository: Vulnerability of SNFs concerning Commuting.\n\nExtract network data from HERE (2021 Q1 in this use case).\nProcess the extracted data to form a network suitable for routing. This includes inference of speed limits for road links where such data is missing.\nPrepare origin-destination pairs. In this case, the list of locations pairs a worker’s home and work locations. The person is constructed in the synthetic population pipeline, and residences and workplaces are derived through the data fusion process used to construct the NSSAC building database.\nConstruct routes using the Quest router.\n\nOnce the routes to an SNF were established, the expected number of nursing staff at an SNF during a flood event could be calculated as the sum of the probabilities of each worker being able to commute to work during a flood event. A computational model was developed using the following data:\n\nSNF locations in Virginia from the Centers for Medicare & Medicaid Services (CMS);\nHome locations of workers at each SNF assigned from the synthetic population and Building Knowledge Base (Beckman, Baggerly, and McKay 1996; Mortveit, Xie, and Marathe 2023);\nVirginia road networks; and\nFEMA census tract-level riverine and coastal flood risks.\n\nUsing router software, the Virginia road network was used from the HERE map data to compute each nursing staff’s likely route to their SNF. Routers are commonly used within transportation and traffic simulators. The router software used for this demonstration is a highly parallelizable router previously developed in BI NSSAC, known as the Simba router (Barrett et al. 2013).\nThe FEMA risk data provide the riverine and coastal flood risks for each census tract in Virginia. Given the routes, the FEMA riverine and coastal flood risks were used to estimate the probability of the nursing staff making it to work. The FEMA technical document National Risk Index Technical Document (FEMA 2021) provides information on how natural hazard risks are calculated. We use these risk estimates ranging from 0 to 100 as a proxy for the probability a worker can reach the SNF by dividing by 100. For example, we assume a risk is zero if there is zero probability of being unable to reach the SNF due to an extreme flood event.\nIn contrast, a risk of 100 indicates the roads are underwater, and the probability of being unable to reach the SNF is one. The maximum risks along transportation routes leading to an SNF range from 0 to 47 for riverine flooding and 0 to 40 for coastal flooding. We assume the combined value of the maximum riverine and coastal flood risks along a worker’s transportation routes, divided by 100, is the worker’s probability of not getting to work during a flooding event.\nSince we do not have data on the exact home locations of the nursing staff, we estimated how many could reach the facility by taking a random sample (whose size is the CMS average daily nursing staff4 for an SNF) from the possible routes identified using the HERE Virginia road network. We calculated the average with a 95% nonparametric confidence interval. The 283 SNFs used in our research have an average daily nursing staff of 12,609. Using the above approach, we estimated that 10,005 (95% CI: 9,013, 10,700) or 79% can get work during an extreme flood event. The individual SNF nursing staff percentage who can make it to work ranges from 48% to 93%.\nFigure 2 visualizes this analysis for the 283 SNFs ordered by the observed average daily nursing staff numbers at the facility from smallest to largest, displayed using the orange line. The black line indicates the expected number in an extreme flood event and the 95% nonparametric confidence interval (grey band). The code for Figure 2 is provided in the GitHub repository.\n\n\n\n\n\n\nFigure 2: SNF Average Observed and Expected Average Daily Nursing Staff Numbers: The horizontal axis is ordered by the size of the nursing staff at the facility from smallest to largest. The orange line displays the observed average daily nursing staff numbers. The black line displays the estimated numbers in the event of an extreme coastal and/or riverine flood event. The grey band is the 95% nonparametric confidence interval.\n\n\n\nFor example, in King George County, the SNF is Heritage Hall King George (Federal Provider Number 495300 in Figure 3), located near the Potomac River, which opens to the Chesapeake Bay. According to CMS, the Heritage Hall King George facility has an average daily skilled nursing staff of 41. Using the HERE Virginia road network, we identified 101 routes the staff could use to reach the facility. The combined maximum coastal and riverine flood risks along these routes ranged from 5.6 to 66.7; a random sample of 41 from the 101 routes gives an average probability of reaching the facility of 0.74 with a 95% nonparametric confidence interval of [0.65, 0.80]. These were used to estimate the average number of nursing staff at the facility, 30, during a flood event, along with a 95% nonparametric confidence interval [14, 38]. Publicly available data from the Federal Emergency Management Administration (FEMA) provided flooding risk estimates along the routes from the nursing staff home to the SNF along with proprietary road and building information.\n\n\n\n\n\n\nFigure 3: An Example of Nursing Staff Routes to Heritage Hall King George SNF: Routes that workers can take to work at Heritage Hall  King George SNF FPN 495300 (identified with the black oval). The risk levels of each road are identified with colors, from low risk (blue), medium-low (yellow), orange (medium), red (medium-high), to high risk (dark red). The risk scores are used to calculate the probability of a worker getting to work during an extreme flood event using publicly available FEMA data and proprietary road and building data.\n\n\n\n\n\n3.2 Question 2. Are SNFs prepared for emergencies?\nTo address this question, we examined how prepared SNFs are for emergencies using annual inspection and deficiency data as a proxy for preparedness. CMS issues deficiencies to SNFs that fail to meet federal Medicare and Medicaid preparedness standards. Every deficiency is classified into one of 12 categories based on the scope and severity of the deficiency. There are two broad types of non-health-related deficiencies:\n\nEmergency Preparedness Deficiencies – There are four elements of emergency preparedness. They cover an emergency plan, policies and procedures, a communication plan, and training and testing.\nFire Life Safety Code – The set of fire protection requirements are designed to provide a reasonable degree of safety from fire. They cover construction, protection, and operational features designed to provide safety from fire, smoke, and panic.\n\nWe calculated separate Emergency Preparedness and Fire Life Safety Code deficiency indices to combine them to create a single index to measure SNF preparedness and distinguish between high and low performing SNFs. The computation of the indices has four steps.\n\nNumber of deficiencies: For each SNF, the total number of deficiencies during the past four years, 2018-2022, was divided by the number of SNF inspections over the same period to estimate the average number of deficiencies per inspection.\nTime to resolve deficiencies: We next computed the average number of days it took to resolve each deficiency.\nScope and severity of deficiencies: We then transformed the deficiency letter inspection rating for scope and severity to a numerical weight using the CMS technical guide, Care Compare Nursing Home Five-Star Quality Rating System (Medicare & Medicaid Services 2022),and averaged the ratings.\nThe estimates from these three steps were summed to compute separate Emergency Preparedness and Fire Life Safety Code deficiency indices (see Figure 4) and are provided for reuse in a .csv file on GitHub.\n\nFigure 4 displays the results of an exploratory data analysis for each index. These analyses assessed fitness-for-use; we wanted to construct an indicator with sufficient variability to discriminate between high and low-performing SNFs. It is evident we accomplished this in Figure 4 there are SNFs with indices outside the main body of the data. We summed the Emergency Preparedness and Fire Life Safety Code indices and categorized them into high, medium, low, and no deficiencies.\n\n\n\n\n\n\nFigure 4: Exploratory Data Analysis Visualizations for the Emergency Preparedness and Fire Life Safety Code Deficiencies\n\n\n\n\n\n3.3 Question 3: Can communities support SNFs during emergencies?\nTo answer this question, we computed a community resiliency index using the US Census American Community Survey and the guidance provided by the Homeland Security document Community Resilience Indicator Analysis: County-Level Analysis of Commonly Used Indicators from Peer-Reviewed Research (Edgemon et al. 2018). The index was constructed by summing the county (census tract) level percentages for the following variables:\n\nfraction employed\nfraction with no disability\nfraction with a high school diploma or greater\nfraction of households with at least one vehicle\nreverse GINI Index – so all indicators are in a positive direction.\n\nFigure 5 displays the combined deficiency indices, Emergency Preparedness + Fire Life Safety Code, for each SNF with the choropleth map for the community resilience index at the census tract level. We also examined the number of shelter facilities and emergency service providers and the availability of medical staff per 10,000 residents. We constructed isochrones to establish the distance from the SNF to these potential sources of support. Working on this component of the use case highlighted the need for cross-agency data, pointing to the utility of future strategic partnering between the US Census Bureau, CMS, and FEMA.\n\n\n\n\n\n\nFigure 5: 2020 Population Resilience Composite Index for Virginia Census Tracts: The light yellow tracts are the least resilient, and the dark green are the most resilient. The locations of the 283 SNFs are identified with filled circles, orange circles with the highest\n\n\n\nIn addition to describing the population using a resilience index, we also developed a measure to present the number of shelter facilities and emergency service providers (data from Homeland Security / Homeland Infrastructure Foundation Level Data) and the availability of medical doctors (MDs) and Doctor of Osteopathic Medicine (ODs) who provide direct patient care (HRSA 2022) (Figure 6). \nThe number of MDs and ODs is described as a primary care health professional shortage area. HRSA defines these contiguous areas where primary medical care professionals are overutilized, excessively distant, or inaccessible to the population of the area under consideration. Figure 6 (bottom) shows that approximately one-third of the counties and independent cities have health professional shortage areas across their entire boundary, and another 40 percent have shortages within parts of their boundaries.\n\n\n\n\n\n\nFigure 6: Assessment of the number of shelter facilities and emergency service providers per 10,000 population (top) and medically underserved areas (bottom): On both maps, the lighter the color, the more in need is the population of shelter facilities and emergency services (top chart) or health professionals (bottom chart). The location of the 283 SNFs are identified with filled circles, orange circles are those with the highest deficiency index and grey circles are those with no deficiencies."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#guiding-principles-for-ethical-transparent-reproducible-statistical-product-development-and-dissemination.",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#guiding-principles-for-ethical-transparent-reproducible-statistical-product-development-and-dissemination.",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "4 Guiding principles for ethical, transparent, reproducible statistical product development and dissemination.",
    "text": "4 Guiding principles for ethical, transparent, reproducible statistical product development and dissemination.\nCommunication\nWe communicated results throughout the Demonstration Use Case research with our Census CDE Working Group (composed of former Census Bureau Directors and Communication Director, and academic and industry census experts), with the Census Bureau, at conferences such as the annual Federal Statistical Committee on Methodology, and sharing drafts to seek input and ideas. The discussions and presentations helped to shape ideas and advance our thinking about how best to address the purpose and use questions.\nStakeholder engagement\nWe engaged stakeholders by sharing our research and results through conference presentations at the American Community Survey Data Users Conference and the Applied Public Data Users Conference. We also shared this demonstration project at Listening Sessions with stakeholders as an example of statistical product development. The Listening Sessions bring together 7 to 12 stakeholders by topic (e.g., children’s health) or function (e.g., state demographers) to seek their ideas for new statistical products.\nEquity and ethics\nAs described in the Introduction, there are ethics and equity issues that drew us to develop this Use Case. Here we focus on equity and ethics vis-a-vis the data choices and analyses. With regard to ethical considerations with our data discovery process, fitness-for-purpose evaluation, and analyses, two questions arose:\n\nWhat role does synthetic data have to play, and how do you benchmark it to evaluate fitness-for-purpose?\nHow do you construct and evaluate an index with the goal of identifying vulnerable populations?\n\nRealizing the importance of nursing staff levels, we discussed and questioned whether the synthetic data had biases and were not representative of SNF residents and employees. We benchmarked the synthetic SNF nursing staff numbers against those submitted quarterly to CMS and observed they were biased low, so we decided to use the CMS data. These data were used to estimate the average number of nursing staff that could reach the facility during an extreme flood event (Figure 2).\nIn this use case, we were fortunate to have the “truth” to benchmark the synthetic data for the average daily nursing staff at each SNF. But this was not the case for the home locations of the nursing staff, therefore, the synthetic locations were not used since we had no way to benchmark them. Ideally, we would use the actual addresses of SNF employees. Instead, we used a simulation to estimate the average risks over routes leading to the SNF. This approach could be replaced with (or benchmarked against) the Census commuting data sets (eg, Commuting Flows or the LEHD Origin-Destination Employment Statistics) and the home census tract used as the starting point for each worker. For the number of nursing staff and their home locations, it is impossible to identify potential biases that would result in the inequitable allocation of emergency rescue resources without a thorough understanding of how the synthetic data were generated.\nHow one evaluates the equity of an index is a more challenging task. Questions that need to be addressed include:\n\nHow do you select the variables used to construct an indicator to guide an equitable allocation of technical assistance?\nWhat relationship between these variables is important?\nWhat are the differences across the numerous publicly available resilience estimators? Do some lead to a more equitable allocation of technical assistance in the event of an extreme clime event?\nHow do you validate a resilience estimator?\n\nThe technical document Community Resilience Indicator Analysis: County-Level Analysis of Commonly Used Indicators from Peer-Reviewed Research (Edgemon et al. 2018) identified the 20 most commonly selected variables for constructing resilience estimators from peer-reviewed research. Future research will need to validate these indices against past extreme climate events.\nPrivacy and confidentiality\nWe did not do a full disclosure review. However, some data are proprietary, and we could not release those data. We discuss how we used these data.\nDissemination\nWe disseminated the final version of the use case in the University of Virginia Libra Open repository (Lancaster et al. 2023).\nCuration\nCuration involves documenting all steps of the process so that they can be repeated, validated, reused, or extended. The final report explains the process in words. Curation must also provide the data, metadata, source code, and products. This led us to construct a GitHub repository. A README file guides the reader through the material and provides instructions for replicating the research results. Note that the README file must be downloaded for the hyperlinks to work."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#using-the-snf-statistical-product",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#using-the-snf-statistical-product",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "5 Using the SNF statistical product",
    "text": "5 Using the SNF statistical product\nThis potential statistical product has many uses. Federal policymakers and administrators regulate SNFs; however, they only sometimes realize the impacts on costs and the need for increased resources to meet these regulations. For example, by reviewing the aggregate inspection deficiency metrics, policymakers can target resources where they are most needed. Providing additional funding to pay workers more, improve their facilities, and address inspection deficiencies would improve the quality of SNFs. \nThe media and advocacy groups play a role in highlighting good and bad cases of SNF care or where communities do not have adequate assets to support SNFs during an emergency event. For example, a New Yorker article (Rafiei 2022) highlighted how nursing homes decline dramatically when bought by private equity owners. The GAO (September 22, 2023) recently identified the need for more information about private equity ownership in CMS data – a gap that CMS needs to address. And, of course, researchers and analysts are essential for conducting research that leads to creating and improving statistical products around SNFs. By releasing a regularly scheduled SNF statistical product, the changes in SNFs over time can be monitored."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#what-cde-capabilities-have-this-use-case-demonstrated",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#what-cde-capabilities-have-this-use-case-demonstrated",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "6 What CDE capabilities have this use case demonstrated?",
    "text": "6 What CDE capabilities have this use case demonstrated?\nAs demonstrated by this use case, the CDE Framework is a powerful process for guiding and curating the development of statistics to address complex purposes and uses. Additionally, use cases help illuminate technical capabilities that should be present in the data enterprise to facilitate and accelerate the reuse of data and methods in the development and dissemination of new statistical products.\nThis CDE demonstration is the first of many use cases needed to define and develop CDE capabilities. Underlying each use case is the curation process. Curation documents each step, including decisions that may involve trade-offs. Curation preserves and adds value to the data. This includes organizing to facilitate data discovery and easy access; providing metadata to enable the reuse in scientific and programmatic research; enhancing the value of the data enterprise through linkages between datasets; and mapping the network of interconnections between datasets, research outputs, researchers, and institutions. Over time, a searchable curation system will be needed as a foundation for creating statistical products in the CDE.\nThe types of products from a use case that can benefit the larger community are only limited by the creativity of the researchers and stakeholders carrying out the use case. The products from this use case are re-useable code; integrated data sets across diverse topics for each SNF; maps and other visualizations; statistical products such as SNF deficiency indices and various indices that measure community and SNF resilience; the probability of a worker reaching an SNF in the event of extreme flooding; and a GitHub repo that provides easy access to all these products plus relevant metadata, literature, and government documents and regulations.\nConducting this use case has been an eye-opening experience as to the amount and quality of publicly available data to address our research questions. The statistical capabilities and products flowing from diverse use cases can only be identified as the program progresses.\n\n\n\n\n← Part 2: What is the CDE?\n\n\n\n\nPart 4: Census Curated Data Enterprise Environment →\n\n\n\n\n\n\n\n\nAbout the authors\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nAaron Schroeder has experience in the technologies and related policies of information and data integration and systems analysis, including policy and program development and implementation.\n\n\nHenning Mortveit develops massively interacting systems and the mathematics supporting rigorous analysis and understanding of their stability and resiliency.\n\n\nSamarth Swarup conducts research in computational social science, resiliency and sustainability, and stimulation analytics.\n\n\nDawen Xie develops geographic information systems, visual analytics, information management systems, and databases, with a current focus on building dynamic web systems.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Ground Picture on Shutterstock.\n\n\n\nHow to cite\n\nLancaster V, Shipp S, Keller S et al. (2024). “Translating the Curated Data Model into Practice - climate resiliency of skilled nursing facilities” Real World Data Science, November 19, 2024. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#footnotes",
    "href": "instruments_old/case-studies/posts/2024/11/19/use-case-2.html#footnotes",
    "title": "Translating the Curated Data Model into Practice - Climate resiliency of skilled nursing facilities",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNursing staff includes medical aides and technicians, certified nursing assistants, licensed practical nurses (LPNs), LPNs with administrative duties, registered nurses (RNs), RNs with administrative duties, and the RN director of nursing.↩︎\nFor example, distinguishing county from city when the name is the same could be done using State/County FIPS codes. Richmond County is 51159; Richmond City is 51760.↩︎\nZIP code is a system of postal codes used by the United States Postal Service. ZIP was chosen to indicate mail travels more quickly when senders use the postal code.↩︎\nAverage Daily Nursing Staff is the daily number of Medical Aides and Technicians, CNAs, LPNs, LPNs with administrative duties, RNs, RNs with administrative duties, and RN Director of Nursing averaged over three months.↩︎"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html",
    "href": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "",
    "text": "Acknowledgments: This research was sponsored by the:  United States Census Bureau Agreement No. 01-21-MOU-06 and  Alfred P. Sloan Foundation Grant No. G-2022-19536\nThe views expressed in this artice are those of the authors and not the Census Bureau."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html#introduction",
    "href": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html#introduction",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "Introduction",
    "text": "Introduction\nTwo centuries ago, when the Framers of the US Constitution laid the cornerstone for the federal statistical system, they could not have imagined the complexity of questions future generations would want to ask or the variety of data sources available to address them. Back in 1787, counting the population and apportioning state seats in the House of Representatives were the most urgent tasks before the young nation, and so a requirement for a decennial census was written into the Constitution. Now, 233 years later, the census continues to serve its original purpose – but purposes and uses for census data have exploded.\nQuestions we now seek to answer go beyond what the census (or surveys) alone can hope to address. Even with the multitude of other surveys commissioned by today’s US Census Bureau, researchers and policymakers find themselves looking to novel sources of data – from structured numeric data in traditional databases to unstructured text documents scraped from the internet – to explore issues such as understanding how prepared nursing homes and communities are for extreme climate events,eg, hurricanes, wildfires, or floods. Wrangling these sources with traditionally designed data, such as censuses and surveys, can fill data gaps, improve the quality and usefulness of statistical products, speed up their dissemination, and inspire the creation of new types of statistical products.\nThat is the impetus for developing the Curated Data Enterprise (CDE), an innovation in data science aimed at creating statistical products from all data types and building the infrastructure to support them. The Curated Data Enterprise, as the name implies, includes an end-to-end curation model to capture the complete statistical product development process. The CDE is designed to enable data discovery and retrieval, data quality assessment across multiple and diverse sources of information, and the reuse of data and models over time to accelerate statistical product development. The US Census Bureau has partnered with the University of Virginia, a working group of former Census Bureau Directors, a Communication Director, and university, non-profit and industry experts to develop this approach.\n\n\n\n\n\n\nThe US Census Bureau\n\n\n\nThe US Census Bureau provides the latest official statistics, facts, and figures about America’s people, places, and economy. It collects data for 130 surveys annually and the decennial census that gives the Bureau its name. The US Census Bureau collects data from households, businesses, governments and non-profit organizations. For each survey, tabulations and margins of error are published in news releases and reports. Public-use microdata subject to disclosure rules are provided for household and demographic surveys. Microdata for economic and household surveys, without disclosure rules applied, are accessible to researchers through the Federal Statistical Research Data Centers.\nStatistical agencies in other countries are also modernizing their surveys and statistical product development. See a summary of selected countries (Lanman, Davis, and Shipp 2023)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html#a-new-approach",
    "href": "instruments_old/case-studies/posts/2024/11/01/policy-problem.html#a-new-approach",
    "title": "Advancing Data Science in Official Statistics – The Policy Problem",
    "section": "A new approach",
    "text": "A new approach\nTo realize the CDE vision, the development of statistical products will address stakeholder questions using all data types – designed surveys and censuses, public and private administrative data, opportunity data scraped from the internet and procedural data (Keller et al. 2022). This new approach aligns with the US Census Bureau’s modernization and transformation (Thieme 2022) while maintaining the fundamental responsibilities of statistical agencies (OMB 2023). It is also consistent with a conclusion by the NASEM Panel on the Implications of Using Multiple Data Sources for Major Survey Programs: ‘The quality of statistics produced from multiple data sources depends on properties of the individual sources as well as the methods used to combine them. A new framework of quality standards and guidelines is needed to evaluate such data sources’ fitness for use’ (NASEM 2023, 192).\nThe CDE approach provides such a framework to address many of the challenges that official statistics face today, as well as demonstrate that they are poised to adopt a new approach to producing official statistics. For example:\n\nThe timeliness and frequency of our official statistics are insufficient when there are shocks to the economy, such as the Covid-19 pandemic, when retrospective survey data were of limited usefulness. Federal agencies responded during the pandemic with relevance and agility by creating and launching fast-response Household Pulse Surveys that met immediate needs for data, trading off timeliness for quality (Groshen 2021). Public engagement and support for these new relevant and timely data products at a time of crisis were essential to the success of this new statistical product.\nThe policy environment has responded to technological, social, and survey changes by encouraging efficient use of existing data, reuse, sharing and furthering open data principles. Researchers are now creating innovative statistical products using multiple data sources to better address the US’s needs and interests. The Commission on Evidence-Based Policymaking (Abraham et al. 2018) and the Federal Data Strategy (“Federal Data Strategy, Leveraging Data as a Strategic Asset” 2021) recommendations encourage agencies to permit access to data to undertake evaluation and research studies.\nTechniques such as rapid scanning, text recognition, user-friendly uploads, and new devices, sensors, and systems can now record and transcribe data in real time. Using these techniques, governments and corporations now routinely and instantaneously collect and store data on behaviors and states as varied as purchase transactions, climate and road conditions, healthcare plan utilization, and land use and zoning. Extensive digitization and recording, better system connectedness and interactivity, and increased human-computer interaction can result in faster data accumulation, enhancing the usability of private and public administrative data while maintaining privacy and confidentiality (Brady 2019; Jarmin 2019).  \nNew techniques and data sources can transform statistical agencies ‘from the 20th-century survey-centric model to a 21st-century model that blends structured survey data with administrative and unstructured alternative digital data sources’, leading to better measures of the gig economy, retail sales, healthcare, workforce, and tools and methods to integrate multiple data sources while maintaining privacy and confidentiality (Jarmin 2019).\n\nThe next three articles in this series will:\n\nprovide an overview of the CDE and its corresponding framework\nput the CDE Framework into practice through a demonstration use case on the resilience of skilled nursing facilities\ndescribe our next steps for developing the CDE through a use case research program.\n\n\n\n\n\nPart 2: What is the Curated Data Enterprise? →\n\n\n\n\n\n\n\n\nAbout the authors\n\nSallie Keller is the Chief Scientist and Associate Director of Research and Methodology at the US Census Bureau. She is a statistician with research interest in social and decision informatics, statistics underpinnings of data science, and data access and confidentiality. Sallie Keller was at the University of Virginia when this work was conducted.\n\n\nStephanie Shipp leads the Curated Data Enterprise research portfolio and collaborates with the US Census. She is an economist with experience in data science, survey statistics, public policy, innovation, ethics, and evaluation.\n\n\nVicki Lancaster is a statistician with expertise in experimental design, linear models, computation, visualizations, data analysis, and interpretation. She works with scientists at federal agencies on projects requiring statistical skills and creativity, eg, defining skilled technical workforce using novel data sources.\n\n\nJoseph Salvo is a demographer with experience in US Census Bureau statistics and data. He makes presentations on demographic subjects to a wide range of groups about managing major demographic projects involving the analysis of large data sets for local applications.\n\n\n\n\n\nCopyright and licence\n\n© 2024 Stephanie Shipp\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Lukas Blazek on Unsplash.\n\n\n\nHow to cite\n\nKeller S, Shipp S, Lancaster V, Salvo J (2024). “Advancing Data Science in Official Statistics: The Policy Problem.” Real World Data Science, November 01, 2024. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "",
    "text": "When Agentic AI started dominating research papers, demos, and conference talks, I was curious but cautious. The idea of intelligent agents, autonomous systems powered by large language models that can plan, reason, and take actions using tools, sounded brilliant in theory. But I wanted to know what happened when you used them. Not in a toy notebook or a slick demo, but in real projects, with real constraints, where things needed to work reliably and repeatably.\nIn my role as Clinical AI & Data Scientist at Bayezian Limited, I work at the intersection of data science, statistical modelling, and clinical AI governance, with a strong emphasis on regulatory-aligned standards such as CDISC. I have been directly involved in deploying agentic systems into environments where trust and reproducibility are not optional. These include real-time protocol compliance, CDISC mapping, and regulatory workflows. We gave agents real jobs. We let them loose on messy documents. And then we watched them work, fail, learn, and (sometimes) recover.\nThis article is not a critique of Agentic AI as a concept. I believe Agentic AI has potential value, but I also believe it demands more critical evaluation. That means assessing these systems in conditions that mirror the real world, not in benchmark papers filled with sanitised datasets. It means observing what happens when agents are under pressure, when they face ambiguity, and when their outputs have real consequences. What follows is not speculation about what Agentic AI might become a decade from now. It is a candid reflection on what it feels like to use these systems today. It is about watching a chain of prompts unravel or a multi-agent system drop the baton halfway through a task. If we want Agentic AI to be trustworthy, robust, and practical, then our standards for evaluating it must be shaped by lived experience rather than theoretical ideals."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#we-built-agentic-systems.-heres-what-broke.",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#we-built-agentic-systems.-heres-what-broke.",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "",
    "text": "When Agentic AI started dominating research papers, demos, and conference talks, I was curious but cautious. The idea of intelligent agents, autonomous systems powered by large language models that can plan, reason, and take actions using tools, sounded brilliant in theory. But I wanted to know what happened when you used them. Not in a toy notebook or a slick demo, but in real projects, with real constraints, where things needed to work reliably and repeatably.\nIn my role as Clinical AI & Data Scientist at Bayezian Limited, I work at the intersection of data science, statistical modelling, and clinical AI governance, with a strong emphasis on regulatory-aligned standards such as CDISC. I have been directly involved in deploying agentic systems into environments where trust and reproducibility are not optional. These include real-time protocol compliance, CDISC mapping, and regulatory workflows. We gave agents real jobs. We let them loose on messy documents. And then we watched them work, fail, learn, and (sometimes) recover.\nThis article is not a critique of Agentic AI as a concept. I believe Agentic AI has potential value, but I also believe it demands more critical evaluation. That means assessing these systems in conditions that mirror the real world, not in benchmark papers filled with sanitised datasets. It means observing what happens when agents are under pressure, when they face ambiguity, and when their outputs have real consequences. What follows is not speculation about what Agentic AI might become a decade from now. It is a candid reflection on what it feels like to use these systems today. It is about watching a chain of prompts unravel or a multi-agent system drop the baton halfway through a task. If we want Agentic AI to be trustworthy, robust, and practical, then our standards for evaluating it must be shaped by lived experience rather than theoretical ideals."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#what-agentic-ai-looks-like-in-practice",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#what-agentic-ai-looks-like-in-practice",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "2 What Agentic AI Looks Like in Practice",
    "text": "2 What Agentic AI Looks Like in Practice\nIf you’re imagining robots in lab coats, that’s not quite what this is. It is more like releasing a highly motivated intern into a complex archive with partial instructions, limited supervision, and the freedom to decide which filing cabinets, databases, or tools to open next. It is messy. It is unpredictable. And it sometimes surprises you with just how resourceful or confused it can get. Agentic AI systems are purpose-built setups where a large language model is given a task and enough autonomy to decide how to approach it. That might mean choosing which tools to use, when to use them, and how to adapt when things go off-script. You are not just sending one prompt and getting an answer. You are watching a system reason, remember, call APIs, retry when things go wrong, and ideally, get to a useful result.\nAt Bayezian, we have explored this in several internal projects, including generating clinical codes from statistical analysis plans and study specifications, monitoring synthetic Electronic Health Records (EHRs) for rule violations, and running chained reasoning loops to validate document alignment. These efforts reflect the reality of building LLM agents into safety-critical and compliance-heavy workflows. Across these deployments, the question is never just “can it do the task” but “can it do the task reliably, interpretably, and safely in context”.\nBroader research has followed similar directions. In clinical pharmacology and translational sciences, researchers have explored how AI agents can automate modelling and trial design while keeping a human in the loop, and offering blueprints for scalable, compliant agentic workflows link. In the context of patient-facing systems, agentic retrieval-augmented generation has improved the quality and safety of educational materials, with LLMs acting as both generators and validators of content link. Other teams have used multi-agent systems to simulate cross-disciplinary collaboration, where each AI agent brings a different scientific role to design and validate therapeutic molecules like SARS-CoV-2 nanobodies link.\nSome of the systems we built used agent frameworks like LangChain or LlamaIndex. Others were bespoke combinations of APIs, function libraries, memory stores, and prompt stacks wired together to mimic workflow behavior. Regardless of the architecture, the core structure remained the same. The agent was given a task, a bit of autonomy, and access to tools, and then left to figure things out. Sometimes it worked. Sometimes it did not. That gap between intention and execution is where most of the interesting lessons sit.\nIn the next section, I describe one of those deployments in more detail: a multi-agent system used to monitor data flow in a simulated clinical trial setting."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#case-study-monitoring-protocol-deviations-with-agentic-ai",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#case-study-monitoring-protocol-deviations-with-agentic-ai",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "3 Case Study: Monitoring Protocol Deviations with Agentic AI",
    "text": "3 Case Study: Monitoring Protocol Deviations with Agentic AI\nWhy We Built It\nClinical trials generate a stream of complex data, from scheduled lab results to adverse event logs. Hidden in that stream are subtle signs that something may be off: a visit occurred too late, a test was skipped, or a dose changed when it shouldn’t have. These are protocol deviations, and catching them quickly matters. They can affect safety, skew outcomes, and trigger regulatory scrutiny.\nTraditionally, reviewing these events is a painstaking task. Study teams trawl through spreadsheets and timelines, cross-referencing against lengthy protocol documents. It is time-consuming, easy to miss context, and prone to delay. We wondered whether an AI-driven approach could act like a vigilant reviewer. Not to replace the team, but to help it focus on what truly needed attention.\nOur motivation was twofold. First, to introduce earlier, more consistent detection without relying on rule-based systems that often buckle under real-world variability. Second, to test whether a group of coordinated language model agents, each with a clear focus, could carry out this work at scale while still being interpretable and auditable.\nTo do that, we built the system from the ground up. We designed a pipeline that could ingest clinical documents, extract key protocol elements, embed them for semantic search, and store them in structured form. That created the foundation for agents to work not just as readers of data, but as context-aware monitors. Understanding whether a missed Electrocardiogram (ECG) or a delayed Day 7 visit violated the protocol required more than lookup tables. It required reasoning. It required memory. It required agents built with intent.\nWhat emerged was a system designed not just to scan data, but to think with constraints, assess context, and escalate issues when the boundaries of the trial were breached. The goal was not perfection, but partnership. A system that could flag what mattered, explain why, and stay open to human feedback.\nHow It Was Set Up\nThe system was built around a group of focused agents, each responsible for checking a specific type of protocol rule. Rather than relying on one large model to do everything, we broke the task into smaller parts. One agent reviewed visit timing. Another checked medication use. Others handled inclusion criteria, missed procedures, or serious adverse events. This made each agent easier to understand, easier to test, and less likely to be overwhelmed by conflicting information.\nBefore any agents could be activated, however, an early classifier was introduced to determine what type of document had arrived. Was it a screening form or a post-randomisation visit report? That initial decision shaped the downstream path. If it was a screening file, the system activated the inclusion and exclusion criteria checker. If it was a visit document, it was handed off to agents responsible for tracking timing, treatment exposure, scheduled procedures, and adverse events.\nThese agents did not operate in isolation. They worked on top of a pipeline that handled the messy reality of clinical data. Documents in different formats were extracted, cleaned, and converted into structured representations. Tables and free text were processed together. Key elements from study protocols were embedded and stored to allow flexible retrieval later. This gave the agents access to a searchable memory of what the trial actually required.\nWhile many agentic systems today rely heavily on frameworks like LangChain or LlamaIndex, our system was built from the ground up to suit the demands of clinical oversight and regulatory traceability. We avoided packaged orchestration frameworks. Instead, we constructed a lightweight pipeline using well-tested Python tools, giving us more control over transparency and integration. For semantic memory and search, protocol content was indexed using FAISS, a vector store optimised for fast similarity-based retrieval. This allowed each agent to fetch relevant rules dynamically and reason through them with appropriate context.\nWhen patient data flowed in, the classifier directed the document to the appropriate agents. If any agent spotted something unusual, it could escalate the case to a second agent responsible for suggesting possible actions. That might mean logging the issue, generating a report, or prompting a review from the study team. Throughout, a human remained involved to validate decisions and interpret edge cases that needed nuance.\nWe did not assume the agents would get everything right. The idea was to create a process where AI could handle the repetitive scanning and flagging, leaving people to focus on the work that demanded clinical judgement. The combination of structured memory, clear responsibilities, document classification, and human oversight formed the backbone of the system.\nFigure 1 illustrates a two-phase agentic system architecture, where protocol documents are first parsed, structured, and embedded into a searchable memory (green), enabling real-time agents (orange) to classify incoming clinical data from the Clinical Trial Management System (CTMS), reason over protocol rules, detect deviations, and escalate issues with human oversight.\n\n\n\n\n\n\nFigure 1: System Architecture and Agent Flow\n\n\n\nWhere It Got Complicated\nIn early tests, the system did what it was built to do. It scanned incoming records, spotted missing data, flagged unexpected medication use, and pointed out deviations that might otherwise have slipped through. On structured examples, it handled the checks with speed and consistency.\nBut as we moved closer to real trial conditions, the gaps started to show. The agents were trained to recognise rules, but real-world data rarely plays by the book. Information arrived out of order. Visit dates overlapped. Exceptions buried in footnotes became critical. Suddenly, a task that looked simple in isolation became tangled in edge cases.\nOne of the most frequent problems was handover failure. A deviation might be correctly identified by the first agent, only to be lost or misunderstood by the next. A flagged issue would travel halfway through the chain and then disappear or be misclassified because the follow-up agent missed a piece of context. These were not coding errors. They were coordination breakdowns, small lapses in memory between steps that led to big differences in outcome.\nWe also found that decisions based on time windows were especially fragile. An agent could recognise that a visit was missing, but not always remember whether the protocol allowed a buffer. That kind of reasoning depended on holding specific details in working memory. Without it, the agents began to misfire, sometimes raising the alarm too early, other times not at all.\nNone of this was surprising. We had built the system to learn from its own limitations. But seeing those moments play out across agents, in ways that were subtle and sometimes difficult to trace, helped surface the exact places where autonomy met ambiguity and where structure gave way to noise.\nA Glimpse Into the Details\nOne case brought the system’s limits into focus. A monitoring agent flagged a protocol deviation for a missing lab test on Day 14. On the surface, it looked like a valid call. The entry for that day was missing, and the protocol required a test at that visit. The alert was logged, and the case moved on to the next agent in the chain.\nBut there was a catch.\nThe protocol did call for a Day 14 lab, but it also allowed a two-day window either side. That detail had been extracted earlier and embedded in the system’s memory. However, at the moment of evaluation, that context was not carried through. The agent saw an empty cell for Day 14 and treated it as a breach. It did not recall that a test on Day 13, which had already been recorded, fulfilled the requirement.\nThis was not a failure of logic. It was a failure of coordination. The information the agent needed was available, but not in the right place at the right time. The memory had thinned just enough between steps to turn a routine variation into a false positive.\nFrom a human perspective, the decision would have been easy. A reviewer would glance at the timeline, check the visit window, and move on. But for the agent, the absence of a test on the exact date triggered a response. It did not understand flexibility unless that flexibility was made explicit in the prompt it received.\nThat small oversight rippled through the process. It triggered an unnecessary escalation, pulled attention away from genuine issues, and reminded us that autonomy without memory is not the same as understanding.\nHow We Measured Success\nTo understand how well the system was performing, we needed something to compare it against. So we asked clinical reviewers to go through a set of patient records and mark any protocol deviations they spotted. This gave us a reference set, a gold standard, that we could use to test the agents.\nWe then ran the same data through the system and tracked how often it matched the human reviewers. When the agent flagged something that was also noted by a reviewer, we counted it as a hit. If it missed something important or raised a false alarm, we marked it accordingly. This gave us basic measures like sensitivity and specificity, in plain terms, how good the system was at picking up real issues and how well it avoided false ones.\nBut we also looked at the process itself. It was not just about whether a single agent made the right call, but whether the information made it through the chain. We tracked handovers between agents, how often a detected issue was correctly passed along, whether follow-up steps were triggered, and whether the right output was produced in the end.\nThis helped us see where the system worked as intended and where things broke down, even when the core detection was accurate. It was never just a question of getting the right answer. It was also about getting it to the right place.\nWhat We Changed Along the Way\nOnce we understood where things were going wrong, we made a few targeted changes to steady the system.\nFirst, we introduced structured memory snapshots. These acted like running notes that captured key protocol rules and exceptions at each stage. Rather than expecting every agent to remember what came before, we gave them a shared space to refer back to. This made it easier to hold onto details like visit windows or exemption clauses, even as the task moved between agents.\nWe also moved beyond rigid prompt templates. Early versions of the system leaned heavily on predefined phrasing, which limited the agents’ flexibility. Over time, we allowed the agents to generate their own sets of questions and reason through the answers independently. This gave them more space to interpret ambiguous situations and respond with a clearer sense of context, rather than relying on tightly scripted instructions. Alongside this, we rewrote prompts to be clearer and more grounded in the original trial language. Ambiguity in wording was often enough to derail performance, so small tweaks, phrasing things the way a study nurse might, made a noticeable difference. We then added stronger handoff signals. These were markers that told the next agent what had just happened, what context was essential, and what action was expected. It was a bit like writing a handover note for a colleague. Without that, agents sometimes acted without full context or missed the point altogether. Finally, we built in simple checks to track what happened after an alert was raised. Did the follow-up agent respond? Was the right report generated? If not, where did the thread break? These checks gave us better visibility into system behaviour and helped us spot patterns that weren’t obvious from the output alone.\nNone of these changes made the system perfect. But they helped close the loop. Errors became easier to trace. Fixes became faster to test. And confidence grew that when something went wrong, we would know where to look.\nWhat It Taught Us\nThe system did not live up to the hype, and it was not flawless, but it proved genuinely useful. It spotted patterns early. It highlighted things we might have overlooked. And, just as importantly, it changed how people interacted with the data. Rather than spending hours checking every line, reviewers began focusing on the edge cases and thinking more critically about how to respond. The role shifted from manual detective work to something closer to intelligent triage.\nWhat agentic AI brought to the table was not magic, but structure. It added pace to routine checks, consistency to decisions, and visibility into what had been flagged and why. Every alert came with a traceable rationale, every step with a record. That made it easier to explain what the system had done and why, which in turn made it easier to trust.\nAt the same time, it reminded us what agents still cannot do. They do not infer the way people do. They do not fill in blanks or read between the lines. But they do follow instructions. They do handle repetition. They do maintain logic across complex checks. And in clinical research, where consistency matters just as much as cleverness, that counts for a lot.\nThis experience did not make us think agentic systems were ready to run trials alone. But it did show us they could support the process in a way that was measurable, transparent, and worth building on.\nWhat This Taught Us About Evaluation\nWorking with agentic systems made one thing especially clear. The way most people assess language models does not prepare you for what happens when those models are placed inside a real workflow.\nIt is easy enough to test for accuracy or coherence in response to a single prompt. But those surface checks do not reflect what it takes to complete a task that unfolds over time. When an agent is making decisions, juggling memory, switching between tools, and coordinating with others, a different kind of evaluation is needed.\nWe began paying attention to the sorts of things that rarely make it into research papers. Could the agent perform the same task consistently across repeated attempts? Did it remember what had just happened a few steps earlier? When one component passed information to another, did it land correctly? Did the agent use the right tool when the moment called for it, even without being told explicitly?\nThese were not academic concerns. They were practical indicators of whether the system would hold up under pressure. So we built simple ways to track them.\nWe looked at how stable the agent remained from one run to the next. We measured how often a person needed to step in. We checked whether the agent could retrieve details it had already encountered. And we monitored how information moved through the system, from one part to another, without being lost or altered along the way.\nNone of this required complex metrics. But each of these signals told us more about how the system behaved in real use than any benchmark ever did."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#a-call-for-practical-evaluation-standards",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#a-call-for-practical-evaluation-standards",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "4 A Call for Practical Evaluation Standards",
    "text": "4 A Call for Practical Evaluation Standards\nIf we want reliable ways to judge these systems, we need to start from what happens when they are used in the real world. Much of the current thinking around evaluating agentic AI remains too abstract. It often focuses on what the system is supposed to do in principle, not what it manages to do in practice. But the most useful insights emerge when things fall apart. When an agent loses track of its task, forgets what just happened, or takes an unexpected turn under pressure.\nA recent assessment of Sakana.ai’s AI Scientist made this point sharply. The system promised end-to-end research automation, from forming hypotheses to writing up results. It was an ambitious step forward. But when tested, it fell short in important ways. It skimmed literature without depth, misunderstood experimental methods, and stitched together reports that looked complete but were riddled with basic errors. One reviewer said it read like something written in a hurry by a student who had not done the reading. The outcome was not a failure of intent, but a reminder that sophisticated language does not always reflect sound reasoning.\nInstead of designing evaluation methods in isolation, we should begin with real scenarios. That means observing where agents stumble, how they recover, and whether they can carry through when steps are long and outcomes matter. It means showing the messy bits, not just polished results. Tools that help us retrace decisions, inspect memory, and understand what went wrong are just as important as the outputs themselves.\nOnly by starting from lived use with its uncertainty, complexity, and human oversight, can we build evaluation methods that truly reflect what it means for these systems to be useful."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#closing-thoughts-from-the-field",
    "href": "instruments_old/case-studies/posts/2025/08/12/deploying-agentic-ai.html#closing-thoughts-from-the-field",
    "title": "Deploying Agentic AI - What Worked, What Broke, and What We Learned",
    "section": "5 Closing Thoughts from the Field",
    "text": "5 Closing Thoughts from the Field\nAgentic AI carries genuine promise, but even a single deployment can reveal how much distance there is between ambition and execution. These systems can be impressively capable in some moments and surprisingly brittle in others. And in domains where decisions must be precise and timelines matter, that brittleness is more than an inconvenience; it introduces real risk.\nThe lessons from our experience were not abstract. They came from watching one system try to handle a demanding, high-context task and seeing where it stumbled. It was not a matter of poor design or unrealistic expectations. The complexity was built in, the kind that only becomes visible once a system moves beyond isolated prompts and into continuous workflows.\nThat is why evaluation needs to begin with real use. With lived attempts, not controlled tests. With unexpected behaviours, not just benchmark scores. As practitioners, we have a front-row seat to what breaks, what improves with small tweaks, and what truly helps. That view should help shape how the field evolves.\nIf agentic systems are to mature, the stories of where they struggled and how we adapted cannot sit on the sidelines. They are part of how progress happens. And they may be the clearest indicators of what needs to change next.\n\nFind more case studies\n\n\nAbout the authors\n\nFrancis Osei is the Lead Clinical AI Scientist and Researcher at Bayezian Limited, where he designs and builds intelligent systems to support clinical trial automation, regulatory compliance, and the safe, transparent use of AI in healthcare. His work brings together data science, statistical modelling, and real-world clinical insight to help organisations adopt AI they can understand, trust, and act on.\n\n\n\n\nCopyright and licence\n\n© 2025 Francis Osei\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by khunkornStudio on Shutterstock.\n\n\n\nHow to cite\n\nOsei, F. (2025). “Deploying Agentic AI: What Worked, What Broke, and What We Learned”, Real World Data Science, August 12, 20245. URL\n\n\n\n::: :::"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/00-food-for-thought.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/00-food-for-thought.html",
    "title": "The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy",
    "section": "",
    "text": "There’s a saying: “You are what you eat.” Its meaning is somewhat open to interpretation, as with many such sayings, but it is typically used to make the point that if you want to be well, you need to eat well. Nutrition scientists and dieticians spend their careers trying to figure out what “eating well” looks like – the foods the human body needs, in what quantities, and how best to consume them. Their research informs advice and guidance issued by health professionals and governments. Ultimately, though, the choice of what to eat falls to us – individuals and families – and our choices are often determined by our tastes, the availability of foodstuffs in our local stores, their price and affordability.\nSo, what exactly do we eat? Answers come from a variety of sources. In the United States, there are dietary recall studies such as the National Health and Nutrition Examination Survey, which asks a sample of respondents to report their food and beverage consumption over a set period of time. There are also organisations like IRI that collect point-of-sale data from retail stores on the actual food and drink being sold to consumers. By and large, this information comes from barcodes on product packaging being scanned at checkouts, so it is often referred to as “scanner data”.\nThis data – from dietary recall studies and retail scanners – is valuable: once we know what people are eating, we can check the nutritional content of those foods and build up a picture of what the diet of a typical individual or family looks like and how it compares to the diet recommended by doctors and policymakers. And, if we know what other foodstuffs are available, how much they cost, and the nutritional value of those items, we can work out how much families need to spend, and on what, in order to eat well and, hopefully, be well.\nFiguring all this out is where something called the Purchase to Plate Crosswalk (PPC) comes in. It’s a key tool for understanding the “healthfulness of retail food purchases” and it does this by linking IRI scanner data on what people buy with data on the nutritional content of those foods, as recorded in the US Department of Agriculture’s Food and Nutrient Database for Dietary Studies (FNDDS). But there’s a catch: scanner data is collected about hundreds of thousands of food products, whereas the FNDDS has nutritional profile information for only a few thousand items. Linking these two datasets therefore gives rise to a one-to-many matching problem – a problem that takes several hundred person-hours to resolve.\nWhat if machine learning can help? That question inspired a competition, the Food for Thought Challenge, organized by the Coleridge Initiative, a nonprofit organization working with governments to ensure that data are more effectively used for public decision-making. Researchers and data scientists were invited to use machine learning and natural language processing to more efficiently link data on supermarket products to nutrient databases.\nThis collection of articles tells the story of the Food for Thought Challenge. We begin by exploring the policy issues that drive the development of the PPC – the need to understand the national diet, developing healthy diet plans, and costing up those plans – and the issues posed by record linkage. Next, we learn about the nature of the challenge and the structure of the competition in more detail, and then the three winning teams walk us through their solutions. We end the collection with some closing thoughts on the value of competitions for addressing data scientific challenges in the public sector.\n\n\n\n\nFind more case studies\n\n\n\n\nPart 1: The Purchase to Plate Suite →\n\n\n\n\n\n\n\n\nAbout the authors\n\nBrian Tarran is editor of Real World Data Science, and head of data science platform at the Royal Statistical Society.\n\n\nJulia Lane is a professor at the NYU Wagner Graduate School of Public Service and a NYU Provostial Fellow for Innovation Analytics. She co-founded the Coleridge Initiative, whose goal is to use data to transform the way governments access and use data for the social good through training programs, research projects and a secure data facility. She recently served on the Advisory Committee on Data for Evidence Building and the National AI Research Resources Task Force.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Royal Statistical Society and Julia Lane\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Melanie Lim on Unsplash.\n\n\n\nHow to cite\n\nTarran, Brian, and Julia Lane. 2023. “The Food for Thought Challenge: Using AI to support evidence-based food and nutrition policy.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "",
    "text": "Disclaimer\n\n\n\nThe findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or US Government determination or policy. This research was supported by the US Department of Agriculture’s Economic Research Service and Center for Nutrition, Policy and Promotion. Findings should not be attributed to Circana (formerly IRI).\nAbout 600,000 deaths per year in the United States are related to chronic diseases that are linked to poor dietary choices. Many other individuals suffer from diet-related health conditions, which may limit their ability to work, learn, and be physically active (US Department of Agriculture and US Department of Health and Human Services 2020). In recognition of the link between diet and health, in 1974 the Senate Select Committee on Nutrition and Human Needs, originally formed to eliminate hunger, expanded its focus to improving eating habits, nutrition policy and the national diet. Since 1980, the Dietary Guidelines for Americans have been released every five years by the US Departments of Agriculture (USDA) and Health and Human Services (DHHS). The guidelines present “advice on what to eat and drink to meet nutrient needs, promote health, and prevent disease”.\nBecause there can be economic and social barriers to maintaining a healthy diet, USDA promotes Food and Nutrition Security so that everyone has consistent and equitable access to healthy, safe, and affordable foods that promote optimal health and well-being. A set of data tools called the Purchase to Plate Suite (PPS) supports these goals by enabling the update of the Thrifty Food Plan (TFP), which estimates how much a budget-conscious family of four needs to spend on groceries to ensure a healthy diet. The TFP market basket – consisting of the specific amounts of various food categories required by the plan – forms the basis of the maximum allotment for the Supplemental Nutrition Assistance Program (SNAP, formerly known as the “Food Stamps” program), which provided financial support towards the cost of groceries for over 41 million individuals in almost 22 million households in fiscal year 2022.\nThe 2018 Farm Act (Agriculture Improvement Act of 2018) requires that USDA reevaluate the TFP every five years using current food composition, consumption patterns, dietary guidance, and food prices, and using approved scientific methods. USDA’s Economic Research Service (ERS) was charged with estimating the current food prices using retail food scanner data (Levin et al. 2018; Muth et al. 2016) and utilized the PPS for this task. The most recent TFP update was released in August 2021 and the revised cost of the market basket was the first non-inflation adjustment increase in benefits for SNAP in over 40 years (US Department of Agriculture 2021).\nThe PPS combines datasets to enhance research related to the economics of food and nutrition. There are four primary components of the suite:\nThe PPC allows researchers to measure the healthfulness of store purchases. On average US consumers acquire about 75% of their calories from retail stores, and there are a number of studies linking the availability of foods at home to the healthfulness of the overall diet (e.g., Gattshall et al. 2008; Hanson et al. 2005). Thus, understanding the healthfulness of store purchases allows us to understand differences in consumers who purchase healthy versus less healthy foods, and may contribute to better policies that promote healthier food purchases. While healthier diets are linked to a lower risk of disease outcomes (Reedy et al. 2014), other factors such as health care access may also be contributors (Cleary, Liu, and Carlson 2022). The PPC also forms the basis of the price tool, PPPT – which allows researchers to estimate custom prices for dietary recall studies – and a new ERS data product, the PP-NAP. The national average prices from PP-NAP are used in reevaluating the TFP. By using the PP-NAP with 24-hour dietary recall information from surveys such as What We Eat in America (WWEIA) – the dietary component of the nationally representative National Health and Nutrition Examination Survey(NHANES)1 – researchers can examine the relationship between the cost of food, dietary intake, and chronic diseases linked to poor diets. The price estimates also allow researchers to develop cost-effective healthy diets such as MyPlate Kitchen. The final component of the Purchase to Plate Suite, the ingredient tool (PPIT), breaks dietary recall-reported foods back into purchasable ingredients, based on US retail food purchases. The PPIT is also used in the revaluation of the TFP, and by researchers who want to look at the relationship between reported ingestion of grocery items, cost and disease outcomes using WWEIA/NHANES. More information on the development of the PPC is available in two papers by Carlson et al. (2019, 2022).\nThe Food for Thought competition aimed to support the development of the PPC – and thus policy-oriented research – by linking retail food scanner data to the USDA nutrition data used to analyze NHANES dietary recall data, specifically the Food and Nutrient Database for Dietary Studies (FNDDS) (2018, 2020). In particular, the competition set out to use artificial intelligence (AI) to reduce human resources in creating the links for the PPC, while still maintaining the high-quality standards required for reevaluating the TFP and for data published by ERS (which is one of 13 Principle Statistical Agencies in the United States Federal Government)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "href": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#methods-used-to-date",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Methods used to date",
    "text": "Methods used to date\nOn the surface, the linking process may appear simple: both the FNDDS and retail food scanner data are databases of food. But the scanner data are produced for market research, and the FNDDS for dietary studies. The scanner data include about 350,000 items with sales each year, while the FNDDS has only 10,000–15,000 items. Scanner data relates to specific products, while FNDDS items are often more general. Both datasets have different hierarchical structures – the FNDDS hierarchy is based around major food groups: dairy; meat, poultry and seafood; eggs; nuts and legumes; grains; fruits; vegetables; fats and oils; and sugars, sweets, and beverages. Items fall into the groups regardless of preparation method or form. That is, broccoli prepared from frozen and from fresh both appear in the vegetable group, and for some fruits and vegetables, the fresh, frozen, canned and dried form are the same FNDDS item. Vegetable-based mixed dishes, such as broccoli and carrot stir-fry or soup, are also classified in the vegetable group. On the other hand, the scanner data classifies foods by grocery aisle. That is, the fresh and frozen broccoli are classified in different areas: produce and frozen vegetables. Similarly, when sold as a prepared food, the broccoli and carrot stir-fry may be found in the frozen entries, as a kit in either the frozen or produce section, refrigerated foods, or all of these.\nTo allow researchers to import the FNDDS nutrient data into the scanner data, a one-to-many match between FNDDS and scanner data items was needed. The food descriptions in the scanner data include brand names and package sizes and are written as a consumer would pronounce them – e.g., fresh and crisp broccoli florets, ready-cut, 10 oz – versus a more general FNDDS description such as “Broccoli, raw”. (Also linked to the “Broccoli, raw” code would be broccoli sold with stems attached, broccoli spears, and any other way raw broccoli is sold.) In the scanner data, the Universal Product Code (UPC) and the European Article Number (EAN) can link items between tables within the scanner data, as well as between datasets of grocery items, such as the USDA Global Branded Foods Product Database, a component of USDA’s Food Data Central. However, these codes are not related to the FNDDS codes, or any other column within the FNDDS. In other words, before development of the PPC, there were no established linking identifiers.\nFigure 1 shows the process USDA uses to develop matches between scanner data and FNDDS.\n\n\nFigure 1: Process currently used to create the matches between the USDA Food and Nutrient Database for Dietary Studies (FNDDS) and the retail scanner data (labelled “IRI” for the IRI InfoScan and Consumer Network) product dictionaries. Source: Author provided.\n\nWe start the linking process by categorizing the scanner data items into homogeneous groups to make the first round of automated matching more efficient. To save time, we use the second lowest hierarchical category in the scanner data which generally divides items within a grocery aisle into homogenous groups such as produce, canned beans, baking mixes, and bread. Once the linking categories for scanner data are established, we select appropriate items from the FNDDS. Since the FNDDS is highly structured, this selection is usually straightforward.\nOur next step is to use semantic matching to create a search table that aligns similar terms within the IRI product dictionary and FNDDS. This first requires that we extract attributes from the FNDDS descriptions into fields similar to those in the scanner data product dictionary. The FNDDS descriptions are found across multiple columns because they are added as the need arises to provide examples of brand names or alternative descriptions of foods which help code the foods WWEIA participants report eating. We manually create matching tables that link terms used in FNDDS to those used in the scanner data, organized by the fields defined in the restructured FNDDS. We then use this table as the basis of a probabilistic matching process. For example, when linking the produce group, “fresh” in the scanner data would be aligned with “raw” and “prepared from fresh” and NOT “prepared from frozen” in the FNDDS, and “broccoli florets” would also be aligned with “raw” and “broccoli”. Since the FNDDS is designed to code the foods individuals report eating, many of the foods in the FNDDS are already prepared and result in descriptions such as “broccoli, steamed, prepared from fresh” or “broccoli, boiled, prepared from frozen”.\nOnce the linking table is established, the probabilistic match process returns the single best possible match for each item in the scanner data. For example, a match between fresh broccoli florets and frozen broccoli would have a lower probability score than “broccoli, raw”. Because these matches form the basis of major USDA policies, we cannot accept an error rate of more than 5 percent, and lower is preferred. To reach that goal, nutritionists review every match to make sure the probabilistic match did not return a match between cauliflower florets and fresh broccoli, say, or that a broccoli and carrot stir-fry is not matched to a dish with broccoli, carrots, and chicken. The correct matches, such as the one between fresh broccoli florets and raw broccoli, are set aside while the items with an incorrect match, such as cauliflower florets and the broccoli and carrot stir-fry, are used to revise the search table. Revisions might include adding (NOT chicken) to the broccoli and carrot stir-fry dish. Mixed dishes — such as the broccoli and carrot stir-fry — pose particular challenges because there are a wide variety of similar products available in the grocery store. After a few rounds of revising the search table and running the probabilistic match process, it is more efficient to use a manual match, established by one nutritionist and reviewed by another, after which the match is assumed to be correct.\nThe process improved with each new wave of FNDDS and IRI data. Our first creation of the PPC linked the FNDDS 2011/12 to the 2013 IRI retail scanner data. Subsequent waves started with the previous search table and resulting matches were reviewed by nutritionists. We also used more fields in the IRI product dictionary to create the homogeneous linking groups and made modifications to these groups with each wave. During each wave we experimented with the number of rounds of probabilistic matching that was the most cost effective. For some linking groups it took less human time to manually match from the start, while for other groups it was more efficient to do multiple rounds of improvements to the search table. Starting with the most recent wave (matching FNDDS 2017/18 to the 2017 and 2018 retail scanner data), we assumed previous matches appearing in the newer data were correct. Although this assumption was good for most matches, a review demonstrated the need to review previous matches prior to removing the item from the list of scanner data items needing FNDDS matches. In the future we intend to explore methods developed by the participants of the Food for Thought competition."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "href": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#linking-challenges",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Linking challenges",
    "text": "Linking challenges\nAn ongoing challenge to the linking problem is that both the scanner data and the FNDDS undergo substantive changes each year, meaning that both the previous matches and search tables need to be reviewed and revised with each new effort, as tables that work with one cycle of FNDDS and scanner data will need revisions to use with the next cycle. Changes to the scanner data that impact our current method include dropped and added items, data corrections, and revisions to the categories that form the basis of the homogeneous linking groups. In addition, there are errors such as incorrect food descriptions, conflicting package size information, and changes in the item description from year to year. Since the FNDDS is designed to support dietary recall studies, revisions reflect both changes to available foods and the level of detail respondents can provide. These revisions result in dropped/added food codes, changes to food descriptions that impact which scanner data items match to the FNDDS items, and revisions to recipes used in the nutrient coding which impacts the number of retail ingredients available in the FNDDS.\nOf the four parts of the PPS, establishing the matches is the most time-consuming task and constitutes at least 60 percent of the total budget. In the most recent round, we had 168 categories and each one went through 2-3 automated matching rounds; after each round, nutritionists spent an average of two hours reviewing the matches. This adds up to somewhere between 670 and 1,000 hours of review time. After the automated review, manual matching requires an additional 300 hours. Reducing the amount of time required to establish matches and link the FNDDS and retail scanner datasets may lead to significant time savings, resulting in faster data availability. That, in turn, could allow more timely policy-based research, and the mandated revision of the Thrifty Food Plan can continue with the most recent food price data.\n\n\n\n\n← Introduction\n\n\n\n\nPart 2: Competition design →\n\n\n\n\n\n\n\n\nAbout the authors\n\nAndrea Carlson is an agricultural economist in the Food Markets Branch of the Food Economics Division in USDA’s Economic Research Service. She is the project lead for the Purchase to Plate Suite, which allows users to import USDA nutrient and food composition data into retail food scanner data acquired by USDA and estimate individual food prices for dietary intake data.\n\n\nThea Palmer Zimmerman is a senior study director and research nutritionist at Westat.\n\n\n\n\n\nImage credit\n\nThumbnail photo by Kenny Eliason on Unsplash.\n\n\n\n\n\nHow to cite\n\nCarlson, Andrea, and Thea Palmer Zimmerman. 2023. “Food for Thought: The importance of the Purchase to Plate Suite.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "href": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#acknowledgements",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe research presented in this compendium supports the Purchase to Plate Suite of data products. Carlson has been privileged to both develop and lead this project over the course of her career, but it is not a solo project. Many thanks to the Linkages Team from USDA’s Economic Research Service (Christopher Lowe, Mark Denbaly Elina Page, and Catherine Cullinane Thomas) the Center for Nutrition Policy and Promotion (Kristin Koegel, Kevin Kuczynski, Kevin Meyers Mathieu, TusaRebecca Pannucci), and our contractor Westat, Inc. (Thea Palmer Zimmerman, Carina E. Tornow, Amber Brown McFadden, Caitlin Carter, Viji Narayanaswamy, Lindsay McDougal, Elisha Lubar, Lynnea Brumby, Raquel Brown, and Maria Tamburri). Many others have supported this project over the years."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "href": "instruments_old/case-studies/posts/2023/08/21/01-purchase-to-plate.html#footnotes",
    "title": "Food for Thought: The importance of the Purchase to Plate Suite",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNHANES is a multi-module continuous survey conducted by the Centers for Disease Control and Prevention. In addition to the WWEIA, NHANES includes a four-hour complete medical exam including a health history, and a blood and urine analysis.↩︎"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "",
    "text": "Undergraduate student Yifan (Rosetta) Hu was responsible for writing the Python script that pre-processes the 2015–2016 UPC, EC, and PPC data for training neural network models. Her script randomly sampled five negative EC descriptions for every positive match between a UPC and EC code. Professor Mandy Korpusik performed the remaining work, including setting up the environment, training the BERT model, and evaluation. Hu spent roughly 10 hours on the competition, and Korpusik spent roughly 40 hours of work (and many additional hours running and monitoring the training and testing scripts)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-perspective-on-the-challenge",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our perspective on the challenge",
    "text": "Our perspective on the challenge\nThe goal of this challenge is to use machine learning and natural language processing (NLP) to link language-based entries in the IRI and FNDDS databases. Our proposed approach is based on our prior work using deep learning models to map users’ natural language meal descriptions to the FNDDS database (Korpusik, Collins, and Glass 2017b) to retrieve nutrition information in a spoken diet tracking system. In the past, we found a trade-off between accuracy and cost, leading us to select convolutional neural networks over recurrent long short-term memory (LSTM) networks – with nearly 10x as many parameters and 2x the training time required, LSTMs achieved slightly lower performance on semantic tagging and food database mapping on meals in the breakfast category. Here, we propose to investigate state-of-the-art transformers, specifically the contextual embedding model (i.e., the entire sentence is used as context to generate the embedding) known as BERT (Bidirectional Encoder Representations from Transformers, Devlin et al. 2018).\n\nRelated work\nWithin the past few years, several papers have come out that learn contextual representations of sentences, where the entire sentence is used to generate embeddings.\nELMo (Peters et al. 2018) uses a linear combination of vectors extracted from intermediate layer representations of a bidirectional LSTM trained on a large text corpus as a language model; in this feature-based approach, the ELMo vector of the full input sentence is concatenated with the standard context-independent token representations and passed through a task-dependent model for final prediction. This showed performance improvement over state-of-the-art on six NLP tasks, including question answering, textual entailment, and sentiment analysis.\nOpenAI GPT (Radford et al. 2018) is a fine-tuning approach, where they first pre-train a multi-layer transformer (Vaswani et al. 2017) as a language model on a large text corpus, and then conduct supervised fine-tuning on the specific task of interest, with a linear softmax layer on top of the pre-trained transformer.\nGoogle’s BERT (2018) is a fine-tuning approach similar to GPT, but with the key difference that instead of combining separately trained forward and backward transformers, they instead use a masked language model for pre-training, where they randomly masked out input tokens and predicted only those tokens. They demonstrated state-of-the-art performance on 11 NLP tasks, including the CoNLL 2003 named entity recognition task, which is similar to our semantic tagging task.\nFinally, many models have recently been developed that improve upon BERT, including RoBERTa (which improves BERT’s pre-training by using bigger batches and more data, Y. Liu et al. 2019), XLNet (which uses Transformer-XL and avoids BERT’s pretrain-finetune discrepancy through learning a truly bidirectional context via permutations over the factorization order, Yang et al. 2019), and ALBERT (a lightweight BERT, Lan et al. 2019).\nIn our prior work on language understanding for nutrition (Korpusik et al. 2014, 2016; Korpusik and Glass 2017, 2018, 2019; Korpusik, Collins, and Glass 2017a), we used a similar binary classification approach for learning embeddings, which were then used at test time to map from user-described meals to USDA food database matches, but with convolutional neural networks (CNNs) instead of BERT. (BERT was not created until 2018, and due to limited memory available for deployment, we needed a smaller model than even BERT base, which has 100 million parameters.) Further work demonstrated that BERT outperformed CNNs on several language understanding tasks, including nutrition (Korpusik, Liu, and Glass 2019)."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-approach",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our approach",
    "text": "Our approach\nOur approach is to fine-tune a large pre-trained BERT language model on the food data. BERT was originally trained on a massive amount of text for a language modelling task (i.e., predicting which word should come next in a sentence). It relies on a transformer model, which uses an “attention” mechanism to identify which words the model should pay the most “attention” to. We are specifically using BERT for binary sequence classification, which refers to predicting a label (i.e., classification) for a sequence of words. In our case, during fine-tuning (i.e., training the model further on our own dataset) we will feed the model pairs of sentences (where one sentence is the UPC description of a food item and the other is the EC description of another food item), and the model will perform binary classification, predicting whether the sentences are a match (i.e., 1) or not (i.e., 0). We start with the 2015–2016 ground truth PPC data for positive examples, and five randomly sampled negative examples per positive example.\n\nTraining methods\nSince we used a neural network model, the only features passed into our model were the tokenized words themselves of the EC and UPC food descriptions – we did not conduct any manual feature engineering (Dong and Liu 2018). The model was trained on a 90/10 split into 90% training and 10% validation data, where the validation data was used as a test set to fine-tune the model’s hyperparameters. We started with a randomly sampled set of 16,000 pairs, batch size of 16 (i.e., the model would train on batches of 16 samples at a time), AdamW (Loshchilov and Hutter 2017) as the optimizer (which adaptively updates the learning rate, or how large the update should be to the model’s parameters), a linear schedule with warmup (i.e., starting with a small learning rate in the first few epochs of training due to large variance in early stages of training, L. Liu et al. 2019), and one epoch (i.e., the number of times the model passes through all the training data). We then added the next randomly sampled set of 16,000 pairs to get a model trained on 32,000 data points. Finally, we reached a total of 48,000 data samples used for training. Each pair of sequences was tokenized with the pre-trained BERT tokenizer, with the special CLS and SEP tokens (where CLS is a learned vector that is typically passed to downstream layers for final classification, and SEP is a learned vector that separates two input sequences), and was padded with zeros to the maximum length input sequence of 240 tokens, so that each input sequence would be the same length.\n\n\nModel development approach\nWe faced many challenges due to the secure nature of the ADRF environment. Since our approach relies on BERT, we were blocked by errors due to the local BERT installation. Typically, BERT is downloaded from the web as the program runs. However, for this challenge, BERT must be installed locally for security reasons. To fix the errors, the BERT models needed to be installed with git lfs clone instead of git.\nSecond, we were unable to retrieve the test data from the database due to SQLAlchemy errors. We found a workaround by using DBeaver directly to save database tables as Excel spreadsheets, rather than accessing the database tables through Python.\nFinally, we needed a GPU in order to efficiently train our BERT models. However, we initially only had a CPU, so there was a delay due to setting up the GPU configuration. Once the GPU image was set up, there was still a CUDA error when running the BERT model during training. We determined that the model was too big to fit into GPU memory, so we found a workaround using gradient checkpointing (trading off computation speed for memory) with the transformers library’s Trainer and TrainingArguments. Unfortunately, the version of transformers we were using did not have these tools, and the library was not updated until less than a week before the deadline, so we still had to train the model on the CPU.\nTo deal with the inability to run jobs in the background, our process was checkpointing our models every five batches, and saving the model predictions during evaluation to a csv file every five batches as well.\n\n\n\n\n\n\nFind the code in the Real World Data Science GitHub repository."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#our-results",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Our results",
    "text": "Our results\nAfter training, the 48K model (so-called because it was trained on 48,000 data samples) was used at test time via ranking all possible 2017–18 EC descriptions given an unseen UPC description. The rankings were obtained through the model’s output value – the higher the output (or confidence), the more highly we ranked that EC description. To speed up the ranking process, we used blocking (i.e., only ranking a subset of all possible matches), specifically with exact word matches (using only the first six words in the UPC description, which appeared to be the most important), and fed all possible matches through the model in one batch per UPC description. Since we still did not have sufficient time to complete evaluation on the full set of test UPC descriptions, we implemented an expedited evaluation that only considered the first 10 matching EC descriptions in the BERT ranking process (which we call BERT-FAST). We also report results for the slower evaluation method that considers all EC descriptions that match at least one of the first six words in a given UPC description, but note that these results are based on just a small subset of the total test set. See Table 1 below for our results, where the (5?) indicates how often the correct match was ranked among the top-5. See Table 2 for an estimate of how long it takes to train and test the model on a CPU.\n\n\n\n\nTable 1: S@5 and NCDG@5 for BERT, both for fast evaluation over the whole test set, and slower evaluation on a smaller subset (711 UPCs out of 37,693 total).\n\n\n\n\nModel\nSuccess@5\nNDCG@5\n\n\n\n\nBERT-FAST\n0.057\n0.047\n\n\nBERT-SLOW\n0.537\n0.412\n\n\n\n\n\n\nTable 2: An estimate of the time required to train and test the model.\n\n\n\n\n\nTime\n\n\n\n\nTraining (on 48K samples)\n16 hours\n\n\nTesting (BERT-FAST)\n52 hours\n\n\nTesting (BERT-SLOW)\n63 days"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#future-workrefinement",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Future work/refinement",
    "text": "Future work/refinement\nIn the future, with more time available, we would train on all data, not just our limited dataset of 48,000 pairs, as well as perform evaluation on the held-out test set with the full set of possible EC matches that have one or more words in common with the UPC description. We would compare against baseline word embedding methods such as word2vec (Mikolov et al. 2017) and Glove (Pennington, Socher, and Manning 2014), and we would explore hierarchical prediction methods for improving efficiency and accuracy. Specifically, we would first train a classifier to predict the generic food category, and then train finer-grained models to predict specific foods within a general food category. Finally, we are exploring multi-modal transformer-based approaches that allow two input modalities (i.e., food images and text descriptions of a meal) for predicting the best UPC match."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "href": "instruments_old/case-studies/posts/2023/08/21/05-third-place-winners.html#lessons-learned",
    "title": "Food for Thought: Third place winners – Loyola Marymount",
    "section": "Lessons learned",
    "text": "Lessons learned\nWe recommend that future challenges provide every team with both a CPU and a GPU in their workspace, to avoid transitioning from one to the other midway through the challenge. In addition, if possible, it would be very helpful to provide a mechanism for running jobs in the background. Finally, it may be useful for teams to submit snippets of code along with library package names, in order for the installations to be tested properly beforehand.\n\n\n\n\n← Part 4: Second place winners\n\n\n\n\nPart 6: The value of competitions →\n\n\n\n\n\n\n\n\nAbout the authors\n\nYifan (Rosetta) Hu is an undergraduate student and Mandy Korpusik is an assistant professor of computer science at Loyola Marymount University’s Seaver College of Science and Engineering.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Yifan Hu and Mandy Korpusik\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence. Thumbnail photo by Peter Bond on Unsplash.\n\n\n\nHow to cite\n\nHu, Yifan, and Mandy Korpusik. 2023. “Food for Thought: Third place winners – Loyola Marymount.” Real World Data Science, August 21, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "",
    "text": "Incarcerated youth are an exceptionally vulnerable population, and body-worn cameras are an important tool of accountability both for those incarcerated and the staff who supervise them. In 2018 the Texas Juvenile Justice Department (TJJD) deployed body-worn cameras for the first time, and this is a case study of how the agency developed a methodology for measuring the success of the camera rollout. This is also a case study of analysis failure, as it became clear that real-world implementation problems were corrupting the data and rendering the methodology unusable. However, the process of working through the causes of this failure helped the agency identify previously unrecognized problems and ultimately proved to be of great benefit. The purpose of this case study is to demonstrate how negative findings can still be incredibly useful in real-world settings."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#background",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Background",
    "text": "Background\nFrom the outset of the rollout of body-worn cameras, TJJD faced a major issue with implementation: in 2019, body worn cameras were an established tool for law enforcement, but there was very little literature or best practice to draw from for their use in a correctional environment. Unlike police officers, juvenile correctional officers (JCOs) deal directly with their charges for virtually their entire shift. In an eight-hour shift, a police officer might record a few calls and traffic stops. A juvenile correctional officer, on the other hand, would record for almost eight consecutive hours. And, because TJJD recorded round-the-clock for hundreds of employees at a time, this added up very quickly to a lot of footage.\nFor example, a typical dorm in a correctional center might have four JCOs assigned to it. Across a single week, these four JCOs would be expected to record at least 160 hours of footage.\n\n\n\n\n\n\nFigure 1: Four JCOs x 40 hours per week = 160 hours of footage.\n\nThis was replicated across every dorm. Three dorms, for example, would produce nearly 500 hours of footage, as seen below.\n\n\n\n\n\n\nFigure 2: Three dorms x four JCOs x 40 hours per week = 480 hours of footage.\n\nFinally, we had more than one facility. Four facilities with three dorms each would produce nearly 2,000 hours of footage every week.\n\n\n\n\n\n\nFigure 3: Four facilities x three dorms x four JCOs x 40 hours per week = 1,960 hours of footage.\n\nIn actuality, we had a total of five facilities each with over a dozen dorms producing an anticipated 17,000 hours of footage every week – an impossible amount to monitor manually.\nAs a result, footage review had to be done in a limited, reactive manner. If our monitoring team received an incident report, they could easily zero in on the cameras of the officers involved and review the incident accordingly. But our executive team had hoped to be able to use the footage proactively, looking for “red flags” in order to prevent potential abuses instead of only responding to allegations.\nBecause the agency had no way of automating the monitoring of footage, any proactive analysis had to be metadata-based. But what to look for in the metadata? Once again, the lack of best-practice literature left us in the lurch. So, we brainstormed ideas for “red flags” and came up with the following that could be screened for using camera metadata:\n\nMinimal quantity of footage – our camera policy required correctional officers to have their cameras on at all times in the presence of youth. No footage meant they weren’t using their cameras.\nFrequently turning the camera on and off – a correctional officer working a dorm should have their cameras always on when around youth and not be turning them on and off repeatedly.\nLarge gaps between clips – it defeats the purpose of having cameras if they’re not turned on.\n\nIn addition, we came up with a fourth red flag, which could be screened for by comparing camera metadata with shift-tracking metadata:\n\nMismatch between clips recorded and shifts worked – the agency had very recently rolled out a new shift tracking software. We should expect to see the hours logged by the body cameras roughly match the shift hours worked."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-1-quality-control-and-footage-analysis",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 1: Quality control and footage analysis",
    "text": "Analysis, part 1: Quality control and footage analysis\nFor this analysis, I gathered the most recent three weeks of body-worn camera data – which, at the time, covered April 1–21, 2019. I also pulled data from Shifthound (our shift management software) covering the same time period. Finally, I gathered HR data from CAPPS, the system that most of the State of Texas used at the time for personnel management and finance.1 I then performed some quality control work, summarized in the dropdown box below.\n\n\n\n\n\n\nInitial quality control steps\n\n\n\n\n\nSkimR is a helpful R package for exploratory analysis that gives summary statistics for every variable in a data frame, including missing values. After using the skim function on clip data, shift data, and HR data, I noticed that the clip data had some missing values for employee ID. This was an error which pointed to data entry mistakes – body-worn cameras do not record footage on their own, after all, so employee IDs should be assigned to each clip.\nFrom here I compared the employee ID field in the clip data to the employee ID field in the HR data. Somewhat surprisingly, IDs existed in the clip data that did not correspond to any entries in the HR data, indicating yet more data entry mistakes – the HR data is the ground truth for all employee IDs. I checked the shift data for the same error – employee IDs that did not exist in the HR data – and found the same problem.\nAs well as employee IDs that did not exist in the HR data, I also looked for employee IDs in the footage and shift data which related to staff who were not actually employed between April 1–21, 2019. I found some examples of this, which indicated yet more errors: staff cannot use a body-worn camera or log a shift if they have yet to begin working or if they have been terminated (system permissions are revoked upon leaving employment).\nI made a list of every erroneous ID to pass off to HR and monitoring staff before excluding them from the subsequent analysis. In total, 10.6% of clips representing 11.3% of total footage had to be excluded due to these initial data quality issues, foreshadowing the subsequent data quality issues the analysis would uncover.\nThe full analysis script can be found on GitHub.\n\n\n\nIn order to operationalize the “red flags” from our brainstorming session, I needed to see what exactly the cameras captured in their metadata. The variables most relevant to our purposes were:\n\nClip start\nClip end\nCamera used\nWho was assigned to the camera at the time\nThe role of the person assigned to the camera\n\nUsing these fields, I first created the following aggregations per employee ID:\n\n\n\n\n\n\nNumber of clips = Number of clips recorded.\n\n\n\n\n\n\n\nDays with footage = Number of discrete dates that appear in these clips.\n\n\n\n\n\n\n\n\n\nFootage hours = Total duration of all shot footage.\n\n\n\n\n\n\n\nSignificant gaps = Number of clips where the previous clip’s end date was either greater than 15 minutes or less than eight hours before current clip’s start date.\n\n\n\n\n\nI used these aggregations to devise the following staff metrics:\n\n\n\n\n\n\nClips per day = Number of clips / Days with footage.\n\n\n\n\n\n\n\nFootage per day = Footage hours / Days with footage.\n\n\n\n\n\n\n\n\n\nAverage clip length = Footage hours / Number of clips.\n\n\n\n\n\n\n\nGaps per day = Gaps / Days with footage.\n\n\n\n\n\nOnce I established these metrics for each employee I looked at their respective distributions. Standard staff shift lengths at the time were eight hours. If staff were using their cameras appropriately, we would expect to see distributions centered around clip lengths of about an hour, eight or fewer clips per day, and 8-12 footage hours per day. We would also expect to see 0 large gaps.\n\n\nShow the code\n\n```{r}\nlibrary(tidyverse)\n\nFootage_Metrics_by_Employee &lt;- read_csv(\"Output/Footage Metrics by Employee.csv\")\n\nFootage_Metrics_by_Employee %&gt;% \n  select(-Clips, -Days_With_Footage, -Footage_Hours, -Gaps) %&gt;% \n  pivot_longer(-Employee_ID, names_to = \"Metric\", values_to = \"Value\") %&gt;% \n  ggplot(aes(x = Value)) +\n  geom_histogram() +\n  facet_wrap(~Metric, scales = \"free\")\n```\n\n\n\n\n\n\nBy eyeballing the distributions I could tell most staff were recording fewer than 10 clips per day, shooting about 0.5–2 hours for each clip, for a total of 2–10 hours of daily footage, with the majority of employees having less than one significant gap per day. Superficially, this appeared to provide evidence of widespread attempts at complying with the body-worn camera policy and no systemic rejection or resistance. If this were indeed the case, then we could turn our attention to individual outliers.\nFirst, though, we thought we would attempt to validate this initial impression by testing another assumption. If each employee works on average 40 hours per week – a substantial underestimate given how common overtime was – we should expect, over a three-week period, to see about 120 hours of footage per employee in the dataset. This is not what we found.\nAverage footage per employee was 70.2 hours over the three-week period, meaning that the average employee was recording less than 60% of shift hours worked. With so many hours going unrecorded for unknown reasons, we needed to investigate further.\nSurely the shift data would clarify this…"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#analysis-part-2-footage-and-shift-comparison",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Analysis, part 2: Footage and shift comparison",
    "text": "Analysis, part 2: Footage and shift comparison\nWith the data on shifts worked from our timekeeping system, I could theoretically compare actual shifts worked to the amount of footage recorded. If there were patterns in where the gaps in footage fell, that comparison might help to explain why.\nIn order to join the shift data to the camera data, I needed a common unit of analysis beyond “Employee ID.” Using only this value would produce a nonsensical table that joined up every clip of footage to every shift worked.\nFor example, let’s take employee #9001005 at Facility Epsilon between April 1–3. This employee has the following clips recorded during that time period:\n\n\n\n\nEmployee_ID\nClip_ID\nClip_Start\nClip_End\n\n\n\n\n9001005\n156421\n2019-04-01 05:54:34\n2019-04-01 08:34:34\n\n\n9001005\n155093\n2019-04-01 08:40:59\n2019-04-01 08:54:51\n\n\n9001005\n151419\n2019-04-01 09:03:16\n2019-04-01 11:00:30\n\n\n9001005\n153133\n2019-04-01 11:10:09\n2019-04-01 12:39:51\n\n\n9001005\n151088\n2019-04-01 12:57:51\n2019-04-01 14:06:44\n\n\n9001005\n150947\n2019-04-02 05:56:34\n2019-04-02 09:48:50\n\n\n9001005\n151699\n2019-04-02 09:54:23\n2019-04-02 12:17:15\n\n\n\n\nWe can join this to a similar table of shifts logged. This particular employee had the following shifts scheduled from April 1–3:\n\n\n\n\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\n\n\n\n\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n\n\n9001005\nE051303\n2019-04-02 06:00:00\n2019-04-02 14:00:00\n\n\n\n\nThe table shows two eight-hour morning shifts from 6:00 am to 2:00 pm. We can join the two tables together by ID on a messy many-to-many join, but that tells us nothing about how much they overlap (or fail to overlap) without extensive additional work. For example, we have a unique identifier for employee clip (Clip_ID) and employee shift (Shift_ID), but what we need is a unique identifier that can be used to join the two. Fortunately, for this particular data we can create a unique identifier since both clips and shifts are fundamentally measures of time. While Employee_ID is not in itself unique (i.e., one employee can have multiple clips attached to that ID), Employee_ID combined with time of day is unique. A person can only be in one place at a time, after all!\nTo reshape the data for joining, I created a function that takes any data frame with a start and end column and unfolds it into discrete units of time. Using the code below to create the “Interval_Convert” function, the shift data above for employee 9001005 converts into one entry per hour of the day per shift. As a result, two eight-hour shifts get turned into 16 employee hours (a sample of which is shown below).\n\n\nShow the code\n\n```{r}\nlibrary(sqldf)\nlibrary(lubridate)\n\nInterval_Convert &lt;- function(DF, Start_Col, End_Col, Int_Unit, Int_Length = 1) {\nbrowser()\n  Start_Col2 &lt;- enquo(Start_Col)\n  End_Col2 &lt;- enquo(End_Col)\n  \n  Start_End &lt;- DF %&gt;%\n    ungroup() %&gt;%\n    summarize(Min_Start = min(!!Start_Col2),\n              Max_End = max(!!End_Col2)) %&gt;%\n    mutate(Start = floor_date(Min_Start, Int_Unit),\n           End = ceiling_date(Max_End, Int_Unit))\n  \n  DF &lt;- DF %&gt;%\n    mutate(Single = !!Start_Col2 == !!End_Col2)\n  \n  Interval_Table &lt;- data.frame(Interval_Start = seq.POSIXt(Start_End$Start[1], Start_End$End[1], by = str_c(Int_Length, \" \", Int_Unit))) %&gt;%\n    mutate(Interval_End = lead(Interval_Start)) %&gt;%\n    filter(!is.na(Interval_End))\n  \n  by &lt;- join_by(Interval_Start &lt;= !!End_Col2, Interval_End &gt;= !!Start_Col2)  \n  \n  Interval_Data_Table &lt;- Interval_Table %&gt;% \n    left_join(DF, by) %&gt;% \n    mutate(Seconds_Duration_Within_Interval = if_else(!!End_Col2 &gt; Interval_End, Interval_End, !!End_Col2) -\n             if_else(!!Start_Col2 &lt; Interval_Start, Interval_Start, !!Start_Col2)) %&gt;%\n    filter(!(Single & Interval_End == !!Start_Col2),\n           as.numeric(Seconds_Duration_Within_Interval) &gt; 0)\n  \n  return(Interval_Data_Table)\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterval_Start\nInterval_End\nEmployee_ID\nShift_ID\nShift_Start\nShift_End\nSeconds_Duration_Within_Interval\n\n\n\n\n2019-04-01 06:00:00\n2019-04-01 07:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 07:00:00\n2019-04-01 08:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 08:00:00\n2019-04-01 09:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n2019-04-01 09:00:00\n2019-04-01 10:00:00\n9001005\nE050603\n2019-04-01 06:00:00\n2019-04-01 14:00:00\n3600 secs\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n\n\nThe footage could be converted in a similar manner, and in this way I could break down both the shift data and the clip data into an hour-by-hour view and compare them to one another. Using this new format, I joined together the full tables of footage and shifts to determine how much footage was recorded with no corresponding shift in the timekeeping system.\n\n\n\n\n\n\n\n\n\nHR_Location\nFootage_Hours_No_Shift\nEmployee_IDs_With_Missing_Shift\n\n\n\n\nAlpha\n1805\n122\n\n\nBeta\n3749\n114\n\n\nDelta\n1208\n133\n\n\nEpsilon\n2899\n157\n\n\nGamma\n4153\n170\n\n\n\n\nTo summarize what the table is telling us: Almost every employee has footage hours that do not match with logged shifts, totaling nearly 14,000 hours when you add up the Footage_Hours_No_Shift column. But what about the opposite case? How many shift hours were logged with no corresponding footage?\n\n\n\n\n\n\n\n\n\nHR_Location\nShift_Hours_No_Footage\nEmployee_IDs_With_Missing_Footage\n\n\n\n\nAlpha\n7338\n127\n\n\nBeta\n6014\n118\n\n\nDelta\n12830\n141\n\n\nEpsilon\n9000\n168\n\n\nGamma\n11960\n183\n\n\n\n\nOh dear. Again, almost every employee has logged shift hours with no footage: 47,000 hours in total. To put it another way, that’s an entire work week per employee not showing up in camera footage.\nAt this point, we could probably rule out deliberate noncompliance. The clip data already implied that most employees were following the policy, and our facility leadership would surely have noticed a mass refusal large enough to show up this clearly in the data.\nOne way to check for deliberate noncompliance would be to first exclude shifts that contain zero footage whatsoever. This would rule out total mismatches, where – for whatever reason – the logged shifts had totally failed to overlap with recorded clips. For the remaining shifts that do contain footage, we could look at the proportion of the shift covered by footage. So, if an eight-hour shift had four hours of recorded footage associated with it, then we could say that 50% of the shift had been recorded. The following histogram is a distribution of the number of employees organized by the percent of their shift-hours they recorded (but only shifts that had a nonzero amount of footage).\n\n\n\n\n\nAs it turned out, most employees recorded the majority of their matching shifts, a finding that roughly aligns with the initial clip analysis. So, what explains the 14,000 hours of footage with no shifts, and the 47,000 hours of shifts with no footage?"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#causes-of-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Causes of failure",
    "text": "Causes of failure\nHere, I believed, we had reached the end of what I could do with data alone, and so I presented these findings (or lack thereof) to executive leadership. The failure to gather reliable data from linking the clip data to the shift data prompted follow-ups into what exactly was going wrong. As it turned out, many things were going wrong.\nFirst, a number of technical problems plagued the early rollout of the cameras:\n\nAll of our facilities suffered from high turnover, and camera ownership was not consistently updated. Employees who no longer worked at the agency could therefore appear in the clip data – somebody else had taken over their camera but had not put their name and ID on it.\nWe had no way of telling if a camera was not recording due to being docked and recharging or not recording due to being switched off.\nIn the early days of the rollout, footage got assigned to an employee based on the owner of the dock, not the camera. In other words, if Employee A had recorded their shift with their camera but uploaded the footage using a dock assigned to Employee B then the footage would show up in the system as belonging to Employee B.\n\nThe shift data was, unsurprisingly, even worse, and it was here we came across our most important finding. While the evidence showed that there wasn’t any widespread non-compliance with the use of the cameras, there was widespread non-compliance with the use of our shift management software. Details are included in the dropdown box below.\n\n\n\n\n\n\nQuality issues in shift tracking data\n\n\n\n\n\nOur HR system, CAPPS, had a feature that tracked hours worked in order to calculate leave and overtime pay. However, CAPPS was a statewide application designed for 9–5 office workers, and could not capture the irregular working hours of our staff (much less aid in planning future shifts). We had obtained separate shift management software to fill these gaps, but not realized how inconsistently it was being used. All facilities were required to have their employees log their shifts, but some followed through on this better than others. And even for those that did make a good-faith effort at follow-through, quality control was nonexistent.\nIn CAPPS, time entry determined pay, so strong incentives existed to ensure accurate entry. But for our shift management software, no incentives existed at all for making sure that entries were correct. For example, a correctional officer could have a 40-hour work week scheduled in the shift software but miss the entire week due to an injury, and the software would still show them as having worked 40 hours that week. Nobody bothered to go back and correct these types of errors because there was no reason to.\nThe software was intended to be used proactively for planning purposes, not after-the-fact for logging and tracking purposes. Thus, it produced data that was totally inconsistent with actual hours worked, which became apparent when compared to data (like body-worn camera footage) that tracked actual hours on the floor.\nIn the end, we had to rethink a number of aspects of the shift software’s implementation. In the process of these fixes, leadership also came to make explicit that the software’s primary purpose was to help facilities schedule future shifts, not audit hours worked after the fact (which CAPPS already did, just on a day-by-day basis as opposed to an hour-by-hour basis). This analysis was the only time we attempted to use the shift data in this manner."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#what-we-learned-from-failure",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "What we learned from failure",
    "text": "What we learned from failure\nWhatever means we used to monitor compliance with the camera policy, we learned that it couldn’t be fully automated. The agency followed up this analysis with a random sampling approach, in which monitors would randomly select times of day they knew a given staff member would have to have their cameras turned on and actually watch the associated clips. This review process confirmed the first impressions from the statistical review above: most employees were making good faith attempts at complying with the policy despite technical glitches, short-staffing, and administrative confusion. It also confirmed that proactive monitoring of correctional officers was a human process which had to come from supervisors and staff.\nThe one piece of the analysis we did use going forward was the clip analysis (converted into a Power BI dashboard and included in the GitHub repository for this article), but only as a supplement for already-launched investigations, not a prompt for one. Body-worn camera footage remained immensely useful for investigations after-the-fact, but inconsistencies in clip data were not, in and of themselves, particularly noteworthy “red flags.” At the end of the day, analytics can contextualize and enhance human judgment, but it cannot replace it.\nIn academia, the bias in favor of positive findings is well-documented. The failure to find something, or a lack of statistical significance, does not lend itself to publication in the same way that a novel discovery does. But, in an applied setting, where results matter more than publication criteria, negative findings can be highly insightful. They can falsify erroneous assumptions, bring unknown problems to light, and prompt the creation of new processes and tools. In this context, a failure is only truly a failure if nothing is learned from it.\n\nFind more case studies\n\n\n\n\n\nAbout the author\n\nNoah Wright is a data scientist with the Texas Juvenile Justice Department. He is interested in the applications of data science to public policy in the context of real-world constraints, and the ethics thereof (ethics being highly relevant in his line of work). He can be reached on LinkedIn.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Noah Wright\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nWright, Noah. 2023. “Learning from failure: ‘Red flags’ in body-worn camera data.” Real World Data Science, November 16, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "href": "instruments_old/case-studies/posts/2023/11/16/learning-from-failure.html#footnotes",
    "title": "Learning from failure: ‘Red flags’ in body-worn camera data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe underlying data for the analysis as presented in this article was requested through the Texas Public Information Act and went through TJJD’s approval process for ensuring anonymity of records. It is available on GitHub along with the rest of the code used to write this article.↩︎"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "href": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "",
    "text": "Reproducibility, or “the ability of a researcher to duplicate the results of a prior study using the same materials as the original investigator”, is critical for sharing and building upon scientific findings. Reproducibility not only verifies the correctness of processes leading to results but also serves as a prerequisite for assessing generalisability to other datasets or contexts. This we refer to as replicability, or “the ability of a researcher to duplicate the results of a prior study if the same procedures are followed but new data are collected”. Reproducibility, which is the focus of our work here, can be challenging – especially in the context of deep learning. This article, and associated material, aims to provide practical advice for overcoming these challenges.\nOur story begins with Davit Svanidze, a master’s degree student in economics at the London School of Economics (LSE). Davit’s efforts to make his bachelor’s thesis reproducible are what inspires this article, and we hope that readers will be able to learn from Davit’s experience and apply those learnings to their own work. Davit will demonstrate the use of Jupyter notebooks, GitHub, and other relevant tools to ensure reproducibility. He will walk us through code documentation, data management, and version control with Git. And, he will share best practices for collaboration, peer review, and dissemination of results.\nDavit’s story starts here, but there is much more for the interested reader to discover. At certain points in this article, we will direct readers to other resources, namely a Jupyter notebook and GitHub repository which contain all the instructions, data and code necessary to reproduce Davit’s research. Together, these components offer a comprehensive overview of the thought process and technical implementation required for reproducibility. While there is no one-size-fits-all approach, the principles remain consistent."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "href": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#davits-journey-towards-reproducibility",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Davit’s journey towards reproducibility",
    "text": "Davit’s journey towards reproducibility\n\nMore power, please\nThe focus of my bachelor’s thesis was to better understand the initial spread of Covid-19 in China using deep learning algorithms. I was keen to make my work reproducible, but not only for my own sake. The “reproducibility crisis” is a well-documented problem in science as a whole,1 2 3 4 with studies suggesting that around one-third of social science studies published between the years 2010 and 2015 in top journals like Nature and Science could not be reproduced.5 Results that cannot be reproduced are not necessarily “wrong”. But, if findings cannot be reproduced, we cannot be sure of their validity.\nFor my own research project, I gathered all data and started working on my computer. After I built the algorithms to train the data, my first challenge to reproducibility was computational. I realised that training models on my local computer was taking far too long, and I needed a faster, more powerful solution to be able to submit my thesis in time. Fortunately, I could access the university server to train the algorithms. Once the training was complete, I could generate the results on my local computer, since producing maps and tables was not so demanding. However…\n\n\nBloody paths!\nIn switching between machines and computing environments, I soon encountered an issue with my code: the paths, or file directory locations, for the trained algorithms had been hardcoded! As I quickly discovered, hardcoding a path can lead to issues when the code is run in a different environment, as the path might not exist in the new environment.\nAs my code became longer, I overlooked the path names linked to algorithms that were generating the results. This mistake – which would have been easily corrected if spotted earlier – resulted in incorrect outputs. Such errors could have enormous (negative) implications in a public health context, where evidence-based decisions have real impacts on human lives. It was at this point that I realised that my code is the fundamental pillar of the validity of my empirical work. How can someone trust my work if they are not able to verify it?\nThe following dummy code demonstrates the hardcoding issue:\n```{python}\n# Hardcoded path\nfile_path = \"/user/notebooks/toydata.csv\"\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nIn the code above, a dummy file (toydata.csv) is used. The dummy file contains data on the prices of three different toys, but only the path of the file is relevant to this example. If the hardcoded file path – \"/user/notebooks/toydata.csv\" – exists on the machine being used, the code will run just fine. But, when run in a different environment without said path, the code will result in a \"File not found error\". Better code that uses relative paths can be written as:\n```{python}\n# Relative path\nimport os\n\nfile_path = os.path.join(os.getcwd(), \"toydata.csv\")\ntry:\n    with open(file_path) as file:\n        data = file.read()\n        print(data)\nexcept FileNotFoundError:\n    print(\"File not found\")\n```\n\nYou can see that this code has successfully imported data from the dataset toydata.csv and printed its two columns (toy and price) and three rows.\nThe following example is a simplified version of what happened when I wrote code to train several models, store the results and run a procedure to compare results with the predictive performance of a benchmark model:\n```{python}\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\"}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\"}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv('/all/notebooks/results-of-model1.csv', index=False)\n```\n```{python}\n# Load the model result and compare with benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model\nresult = pd.read_csv('/all/notebooks/results-of-model2.csv').iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nEverything looks fine at a glance. But, if you examine the code carefully, you may spot the problem. Initially, when I coded the procedure (training the model, saving and loading the results), I hardcoded the paths and had to change them for each tested model. First, I trained model2, a complex model, and tested it against the benchmark (70 &gt; 50 → accepted). I repeated the procedure for model1 (a simple model). Its result was identical to model2, therefore I kept model1 following the parsimony principle.\nHowever, for the code line loading the result for the current model (line 5, second cell), I forgot to amend the path and so mistakenly loaded the result of model2. As a consequence, I accepted a model which should have been rejected. These wrong results were then spread further in the code, including all charts and maps and the conclusions of my analysis.\nA small coding error like this can therefore be fatal to an analysis. Below is the corrected code:\n```{python}\nimport os\n# Set an arbitrary predictive performance value of a benchmark model\n# and accept/reject models if the results are above/below the value.\nbenchmark = 50\n# Set the model details (INCLUDING PATHS) in one place for a better overview\nmodel = {\n    \"model1\": {\"name\": \"model1\", \"type\": \"simple\", \"path\": os.path.join(os.getcwd(), \"results-of-model1.csv\")}, \n    \"model2\": {\"name\": \"model2\", \"type\": \"complex\", \"path\": os.path.join(os.getcwd(), \"results-of-model2.csv\")}\n}\n# Set the current model to \"model1\" to use it for training and check its results\ncurrent_model = model[\"model1\"]\n# Train a simple model for \"model1\" and a complex model for \"model2\"\n# Training result of the \"model1\" is 30 and for \"model2\" is 70\nmodel_structure = train(current_model[\"type\"])\n# Save the model and its result in a .csv file\nmodel_structure.to_csv(current_model[\"path\"], index=False)\n```\n```{python}\n# Get the model result and compare with the benchmark\nprint(\"Model name: {}\".format(current_model[\"name\"]))\nprint(\"Model type: {}\".format(current_model[\"type\"]))\n# Load the result of the current model WITH a VARIABLE PATH\nresult = pd.read_csv(current_model[\"path\"]).iloc[0, 0]\nprint(\"Result: {}\".format(result))\n\nif result &gt; benchmark:\n    print(\"\\033[3;32m&gt;&gt;&gt; Result is better than the benchmark -&gt; Accept the model and use it for calculations\")\nelse:\n    print(\"\\033[3;31m&gt;&gt;&gt; Result is NOT better than the benchmark -&gt; Reject the model as it is not optimal\")\n```\n\nHere, the paths are stored with other model details (line 7–8, first cell). Therefore, we can use them as variables when we need them (e.g., line 16, first cell, and line 5, second cell). Now, when the current model is set to model1 (line 11, first cell), everything is automatically adjusted. Also, if the path details need to be changed, we only need to change them once and everything else is automatically adjusted and updated. The code now correctly states that model1 performs worse than the benchmark and is therefore rejected and we should keep model2, which performs best.\nI managed to catch this error in time, but it often can be difficult to spot our own mistakes. That is why making code available to others is crucial. A code review by a second (or third) pair of eyes can save everyone a lot of time and avoid spreading incorrect results and conclusions.\n\n\nSolving compatibility chaos with Docker\nOne might think that it would be easy to copy code from one computer to another and run it without difficulties, but it turns out to be a real headache. Different operating systems on my local computer and the university server caused multiple compatibility issues and it was very time-consuming to try to solve them. The university server was running on Ubuntu, a Linux distribution, which was not compatible with my macOS-based code editor. Moreover, the server did not support the Python programming language – and all the deep learning algorithm packages that I needed – in the same way as my macOS computer did.\nAs a remedy, I used Docker containers, which allowed me to create a virtual environment with all the necessary packages and dependencies installed. This way, I could integrate them with different hardware and use the processing power of that hardware. To get started with Docker, I first had to install it on my local computer. The installation process is straightforward and the Docker website provides step-by-step instructions for different operating systems. In fact, I found the Docker website very helpful, with lots of resources and tutorials available. Once Docker was installed, it was easy to create virtual environments for my project and work with my code, libraries, and packages, without any compatibility issues. Not only did Docker containers save me a lot of time and effort, but they could also make it easier for others to reproduce my work.\nBelow is an example of a Dockerfile which recreates an environment with Python 3.7 on Linux. It describes what, how, when and in which order operations should be carried out to generate the environment with all Python packages required to run the main Python script, main.py.\n\n\nAn example of a Dockerfile.\n\nIn this example, by downloading the project, including the Dockerfile, anyone can run main.py without installing packages or worrying about what OS was used for development or which Python version should be installed. You can view Docker as a great robot chef: show it a recipe (Dockerfile), provide the ingredients (project files), push the start button (to build the container) and wait to sample the results.\n\n\nWhy does nobody check your code?\nEven after implementing Docker, I still faced another challenge to reproducibility: making the verification process for my code easy enough that it could be done by anyone, without them needing a degree in computer science! Increasingly, there is an expectation for researchers to share their code so that results can be reproduced, but there are as yet no widely accepted or enforced standards on how to make code readable and reusable. However, if we are to embrace the concept of reproducibility, we must write and publish code under the assumption that someone, somewhere – boss, team member, journal reviewer, reader – will want to rerun our code. And, if we expect that someone will want to rerun our code (and hopefully check it), we should ensure that the code is readable and does not take too long to run.\nIf your code does take too long to run, some operations can often be accelerated – for example, by reducing the size of the datasets or by implementing computationally efficient data processing approaches (e.g., using PyTorch). Aim for a running time of a few minutes – or about as long as it takes to make a cup of tea or coffee. Of course, if data needs to be reduced to save computational time, the person rerunning your code won’t generate the same results as in your original analysis. This therefore will not lead to reproducibility, sensu stricto. However, as long as you state clearly what are the expected results from the reduced dataset, your peers can at least inspect your code and offer feedback, and this marks a step towards reproducibility.\nWe should also make sure our code is free from bugs – both the kind that might lead to errors in analysis and also those that stop the code running to completion. Bugs can occur for various reasons. For example, some code chunks written on a Windows machine may not properly execute on a macOS machine because the former uses \\ for file paths, while the latter uses /:\n```{python}\n# Path works on macOS/Linux\nwith open(\"../../all/notebooks/toydata.csv\", \"r\") as f:\n    print(f.read())\n\n# Path works only on Windows    \nwith open(r\"..\\..\\all\\notebooks\\toydata.csv\", \"r\") as f:\n   print(f.read())\n```\n\nHere, only the macOS/Linux version works, since the code this capture was taken from was implemented on a Linux server. There are alternatives, however. The code below works on macOS, Linux, and also Windows machines:\n```{python}\nfrom pathlib import Path\n\n# Path works on every OS: macOS/Linux/Windows\n# It will automatically replace the path to \"..\\..\\all\\notebooks\\toydata.csv\" when it runs on Windows\nwith open(Path(\"../../all/notebooks/toydata.csv\"), \"r\") as f:\n    print(f.read())\n```\n\nThe extra Python package, pathlib, is of course unnecessary if you build a Docker container for your project, as discussed in the previous section.\n\n\nJupyter, King of the Notebooks\nBy this stage in my project, I was feeling that I’d made good progress towards ensuring that my work would be reproducible. I’d expended a lot of effort to make my code readable, efficient, and also absent of bugs (or, at least, this is what I was hoping for). I’d also built a Docker container to allow others to replicate my computing environment and rerun the analysis. Still, I wanted to make sure there were no barriers that would prevent people – my supervisors, in particular – from being able to review the work I had done for my undergraduate thesis. What I wanted was a way to present a complete narrative of my project that was easy to understand and follow. For this, I turned to Jupyter Notebook.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nJupyter notebooks combine Markdown text, code, and visualisations. The notebook itself can sit within an online directory of folders and files that contain all the data and code related to a project, allowing readers to understand the processes behind the work and also access the raw resources. From the notebook I produced, readers can see exactly what I did, how I did it, and what my results were.\nWhile creating my notebook, I was able to experiment with my code and iterate quickly. Code cells within a document can be run interactively, which allowed me to try out different approaches to solving a problem and see the results almost in real time. I could also get feedback from others and try out new ideas without having to spend a lot of time writing and debugging code.\n\n\nVersion control with Git and GitHub\nMy Jupyter notebook and associated folders and files are all available via GitHub. Git is a version control system that allows you to keep track of changes to your code over time, while GitHub is a web-based platform that provides a central repository for storing and sharing code. With Git and GitHub, I was able to version my code and collaborate with others without the risk of losing any work. I really couldn’t afford to redo the entire year I spent on my dissertation!\nGit and GitHub are great for reproducibility. By sharing code via these platforms, others can access your work, verify it and reproduce your results without risking changing or, worse, destroying your work – whether partially or completely. These tools also make it easy for others to build on your work if they want to further develop your research. You can also use Git and GitHub to share or promote your results across a wider community. The ability to easily store and share your code also makes it easy to keep track of the different versions of your code and to see how your work has evolved.\nThe following illustration shows the tracking of very simple changes in a Python file. The previous version of the code is shown on the left; the new version is shown on the right. Additions and deletions are highlighted in green and red, and with + and - symbols, respectively.\n\n\nA simple example of GitHub version tracking."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "href": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#the-deep-learning-challenge",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "The deep learning challenge",
    "text": "The deep learning challenge\nSo far, this article has dealt with barriers to reproducibility – and ways around them – that will apply to most, if not all, modern research projects. While I’d encourage any scientist to adopt these practices in their own work, it is important to stress that these alone cannot guarantee reproducibility. In cases where standard statistical procedures are used within statistical software packages, reproducibility is often achievable. However, in reality, even when following the same procedures, differences in outputs can occur, and identifying the reasons for this may be challenging. Cooking offers a simple analogy: subtle changes in room temperature or ingredient quality from one day to the next can impact the final product.\nOne of the challenges for research projects employing machine learning and deep learning algorithms is that outputs can be influenced by the randomness that is inherent in these approaches. Consider the four portraits below, generated by the Midjourney bot.\n\n\n\n\n\n\nCredit: Discord software, Midjourney bot.\n\nEach portrait looks broadly similar at first glance. However, upon closer inspection, critical differences emerge. These differences arise because deep learning models rely on numerous interconnected layers to learn intricate patterns and representations. Slight random perturbations, such as initial parameter values or changes in data samples, can propagate through the network, leading to different decisions during the learning process. As a result, even seemingly negligible randomness can amplify and manifest as considerable differences in the final output, as with the distinct features of the portraits.\nRandomness is not necessarily a bad thing – it mitigates overfitting and helps predictions to be generalised. However, it does present an additional barrier to reproducibility. If you cannot get the same results using the same raw materials – data, code, packages and computing environment – then you might have good reasons to doubt the validity of the findings.\nThere are many elements of an analysis in which randomness may be present and lead to different results. For example, in a classification (where your dependent variable is binary, e.g., success/failure with 1 and 0) or a regression (where your dependent variable is continuous, e.g., temperature measurements of 10.1°C, 2.8°C, etc.), you might need to split your data into training and testing sets. The training set is used to estimate the model (hyper)parameters and the testing set is used to compute the performance of the model. The way the split is usually operationalised is as a random selection of rows of your data. So, in principle, each time you split your data into training and testing sets, you may end up with different rows in each set. Differences in the training set may therefore lead to different values of the model (hyper)parameters and affect the predictive performance that is measured from the testing set. Also, differences in the testing set may lead to variations in the predictive performance scores, which in turn lead to potentially different interpretations and, ultimately, decisions if the results are used for that purpose.\nThis aspect of randomness in the training of models is relatively well known. But randomness may hide in other parts of code. One such example is illustrated below. Here, using Python, we set the seed number to 0 using np.random.seed(seed value). The random.seed() function from the package numpy (abbreviated np) saves the state of a random function so that it can create identical random numbers independently of the machine you use, and this is for any number of executions. A seed value is an initial input or starting point used by a pseudorandom number generator to generate a sequence of random numbers. It is often an integer or a timestamp. The number generator takes this seed value and uses it to produce a deterministic series of random numbers that appear to be random but can be recreated by using the same seed value. Without providing this seed value, the first execution of the function typically uses the current system time. The animation below generates two random arrays arr1 and arr2 using np.random.rand(3,2). Note that the values 3,2 indicate that we want random values for an array that has 3 rows and 2 columns.\n```{python}\nimport numpy as np\n\n#Set the seed number e.g. to 0\nnp.random.seed(0)\n# Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Set the seed number as before to get the same results\nnp.random.seed(0)\n# Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nIf you run the code yourself multiple times, the values of arr1 and arr2 should remain identical. If this is not the case, check that the seed value is set to 0 in lines 4 and 11. These identical results are possible because we set the seed value to 0, which ensures that the random number generator produces the same sequence of numbers each time the code is run. Now, let’s look at what happens if we remove the line np.random.seed(0):\n```{python}\n#Generate random array\narr1 = np.random.rand(3,2)\n## print(\"Array 1:\")\n## print(arr1)\n\n#Generate another random array\narr2 = np.random.rand(3,2)\n## print(\"\\nArray 2:\")\n## print(arr2)\n```\n\nHere, the values of arr1 and arr2 will be different each time we run the code since the seed value was not set and is therefore changing over time.\nThis short code demonstrates how randomness that can be controlled by the seed value may affect your code. Therefore, unless randomness is required, e.g., to get some uncertainty in the results, setting the seed value will contribute to making your work reproducible. I also find it helpful to document the seed number I use in my code so that I can easily reproduce my findings in the future. If you are currently working on some code that involves random number generators, it might be worth checking your code and making all necessary changes. In our work (see code chunk 9 in the Jupyter notebook) we set the seed value in a general way, using a framework (config) so that our code always uses the same seed to train our algorithm."
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "href": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#conclusion",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "Conclusion",
    "text": "Conclusion\nWe hope you have enjoyed learning more about our quest for reproducibility. We have explained why reproducibility matters and provided tips for how to achieve it – or, at least, work towards it. We have introduced a few important issues that you are likely to encounter on your own path to reproducibility. In sum, we have mentioned:\n\nThe importance of having relative instead of hard-coded paths in code.\nOperating system compatibility issues, which can be solved by using Docker containers for a consistent computing environment.\nThe convenience of Jupyter notebooks for code editing – particularly useful for data science projects and work using deep learning because of the ability to include text and code in the same document and make the work accessible to everyone (so long as they have an internet connection).\nThe need for version control using, for example, Git and GitHub, which allows you to keep track of changes in your code and collaborate with others efficiently.\nThe importance of setting the seed values in random number generators.\n\nThe graphic below provides a visual overview of the different components of our study and shows how each component works with the others to support reproducibility.\n\nWe use (A) the version control system, Git, and its hosting service, GitHub, which enables a team to share code with peers, efficiently track and synchronise code changes between local and server machines, and reset the project to a working state in case something breaks. Docker containers (B) include all necessary objects (engine, data, and scripts). Docker needs to be installed (plain-line arrows) by all users (project leader, collaborator(s), reviewer(s), and public user(s)) on their local machines (C); and (D) we use a user-friendly interface (JupyterLab) deployed from a local machine to facilitate the operations required to reproduce the work. The project leader and collaborators can edit (upload/download) the project files stored on the GitHub server (plain-line arrows) while reviewers and public users can only read the files (dotted-line arrows).\nNow, it is over to you. Our Jupyter notebook provides a walkthrough of our research. Our GitHub repository has all the data, code and other files you need to reproduce our work, and this README file will help you get started.\nAnd with that, we wish you all the best on the road to reproducibility!\n\nFind more case studies\n\n\n\n\n\nAbout the authors\n\nDavit Svanidze is a master’s degree student in economics at the London School of Economics (LSE). Andre Python is a young professor of statistics at Zhejiang University’s Center for Data Science. Christoph Weisser is a senior data scientist at BASF. Benjamin Säfken is professor of statistics at TU Clausthal. Thomas Kneib is professor of statistics and dean of research at the Faculty of Business and Economic Sciences at Goettingen University. Junfen Fu is professor of pediatrics, chief physician and director of the Endocrinology Department of Children’s Hospital, Zhejiang University, School of Medicine.\n\n\n\n\n\nAcknowledgement\n\nAndre Python has been funded by the National Natural Science Foundation of China (82273731), the National Key Research and Development Program of China (2021YFC2701905) and Zhejiang University global partnership fund (188170-11103).\n\n\n\n\n\nCopyright and licence\n\n© 2023 Davit Svanidze, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu.\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nSvanidze, Davit, Andre Python, Christoph Weisser, Benjamin Säfken, Thomas Kneib, and Junfen Fu. 2023. “The road to reproducible research: hazards to avoid and tools to get you there safely.” Real World Data Science, June 15, 2023. URL"
  },
  {
    "objectID": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "href": "instruments_old/case-studies/posts/2023/06/15/road-to-reproducible-research.html#footnotes",
    "title": "The road to reproducible research: hazards to avoid and tools to get you there safely",
    "section": "References",
    "text": "References\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–1227.↩︎\nIoannidis, John P. A., Sander Greenland, Mark A. Hlatky, Muin J. Khoury, Malcolm R. Macleod, David Moher, Kenneth F. Schulz, and Robert Tibshirani. 2014. “Increasing Value and Reducing Waste in Research Design, Conduct, and Analysis.” The Lancet 383 (9912): 166–175.↩︎\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716.↩︎\nBaker, Monya. 2016. “Reproducibility Crisis?” Nature 533 (26): 353–366.↩︎\nCamerer, Colin F., Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, Gideon Nave, Brian A. Nosek, Thomas Pfeiffer, et al. 2018. “Evaluating the Replicability of Social Science Experiments in Nature and Science between 2010 and 2015.” Nature Human Behaviour 2: 637–644.↩︎"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html",
    "href": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html",
    "title": "Creating Christmas cards with R",
    "section": "",
    "text": "When you think about data visualisation in R (R Core Team 2022), you’d be forgiven for not jumping straight to thinking about creating Christmas cards. However, the package and functions we often use to create bar charts and line graphs can be repurposed to create festive images. This tutorial provides a step-by-step guide to creating a Christmas card featuring a snowman – entirely in R. Though this seems like just a fun exercise, the functions and techniques you learn in this tutorial can also transfer into more traditional data visualisations created using {ggplot2} (Wickham 2016) in R.\nThe code in this tutorial relies on the following packages:"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "href": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#lets-build-a-snowman",
    "title": "Creating Christmas cards with R",
    "section": "Let’s build a snowman!",
    "text": "Let’s build a snowman!\nBefore we jump in to writing R code, let’s take a step back and think about what you actually need to build a snowman. If you were given some crayons and a piece of paper, what would you draw?\nYou might draw two or three circles to make up the head and body. Perhaps some smaller dots for buttons and eyes, and a (rudimentary) hat constructed from some rectangles. Some brown lines create sticks for arms and, of course, a triangle to represent a carrot for a nose. For the background elements of our Christmas card, we also need the night sky (or day if you prefer), a light dusting of snow covering the ground, and a few snowflakes falling from the sky.\nNow lines, rectangles, circles, and triangles are all just simple geometric objects. Crucially, they’re all things that we can create with {ggplot2} in R."
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "href": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#build-a-snowman-with-r",
    "title": "Creating Christmas cards with R",
    "section": "Build a snowman with R",
    "text": "Build a snowman with R\nLet’s start with the background. The easiest way to start with a blank canvas in {ggplot2} is to create an empty plot using ggplot() with no arguments. We can also remove all theme elements (such as the grey background and grid lines) with theme_void(). To change the background colour to a dark blue for the night sky, we can edit the plot.background element of the theme using element_rect() (since the background is essentially just a big rectangle).\nIn {ggplot2} fill is the inner colour of shapes whilst colour is the outline colour. You can specify colours in different ways in R: either via the rgb() function, using a character string for a hex colour such as \"#000000\", or using a named colour. If you run colors(), you’ll see all the valid named colours you can use. Here, we’ve picked \"midnightblue\".\nLet’s save this initial plot as an object s1 that we’ll keep adding layers to. Saving plots in different stages of styling as objects can help to keep your code more modular.\ns1 &lt;- ggplot() +\n  theme_void() +\n  theme(\n    plot.background = element_rect(\n      fill = \"midnightblue\"\n      )\n  )\ns1\nNext we’ll add some snow on the ground. We’ll do this by drawing a white rectangle along the bottom of the plot. There are two different functions that we could use to add a rectangle: geom_rect() or annotate(). The difference between the two is that geom_rect() maps columns of a data.frame to different elements of a plot whereas annotate() can take values passed in as vectors. Most of the {ggplot2} graphs you’ll see will use geom_*() functions. However, if you’re only adding one or two elements to a plot then annotate() might be quicker.\nSince we’re only adding one rectangle for the snow, it’s easier to use annotate() with the \"rect\" geometry. This requires four arguments: the minimum and maximum x and y coordinates of the rectangle – essentially specifying where the corners are. We can also change the colour of the rectangle and its outline using the fill and colour arguments. Here, I’ve used a very light grey instead of white.\nIf we don’t set the axis limits using xlim() and ylim(), the plot area will resize to fit the area of the snow rectangle. The night sky background will disappear. You can choose any axis limits you wish here – but the unit square will make it easier to find the right coordinates when deciding where to position other elements. Finally, we add coord_fixed() to fix the 1:1 aspect ratio and make sure our grid is actually square with expand = FALSE to remove the additional padding at the sides of the plot.\ns2 &lt;- s1 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0, xmax = 1,\n    ymin = 0, ymax = 0.2,\n    fill = \"grey98\",\n    colour = \"grey98\"\n  ) +\n  xlim(0, 1) +\n  ylim(0, 1) +\n  coord_fixed(expand = FALSE)\ns2\n\n\n\n\n\n\n\n\n\n\nTo finish off the background, we’ll add some falling snowflakes. We first need to decide where on the plot the snowflakes will appear. We’ll be plotting lots of snowflakes, so manually typing out the coordinates of where they’ll be would be very inefficient. Instead, we can use functions to generate the locations randomly. For this we’ll use the uniform distribution. The uniform distribution has two parameters – the lower and upper bounds where any values between the bounds are equally likely. You can generate samples from a uniform distribution in R using the runif() function.\nWhen generating random numbers in R (or any other programming language), it’s important to set a seed. This means that if you give your code to someone else, they’ll get the same random numbers as you. Some people choose to use the date as the random seed and since we’re making Christmas cards, we’ll use Christmas day as the random seed – in yyyymmdd format, of course!\nWe create a variable n specifying how many snowflakes we’ll create. Creating a variable rather than hard coding the variables makes it easier to vary how many snowflakes we want. Since our plot grid goes between 0 and 1 in both the x and y directions, we generate random numbers between 0 and 1 for both the x and y coordinates and store the values in a data.frame called snowflakes.\nset.seed(20231225)\nn &lt;- 100\nsnowflakes &lt;- data.frame(\n  x = runif(n, 0, 1),\n  y = runif(n, 0, 1)\n)\nNow we can plot the snowflakes data using geom_point() – the same function you’d use for a scatter plot. Since we’re using a geom_*() function, we need to tell {ggplot2} which columns go on the x and y axes inside the aes() function. To plot the snowflakes, we’re going to make using of R’s different point characters. The default when plotting with geom_point() is a small black dot, but we can choose to use a small star (close enough to a snowflake!) by setting pch = 8 and changing the colour to \"white\".\ns3 &lt;- s2 +\n  geom_point(\n    data = snowflakes,\n    mapping = aes(\n      x = x,\n      y = y\n    ),\n    colour = \"white\",\n    pch = 8\n  )\ns3\nNow comes the part where we start rolling up some snowballs! Or, in the case of an R snowman, we draw some circles. Unfortunately, there isn’t a built-in geom_*() function in {ggplot2} for plotting circles. We could use geom_point() here and increase the size of the points but this approach can look a little bit fuzzy when the points are very large. Instead, we’ll turn to a {ggplot2} extension package for some additional geom_* functions - {ggforce} (Pedersen 2022).\nThe geom_circle() function requires at least three elements mapped to the aesthetics inside aes(): the coordinates of the centre of the circle given by x0 and y0, and the radii of each of the circles, r. Instead of creating a separate data frame and passing it into geom_circle(), we can alternatively create the data frame inside the function. The fill and colour arguments work as they do in {ggplot2} and we can set both to \"white\".\ns4 &lt;- s3 +\n  geom_circle(\n    data = data.frame(\n      x0 = c(0.6, 0.6),\n      y0 = c(0.3, 0.5),\n      r = c(0.15, 0.1)\n    ),\n    mapping = aes(x0 = x0, y0 = y0, r = r),\n    fill = \"white\",\n    colour = \"white\"\n  )\ns4\n\n\n\n\n\n\n\n\n\n\nWe can use geom_point() again to add some more points to represent the buttons and the eyes. Here, we’ll manually specify the coordinates of the points. For the buttons we add them in a vertical line in the middle of the snowman’s body circle, and for the eyes we add them in a horizontal line in the middle of the head circle.\nSince no two rocks are exactly the same size, we can add some random variation to the size of the points using runif() again. We generate five different sizes between 2 and 4.5. For reference, the default point size is 1.5. Adding scale_size_identity() means that the sizes of the points are actually equally to the sizes we generated from runif() and removes the legend that is automatically added when we add size inside aes().\ns5 &lt;- s4 +\n  geom_point(\n    data = data.frame(\n      x = c(0.6, 0.6, 0.6, 0.57, 0.62),\n      y = c(0.25, 0.3, 0.35, 0.52, 0.52),\n      size = runif(5, 2, 4.5)\n    ),\n    mapping = aes(x = x, y = y, size = size)\n  ) +\n  scale_size_identity()\ns5\nTo add sticks for arms, we can make use of geom_segment() to draw some lines. We could also use geom_path() but that is designed to connect points across multiple cases, whereas geom_segment() draws a single line per row of data – and we don’t want to join the snowman’s arms together!\nTo use geom_segment() we need to create a data frame containing the x and y coordinates for the start and end of each line, and then pass this into the aesthetic mapping with aes(). We can control the colour and width of the lines using the colour and linewidth arguments. Setting the lineend argument to \"round\" means that the ends of the lines will be rounded rather than the default straight edge.\ns6 &lt;- s5 + \n  geom_segment(\n    data = data.frame(\n      x = c(0.46, 0.7),\n      xend = c(0.33, 0.85),\n      y = c(0.3, 0.3),\n      yend = c(0.4, 0.4)\n    ),\n    mapping = aes(x = x, y = y, xend = xend, yend = yend),\n    colour = \"chocolate4\",\n    lineend = \"round\",\n    linewidth = 2\n  )\ns6\n\n\n\n\n\n\n\n\n\n\nWe’ll now add a (very simple) hat to our snowman, fashioned out of two rectangles. We can add the rectangles as we did before using the annotate() function and specifying the locations of the corners of the rectangles. We start with a shorter wider rectangle for the brim of the hat, and then a taller, narrower rectangle for the crown of the hat. Since we’ll colour them both \"brown\", it doesn’t matter if they overlap a little bit.\nThis might be one of the situations we should have used geom_rect() instead of annotate() but it might take a lot of trial and error to position the hat exactly where we want it, and this seemed a little easier with annotate().\ns7 &lt;- s6 +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.46, xmax = 0.74,\n    ymin = 0.55, ymax = 0.60,\n    fill = \"brown\"\n  ) +\n  annotate(\n    geom = \"rect\",\n    xmin = 0.50, xmax = 0.70,\n    ymin = 0.56, ymax = 0.73,\n    fill = \"brown\"\n  )\ns7\nNow we can move on to the final component of building a snowman – the carrot for his nose! We’re going to use a triangle for the nose. Unfortunately, there are no built-in triangle geoms in {ggplot2} so we’ll have to make our own. There are different ways to do this, but here we’re going to make use of the {sf} package (Pebesma 2018). The {sf} package (short for simple features) is designed for working with spatial data. Although we’re not working with maps, we can still use {sf} to make shapes – including polygons.\nWe start by constructing a matrix with two columns – one for x coordinates and one for y. The x coordinates start in the middle of the head and go slightly to the right for the triangle point. The y coordinates take a little bit more trial and error to get right. Note that although triangles only have three corners, we have four rows of points. The last row must be the same as the first to make the polygon closed. The matrix is then converted into a spatial object using the st_polygon() function, and we can check how it looks using plot().\nnose_pts &lt;- matrix(\n  c(\n    0.6, 0.5,\n    0.65, 0.48,\n    0.6, 0.46,\n    0.6, 0.5\n  ),\n  ncol = 2,\n  byrow = TRUE\n)\nnose &lt;- st_polygon(list(nose_pts))\nplot(nose)\n\n\n\n\n\n\n\n\n\n\nWe can plot sf objects with {ggplot2} using geom_sf(). geom_sf() is a slightly special geom since we don’t need to specify an aesthetic mapping for the x and y axes – they are determined automatically from the sf object along with which type of geometry to draw. If your sf object has points, points will be drawn. If it has country shapes, polygons will be drawn. Like other geom_*() functions, we can change the colour and fill arguments to a different colour – in this case \"orange\" to represent a carrot!\nYou should see a Coordinate system already present. Adding new coordinate system, which will replace the existing one. message when you run the following code. The is because geom_sf forces it’s own coordinate system on the plot overriding our previous code specifying coord_fixed(). If you run it without the coord_sf(expand = FALSE), the extra space around the plot will reappear. We can remove it again with expand = FALSE.\ns8 &lt;- s7 +\n  geom_sf(\n    data = nose,\n    fill = \"orange\",\n    colour = \"orange\"\n  ) +\n  coord_sf(expand = FALSE)\ns8\n\nYou could skip the sf part of this completely and pass the coordinates directly into geom_polygon() instead. However, I’ve often found it quicker and easier to tinker with polygon shapes using sf.\n\nA key part of any Christmas card is the message wishing recipients a Merry Christmas! We can add text to our plot using the annotate() function and the \"text\" geometry (you could instead use geom_text() if you prefer). When adding text, we require at least three arguments: the x and y coordinates of where the text should be added, and the label denoting what text should appear. We can supply additional arguments to annotate() to style the text, such as: colour (which changes the colour of the text); family (to define which font to use); fontface (which determines if the font is bold or italic, for example); and size (which changes the size of the text). The \"mono\" option for family tells {ggplot2} to use the default system monospace font.\ns9 &lt;- s8 +\n  annotate(\n    geom = \"text\",\n    x = 0.5, y = 0.07,\n    label = \"Merry Christmas\",\n    colour = \"red3\",\n    family = \"mono\",\n    fontface = \"bold\", size = 7\n  )\ns9"
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "href": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#sending-christmas-cards-in-r",
    "title": "Creating Christmas cards with R",
    "section": "Sending Christmas cards in R",
    "text": "Sending Christmas cards in R\nNow that we’ve finished creating our Christmas card, we need to think about how to send it. You could save it as an image file using ggsave(), print it out, and send it in the post. Or you could also use R to send it!\nThere are many different R packages for sending emails from R. If you create a database of email addresses and names, you could personalise the message on the Christmas card and then send it automatically as an email from R. If you want to automate the process of sending physical cards from R, you might be interested in the {ggirl} package from Jacqueline Nolis (Nolis 2023). {ggirl} allows you to send postcards with a ggplot object printed on the front. {ggirl} is also an incredible example of an eCommerce platform built with R! Note that {ggirl} can currently only send physical items to addresses in the United States."
  },
  {
    "objectID": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "href": "instruments_old/tutorials/posts/2023/12/12/xmas-cards.html#other-christmas-r-packages",
    "title": "Creating Christmas cards with R",
    "section": "Other Christmas R packages",
    "text": "Other Christmas R packages\nIf you’re curious about making Christmas cards with R but you don’t have the time to make them from scratch, you’ll likely find the christmas R package (Barrera-Gomez 2022) helpful. This package from Jose Barrera-Gomez can generate lots of different Christmas cards, many of them animated and available in different languages (English, Catalan and Spanish).\nEmil Hvitfeldt has also created a Quarto extension that gives the effect of falling snowflakes on HTML outputs – including revealjs slides which is perfect for festive presentations!\nHave you made your own Christmas cards with R? We’d love to see your designs!\n\n\n\n\n\n\nInspired by Nicola’s tutorial, Real World Data Science has indeed made its own Christmas card design. Check out our attempt over at the Editors’ Blog!\n\n\n\n\nExplore more Tutorials\n\n\n\n\n\nAbout the author\n\nNicola Rennie is a lecturer in health data science in the Centre for Health Informatics, Computing, and Statistics (CHICAS) within Lancaster Medical School at Lancaster University. She’s an R enthusiast, data visualisation aficionado, and generative artist, among other things. Her personal website is hosted at nrennie.rbind.io, and she is a co-author of the Royal Statistical Society’s Best Practices for Data Visualisation.\n\n\n\n\n\nCopyright and licence\n\n© 2023 Nicola Rennie\n\n\n  This article is licensed under a Creative Commons Attribution 4.0 (CC BY 4.0)  International licence.\n\n\n\nHow to cite\n\nRennie, Nicola. 2023. “Creating Christmas cards with R.” Real World Data Science, December 12, 2023."
  }
]